{
  "id": "error-handling",
  "name": "error-handling",
  "version": "2.0.0",
  "description": "Implement robust error handling strategies for applications. Covers error classification, custom error hierarchies, propagation patterns, recovery mechanisms (retry, fallback, circuit breaker, bulkhead), logging and observability, user-facing messages, and boundary error handling across API, UI, and background job layers.",
  "phase": "IMPLEMENT",
  "category": "core",
  "content": "# Error Handling\n\nImplement robust error handling strategies that keep systems reliable and debuggable.\n\n## When to Use\n\n- **Implementing a new service or feature** -- needs error classification, propagation, and recovery from the start\n- **Adding external integrations** -- API calls, databases, message queues require retry, timeout, and fallback logic\n- **Hardening existing code** -- swallowed errors, missing context, or silent failures need systematic remediation\n- **Building user-facing flows** -- error messages must be safe, helpful, and actionable without leaking internals\n- **Designing distributed systems** -- cross-service errors need correlation IDs, circuit breakers, and bulkheads\n- **After a production incident** -- post-mortem reveals gaps in error handling, logging, or alerting\n- When you say: \"handle errors properly\", \"add error handling\", \"make this resilient\", \"improve reliability\"\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `error-classification.md` | Operational vs programmer error taxonomy and severity levels |\n| `recovery-strategies.md` | Retry, fallback, circuit breaker, and bulkhead patterns with decision trees |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `logging-standards.md` | When establishing structured logging and log levels for errors |\n| `retry-patterns.md` | When implementing retry with exponential backoff, jitter, and idempotency |\n| `circuit-breaker-patterns.md` | When protecting services from cascading failures via circuit breakers and bulkheads |\n\n**Verification:** Ensure every public function has explicit error handling, custom errors carry classification and context, and recovery strategies are documented for each failure mode.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| Custom error hierarchy | `src/errors/` or `src/common/errors.ts` | Always |\n| Error handling middleware | `src/middleware/errorHandler.ts` | When building an API |\n| Recovery configuration | `src/config/resilience.ts` | When using retry, circuit breaker, or fallback |\n| Error boundary components | `src/components/ErrorBoundary.tsx` | When building a UI |\n| Error logging integration | `src/lib/logger.ts` | Always |\n\n## Core Concept\n\nError handling answers: **\"What happens when things go wrong?\"**\n\nRobust error handling is:\n- **Classified** -- every error has a type, severity, and whether it is operational or a programmer bug\n- **Contextual** -- errors carry enough information to diagnose root cause without reproducing\n- **Recoverable** -- transient failures are retried, degraded paths exist, blast radius is contained\n- **Observable** -- errors are logged with structured data, correlated across services, and alerted on\n- **User-respectful** -- users see helpful guidance, never stack traces or internal codes\n\nError handling is NOT:\n- Swallowing exceptions silently\n- Logging everything at ERROR level regardless of severity\n- Wrapping every line in try/catch without a strategy\n- Showing raw error messages to end users\n- Treating all failures identically (retry everything, fail on everything)\n\n## The Error Handling Process\n\n```\n+-------------------------------------------------------------+\n|               ERROR HANDLING PROCESS                         |\n|                                                              |\n|  1. CLASSIFY ERRORS                                          |\n|     +-> Operational vs programmer? Transient vs permanent?   |\n|                                                              |\n|  2. DEFINE ERROR HIERARCHY                                   |\n|     +-> Base error class, domain errors, infrastructure      |\n|                                                              |\n|  3. DESIGN PROPAGATION STRATEGY                              |\n|     +-> Where to catch, wrap, rethrow, or transform?         |\n|                                                              |\n|  4. IMPLEMENT RECOVERY PATTERNS                              |\n|     +-> Retry, fallback, circuit breaker, bulkhead           |\n|                                                              |\n|  5. ADD LOGGING & OBSERVABILITY                              |\n|     +-> Structured logs, correlation IDs, metrics, alerts    |\n|                                                              |\n|  6. CRAFT USER-FACING MESSAGES                               |\n|     +-> Safe, helpful, actionable -- no internals leaked     |\n|                                                              |\n|  7. HANDLE BOUNDARY ERRORS                                   |\n|     +-> API responses, UI boundaries, background job DLQs    |\n+-------------------------------------------------------------+\n```\n\n## Step 1: Classify Errors\n\nEvery error falls into one of two fundamental categories. This classification determines how you handle it.\n\n### Operational vs Programmer Errors\n\n| Aspect | Operational Error | Programmer Error |\n|--------|-------------------|------------------|\n| **Cause** | External conditions the program cannot prevent | Bugs in the code itself |\n| **Examples** | Network timeout, disk full, invalid user input, service unavailable | Null reference, type error, index out of bounds, assertion failure |\n| **Expected?** | Yes -- these will happen in production | No -- these indicate defects |\n| **Recovery** | Retry, fallback, degrade, report to user | Crash fast, fix the code, deploy a patch |\n| **Logging** | WARN or ERROR depending on impact | ERROR or FATAL -- always investigate |\n| **User message** | Helpful guidance (\"Try again\", \"Contact support\") | Generic (\"Something went wrong\") |\n\n### Transient vs Permanent Classification\n\n| Type | Description | Action |\n|------|-------------|--------|\n| **Transient** | Temporary failure that may succeed on retry | Retry with backoff |\n| **Permanent** | Condition that will not change without intervention | Fail immediately, notify |\n| **Indeterminate** | Cannot determine if transient or permanent | Retry with limited attempts, then escalate |\n\n### Severity Levels\n\n```typescript\nenum ErrorSeverity {\n  /** Informational -- operation degraded but succeeded via fallback */\n  LOW = 'low',\n  /** Warning -- operation failed but system is healthy, user can retry */\n  MEDIUM = 'medium',\n  /** Error -- operation failed, feature unavailable, needs attention */\n  HIGH = 'high',\n  /** Critical -- system health impacted, cascading risk, page immediately */\n  CRITICAL = 'critical',\n}\n```\n\n### Classification Decision Tree\n\n```\nIs the error caused by a bug in our code?\n+-- Yes -> PROGRAMMER ERROR -> crash fast, log FATAL, fix code\n+-- No -> OPERATIONAL ERROR\n             |\n             Is the error transient?\n             +-- Yes -> TRANSIENT OPERATIONAL\n             |          Retry with backoff (see Step 4)\n             +-- No -> PERMANENT OPERATIONAL\n             |          Fail, notify user, log for investigation\n             +-- Unknown -> INDETERMINATE\n                            Limited retry, then escalate\n```\n\n## Step 2: Define Error Hierarchy\n\nBuild a custom error hierarchy that carries classification, context, and operational metadata.\n\n### Base Error Class\n\n```typescript\n// src/errors/AppError.ts\n\nexport interface ErrorContext {\n  [key: string]: unknown;\n}\n\nexport abstract class AppError extends Error {\n  /** Machine-readable error code (e.g., 'PAYMENT_FAILED', 'USER_NOT_FOUND') */\n  abstract readonly code: string;\n\n  /** HTTP status code for API responses */\n  abstract readonly statusCode: number;\n\n  /** Is this an operational error (expected) vs a programmer error (bug)? */\n  abstract readonly isOperational: boolean;\n\n  /** Severity level for logging and alerting */\n  readonly severity: ErrorSeverity;\n\n  /** Structured context for debugging -- never exposed to users */\n  readonly context: ErrorContext;\n\n  /** Correlation ID for distributed tracing */\n  readonly correlationId?: string;\n\n  /** Original error that caused this one */\n  readonly cause?: Error;\n\n  constructor(\n    message: string,\n    options: {\n      severity?: ErrorSeverity;\n      context?: ErrorContext;\n      correlationId?: string;\n      cause?: Error;\n    } = {},\n  ) {\n    super(message);\n    this.name = this.constructor.name;\n    this.severity = options.severity ?? ErrorSeverity.MEDIUM;\n    this.context = options.context ?? {};\n    this.correlationId = options.correlationId;\n    this.cause = options.cause;\n\n    // Capture proper stack trace (V8 engines)\n    if (Error.captureStackTrace) {\n      Error.captureStackTrace(this, this.constructor);\n    }\n  }\n\n  /** Serialize for logging -- includes all diagnostic data */\n  toLogObject(): Record<string, unknown> {\n    return {\n      name: this.name,\n      code: this.code,\n      message: this.message,\n      severity: this.severity,\n      isOperational: this.isOperational,\n      statusCode: this.statusCode,\n      context: this.context,\n      correlationId: this.correlationId,\n      cause: this.cause?.message,\n      stack: this.stack,\n    };\n  }\n\n  /** Serialize for API responses -- safe for external consumption */\n  toApiResponse(): { error: { code: string; message: string } } {\n    return {\n      error: {\n        code: this.code,\n        message: this.isOperational ? this.message : 'An unexpected error occurred',\n      },\n    };\n  }\n}\n```\n\n### Domain Error Classes\n\n```typescript\n// src/errors/domain.ts\n\n/** Validation errors -- bad user input, schema violations */\nexport class ValidationError extends AppError {\n  readonly code = 'VALIDATION_ERROR';\n  readonly statusCode = 400;\n  readonly isOperational = true;\n\n  constructor(\n    message: string,\n    public readonly fieldErrors: Record<string, string[]> = {},\n    options?: { context?: ErrorContext; correlationId?: string },\n  ) {\n    super(message, { severity: ErrorSeverity.LOW, ...options });\n  }\n\n  toApiResponse() {\n    return {\n      error: {\n        code: this.code,\n        message: this.message,\n        fields: this.fieldErrors,\n      },\n    };\n  }\n}\n\n/** Resource not found */\nexport class NotFoundError extends AppError {\n  readonly code = 'NOT_FOUND';\n  readonly statusCode = 404;\n  readonly isOperational = true;\n\n  constructor(resource: string, identifier: string, options?: { correlationId?: string }) {\n    super(`${resource} not found: ${identifier}`, {\n      severity: ErrorSeverity.LOW,\n      context: { resource, identifier },\n      ...options,\n    });\n  }\n}\n\n/** Authorization / permission denied */\nexport class ForbiddenError extends AppError {\n  readonly code = 'FORBIDDEN';\n  readonly statusCode = 403;\n  readonly isOperational = true;\n\n  constructor(message = 'You do not have permission to perform this action', options?: { context?: ErrorContext }) {\n    super(message, { severity: ErrorSeverity.MEDIUM, ...options });\n  }\n}\n\n/** Business rule violation */\nexport class BusinessRuleError extends AppError {\n  readonly code: string;\n  readonly statusCode = 422;\n  readonly isOperational = true;\n\n  constructor(\n    code: string,\n    message: string,\n    options?: { context?: ErrorContext; correlationId?: string },\n  ) {\n    super(message, { severity: ErrorSeverity.MEDIUM, ...options });\n    this.code = code;\n  }\n}\n\n/** Conflict -- duplicate, stale data, optimistic lock failure */\nexport class ConflictError extends AppError {\n  readonly code = 'CONFLICT';\n  readonly statusCode = 409;\n  readonly isOperational = true;\n\n  constructor(message: string, options?: { context?: ErrorContext }) {\n    super(message, { severity: ErrorSeverity.MEDIUM, ...options });\n  }\n}\n```\n\n### Infrastructure Error Classes\n\n```typescript\n// src/errors/infrastructure.ts\n\n/** External service failure -- API call, database, message queue */\nexport class ExternalServiceError extends AppError {\n  readonly code = 'EXTERNAL_SERVICE_ERROR';\n  readonly statusCode = 502;\n  readonly isOperational = true;\n\n  constructor(\n    public readonly serviceName: string,\n    message: string,\n    options?: { cause?: Error; context?: ErrorContext; correlationId?: string },\n  ) {\n    super(message, { severity: ErrorSeverity.HIGH, ...options });\n  }\n}\n\n/** Timeout -- operation exceeded time limit */\nexport class TimeoutError extends AppError {\n  readonly code = 'TIMEOUT';\n  readonly statusCode = 504;\n  readonly isOperational = true;\n\n  constructor(\n    operation: string,\n    timeoutMs: number,\n    options?: { cause?: Error; correlationId?: string },\n  ) {\n    super(`Operation '${operation}' timed out after ${timeoutMs}ms`, {\n      severity: ErrorSeverity.HIGH,\n      context: { operation, timeoutMs },\n      ...options,\n    });\n  }\n}\n\n/** Rate limited -- too many requests to external resource */\nexport class RateLimitError extends AppError {\n  readonly code = 'RATE_LIMITED';\n  readonly statusCode = 429;\n  readonly isOperational = true;\n\n  constructor(\n    public readonly retryAfterMs: number,\n    options?: { context?: ErrorContext },\n  ) {\n    super(`Rate limited. Retry after ${retryAfterMs}ms`, {\n      severity: ErrorSeverity.MEDIUM,\n      context: { retryAfterMs },\n      ...options,\n    });\n  }\n}\n\n/** Internal / programmer error -- should never reach the user */\nexport class InternalError extends AppError {\n  readonly code = 'INTERNAL_ERROR';\n  readonly statusCode = 500;\n  readonly isOperational = false;\n\n  constructor(message: string, options?: { cause?: Error; context?: ErrorContext }) {\n    super(message, { severity: ErrorSeverity.CRITICAL, ...options });\n  }\n}\n```\n\n### Hierarchy Checklist\n\n```markdown\n- [ ] Base AppError class with code, statusCode, isOperational, severity, context\n- [ ] toLogObject() for structured logging (includes all diagnostic data)\n- [ ] toApiResponse() for safe external serialization (no internals leaked)\n- [ ] Domain errors: ValidationError, NotFoundError, ForbiddenError, BusinessRuleError, ConflictError\n- [ ] Infrastructure errors: ExternalServiceError, TimeoutError, RateLimitError, InternalError\n- [ ] All errors carry correlationId for distributed tracing\n- [ ] All errors preserve cause chain for root cause analysis\n- [ ] Error codes are UPPER_SNAKE_CASE strings, stable across versions\n```\n\n## Step 3: Design Propagation Strategy\n\nError propagation determines where errors are caught, where they are transformed, and where they surface.\n\n### The Propagation Principle\n\n```\n+-------------------------------------------------------------+\n|               ERROR PROPAGATION LAYERS                       |\n|                                                              |\n|   Layer          Action            Why                       |\n|   ------         ------            ---                       |\n|   Origin         Throw with        Accurate context          |\n|                  full context      at point of failure        |\n|                                                              |\n|   Service        Catch + wrap      Add business context,     |\n|                  if crossing       translate infrastructure   |\n|                  boundary          errors into domain errors  |\n|                                                              |\n|   Controller/    Catch + format    Transform into API         |\n|   Handler        for consumer      response or UI state       |\n|                                                              |\n|   Global         Catch-all         Safety net for             |\n|   Handler        for uncaught      unhandled errors           |\n+-------------------------------------------------------------+\n```\n\n### Rules of Propagation\n\n| Rule | Description |\n|------|-------------|\n| **Catch at the right level** | Catch where you can do something useful -- recover, translate, or report |\n| **Never swallow silently** | Every catch block must log, rethrow, or return an error value |\n| **Wrap at boundaries** | When crossing a layer boundary, wrap with context from the new layer |\n| **Preserve the cause chain** | Always pass the original error as `cause` when wrapping |\n| **Transform, do not expose** | Infrastructure errors become domain errors at the service boundary |\n| **Let programmer errors crash** | Programmer errors should propagate to the global handler and crash/restart |\n\n### Propagation in Practice\n\n```typescript\n// ORIGIN: Repository throws with infrastructure context\nclass OrderRepository {\n  async findById(id: string): Promise<Order | null> {\n    try {\n      return await db.selectFrom('orders').where('id', '=', id).executeTakeFirst();\n    } catch (error) {\n      throw new ExternalServiceError('database', `Failed to fetch order ${id}`, {\n        cause: error as Error,\n        context: { orderId: id, operation: 'findById' },\n      });\n    }\n  }\n}\n\n// SERVICE: Wraps infrastructure error into domain context\nclass OrderService {\n  async getOrder(id: string, requestingUserId: string): Promise<Order> {\n    const order = await this.orderRepo.findById(id);\n    // ExternalServiceError from repo propagates up -- intentional\n\n    if (!order) {\n      throw new NotFoundError('Order', id);\n    }\n\n    if (order.userId !== requestingUserId) {\n      throw new ForbiddenError('You can only view your own orders', {\n        context: { orderId: id, requestingUserId },\n      });\n    }\n\n    return order;\n  }\n}\n\n// CONTROLLER: Formats for API consumer\nclass OrderController {\n  getById = async (req: Request, res: Response, next: NextFunction) => {\n    try {\n      const order = await this.orderService.getOrder(req.params.id, req.user.id);\n      res.json({ data: order });\n    } catch (error) {\n      next(error); // Delegate to global error handler\n    }\n  };\n}\n\n// GLOBAL HANDLER: Safety net (see Step 7: Boundary Error Handling)\n```\n\n### Anti-Patterns to Avoid\n\n```typescript\n// ANTI-PATTERN 1: Swallowing errors\ntry {\n  await sendEmail(user.email);\n} catch (error) {\n  // Silently ignored -- email failures are invisible\n}\n\n// FIX: Log even if not rethrowing\ntry {\n  await sendEmail(user.email);\n} catch (error) {\n  logger.warn('Email send failed', { email: user.email, error: (error as Error).message });\n  // Intentionally not rethrowing -- email is non-critical\n}\n\n// ANTI-PATTERN 2: Losing context\ntry {\n  await externalApi.call(payload);\n} catch (error) {\n  throw new Error('API call failed'); // Original error lost\n}\n\n// FIX: Preserve cause chain\ntry {\n  await externalApi.call(payload);\n} catch (error) {\n  throw new ExternalServiceError('payment-api', 'Payment processing failed', {\n    cause: error as Error,\n    context: { payload },\n  });\n}\n\n// ANTI-PATTERN 3: Catching too broadly\ntry {\n  const data = JSON.parse(untrustedInput); // Could throw SyntaxError\n  const result = processData(data);          // Could throw BusinessRuleError\n  await saveResult(result);                  // Could throw ExternalServiceError\n} catch (error) {\n  // Which operation failed? Cannot tell.\n  res.status(500).json({ error: 'Something went wrong' });\n}\n\n// FIX: Granular try/catch or let errors propagate to appropriate handler\n```\n\n## Step 4: Implement Recovery Patterns\n\nRecovery patterns turn transient failures into successful operations and contain blast radius for permanent failures.\n\n### Pattern Selection Guide\n\n| Situation | Pattern | When |\n|-----------|---------|------|\n| Transient external failure | **Retry with backoff** | Network blip, temporary overload |\n| Repeated failures to same service | **Circuit breaker** | Downstream service degraded |\n| Primary path unavailable | **Fallback** | Cache stale data, use default, degrade feature |\n| Protecting shared resources | **Bulkhead** | Isolate failures to prevent cascade |\n| Nonessential operation | **Fire and forget** | Analytics, logging, notifications |\n| Long-running operation | **Timeout** | Prevent indefinite blocking |\n\n### Retry with Exponential Backoff\n\n```typescript\n// src/lib/retry.ts\n\nexport interface RetryOptions {\n  /** Maximum number of attempts (including the first) */\n  maxAttempts: number;\n  /** Base delay in milliseconds */\n  baseDelayMs: number;\n  /** Maximum delay cap in milliseconds */\n  maxDelayMs: number;\n  /** Multiplier for exponential backoff (default: 2) */\n  backoffMultiplier?: number;\n  /** Add random jitter to prevent thundering herd (default: true) */\n  jitter?: boolean;\n  /** Predicate to decide if the error is retryable */\n  isRetryable?: (error: Error) => boolean;\n  /** Called before each retry with attempt number and delay */\n  onRetry?: (attempt: number, delayMs: number, error: Error) => void;\n}\n\nexport async function withRetry<T>(\n  fn: () => Promise<T>,\n  options: RetryOptions,\n): Promise<T> {\n  const {\n    maxAttempts,\n    baseDelayMs,\n    maxDelayMs,\n    backoffMultiplier = 2,\n    jitter = true,\n    isRetryable = () => true,\n    onRetry,\n  } = options;\n\n  let lastError: Error | undefined;\n\n  for (let attempt = 1; attempt <= maxAttempts; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      lastError = error as Error;\n\n      if (attempt === maxAttempts || !isRetryable(lastError)) {\n        throw lastError;\n      }\n\n      // Exponential backoff: baseDelay * multiplier^(attempt-1)\n      let delay = Math.min(\n        baseDelayMs * Math.pow(backoffMultiplier, attempt - 1),\n        maxDelayMs,\n      );\n\n      // Add jitter: randomize between 0 and computed delay\n      if (jitter) {\n        delay = Math.floor(Math.random() * delay);\n      }\n\n      onRetry?.(attempt, delay, lastError);\n      await sleep(delay);\n    }\n  }\n\n  throw lastError!;\n}\n\nfunction sleep(ms: number): Promise<void> {\n  return new Promise((resolve) => setTimeout(resolve, ms));\n}\n\n// Usage\nconst result = await withRetry(\n  () => paymentApi.charge(userId, amountCents),\n  {\n    maxAttempts: 3,\n    baseDelayMs: 500,\n    maxDelayMs: 5000,\n    isRetryable: (error) =>\n      error instanceof ExternalServiceError ||\n      error instanceof TimeoutError,\n    onRetry: (attempt, delay, error) => {\n      logger.warn('Retrying payment charge', { attempt, delay, error: error.message });\n    },\n  },\n);\n```\n\n### Circuit Breaker\n\n```typescript\n// src/lib/circuitBreaker.ts\n\nenum CircuitState {\n  CLOSED = 'closed',       // Normal operation -- requests pass through\n  OPEN = 'open',           // Failing -- requests rejected immediately\n  HALF_OPEN = 'half_open', // Testing -- single request allowed to probe\n}\n\nexport interface CircuitBreakerOptions {\n  /** Number of failures before opening the circuit */\n  failureThreshold: number;\n  /** Time in ms to wait before moving from OPEN to HALF_OPEN */\n  resetTimeoutMs: number;\n  /** Number of successes in HALF_OPEN before closing */\n  successThreshold: number;\n  /** Name for logging and metrics */\n  name: string;\n  /** Predicate to classify which errors count as failures */\n  isFailure?: (error: Error) => boolean;\n}\n\nexport class CircuitBreaker {\n  private state: CircuitState = CircuitState.CLOSED;\n  private failureCount = 0;\n  private successCount = 0;\n  private lastFailureTime = 0;\n\n  constructor(private readonly options: CircuitBreakerOptions) {}\n\n  async execute<T>(fn: () => Promise<T>): Promise<T> {\n    if (this.state === CircuitState.OPEN) {\n      if (Date.now() - this.lastFailureTime >= this.options.resetTimeoutMs) {\n        this.state = CircuitState.HALF_OPEN;\n        this.successCount = 0;\n        logger.info(`Circuit ${this.options.name}: OPEN -> HALF_OPEN`);\n      } else {\n        throw new ExternalServiceError(\n          this.options.name,\n          `Circuit breaker is open for ${this.options.name}`,\n        );\n      }\n    }\n\n    try {\n      const result = await fn();\n      this.onSuccess();\n      return result;\n    } catch (error) {\n      const shouldCountFailure = this.options.isFailure?.(error as Error) ?? true;\n      if (shouldCountFailure) {\n        this.onFailure();\n      }\n      throw error;\n    }\n  }\n\n  private onSuccess(): void {\n    if (this.state === CircuitState.HALF_OPEN) {\n      this.successCount++;\n      if (this.successCount >= this.options.successThreshold) {\n        this.state = CircuitState.CLOSED;\n        this.failureCount = 0;\n        logger.info(`Circuit ${this.options.name}: HALF_OPEN -> CLOSED`);\n      }\n    } else {\n      this.failureCount = 0;\n    }\n  }\n\n  private onFailure(): void {\n    this.failureCount++;\n    this.lastFailureTime = Date.now();\n\n    if (\n      this.failureCount >= this.options.failureThreshold ||\n      this.state === CircuitState.HALF_OPEN\n    ) {\n      this.state = CircuitState.OPEN;\n      logger.warn(`Circuit ${this.options.name}: -> OPEN after ${this.failureCount} failures`);\n    }\n  }\n\n  getState(): CircuitState {\n    return this.state;\n  }\n}\n\n// Usage\nconst paymentCircuit = new CircuitBreaker({\n  name: 'payment-service',\n  failureThreshold: 5,\n  resetTimeoutMs: 30_000,\n  successThreshold: 2,\n  isFailure: (error) => error instanceof ExternalServiceError,\n});\n\nconst result = await paymentCircuit.execute(() => paymentApi.charge(userId, amount));\n```\n\n### Fallback Pattern\n\n```typescript\n// src/lib/fallback.ts\n\nexport async function withFallback<T>(\n  primary: () => Promise<T>,\n  fallback: () => Promise<T>,\n  options?: { onFallback?: (error: Error) => void },\n): Promise<T> {\n  try {\n    return await primary();\n  } catch (error) {\n    options?.onFallback?.(error as Error);\n    return await fallback();\n  }\n}\n\n// Usage: Cache fallback when API is down\nconst products = await withFallback(\n  () => productApi.listFeatured(),\n  () => cache.get<Product[]>('featured-products') ?? [],\n  {\n    onFallback: (error) => {\n      logger.warn('Product API unavailable, serving cached data', {\n        error: error.message,\n      });\n    },\n  },\n);\n```\n\n### Bulkhead Pattern\n\n```typescript\n// src/lib/bulkhead.ts\n\nexport class Bulkhead {\n  private activeCount = 0;\n  private queue: Array<{ resolve: () => void; reject: (err: Error) => void }> = [];\n\n  constructor(\n    private readonly maxConcurrent: number,\n    private readonly maxQueue: number,\n    private readonly name: string,\n  ) {}\n\n  async execute<T>(fn: () => Promise<T>): Promise<T> {\n    if (this.activeCount >= this.maxConcurrent) {\n      if (this.queue.length >= this.maxQueue) {\n        throw new ExternalServiceError(\n          this.name,\n          `Bulkhead ${this.name} at capacity: ${this.maxConcurrent} active, ${this.maxQueue} queued`,\n        );\n      }\n      await new Promise<void>((resolve, reject) => {\n        this.queue.push({ resolve, reject });\n      });\n    }\n\n    this.activeCount++;\n    try {\n      return await fn();\n    } finally {\n      this.activeCount--;\n      const next = this.queue.shift();\n      if (next) next.resolve();\n    }\n  }\n}\n\n// Usage: Isolate payment processing from order queries\nconst paymentBulkhead = new Bulkhead(10, 50, 'payment');\nconst orderQueryBulkhead = new Bulkhead(50, 200, 'order-query');\n```\n\n### Recovery Checklist\n\n```markdown\n- [ ] Every external call has a timeout configured\n- [ ] Transient failures use retry with exponential backoff and jitter\n- [ ] Retry logic has maxAttempts cap (never retry indefinitely)\n- [ ] Only retryable errors trigger retry (validation errors do not)\n- [ ] Critical external services use circuit breakers\n- [ ] Fallback paths exist for non-critical features\n- [ ] Shared resources are protected by bulkheads\n- [ ] Recovery events are logged for observability\n```\n\n## Step 5: Add Logging and Observability\n\nErrors that are not logged are invisible. Errors without context are useless.\n\n### Structured Error Logging\n\n```typescript\n// src/lib/logger.ts\n\ninterface LogContext {\n  correlationId?: string;\n  userId?: string;\n  operation?: string;\n  duration?: number;\n  [key: string]: unknown;\n}\n\nclass Logger {\n  error(message: string, error: Error, context?: LogContext): void {\n    const logEntry = {\n      level: 'error',\n      timestamp: new Date().toISOString(),\n      message,\n      error: {\n        name: error.name,\n        message: error.message,\n        stack: error.stack,\n        ...(error instanceof AppError ? error.toLogObject() : {}),\n      },\n      ...context,\n    };\n    console.error(JSON.stringify(logEntry));\n  }\n\n  warn(message: string, context?: LogContext): void {\n    const logEntry = {\n      level: 'warn',\n      timestamp: new Date().toISOString(),\n      message,\n      ...context,\n    };\n    console.warn(JSON.stringify(logEntry));\n  }\n\n  info(message: string, context?: LogContext): void {\n    const logEntry = {\n      level: 'info',\n      timestamp: new Date().toISOString(),\n      message,\n      ...context,\n    };\n    console.info(JSON.stringify(logEntry));\n  }\n}\n\nexport const logger = new Logger();\n```\n\n### Log Level Guidelines\n\n| Level | When | Example |\n|-------|------|---------|\n| **DEBUG** | Diagnostic detail for development | \"Retrying request, attempt 2 of 3\" |\n| **INFO** | Normal operational events | \"Order created\", \"Payment processed\" |\n| **WARN** | Recoverable issues, degraded operation | \"Using cached data, API unavailable\" |\n| **ERROR** | Operation failed, needs attention | \"Payment charge failed after 3 retries\" |\n| **FATAL** | System cannot continue | \"Database connection pool exhausted\" |\n\n### Correlation IDs\n\n```typescript\n// src/middleware/correlationId.ts\nimport { randomUUID } from 'crypto';\nimport { AsyncLocalStorage } from 'async_hooks';\n\nconst correlationStorage = new AsyncLocalStorage<string>();\n\nexport function correlationMiddleware(req: Request, _res: Response, next: NextFunction): void {\n  const correlationId = (req.headers['x-correlation-id'] as string) ?? randomUUID();\n  req.correlationId = correlationId;\n  correlationStorage.run(correlationId, () => next());\n}\n\nexport function getCorrelationId(): string | undefined {\n  return correlationStorage.getStore();\n}\n\n// Attach to outgoing requests\nexport function withCorrelation(headers: Record<string, string>): Record<string, string> {\n  const id = getCorrelationId();\n  return id ? { ...headers, 'x-correlation-id': id } : headers;\n}\n```\n\n### Error Metrics\n\nTrack these metrics for alerting and dashboards:\n\n| Metric | Type | Purpose |\n|--------|------|---------|\n| `error_total{code, severity}` | Counter | Total errors by type |\n| `error_rate_5m{service}` | Gauge | Error rate over 5-minute window |\n| `circuit_breaker_state{name}` | Gauge | Current state (0=closed, 1=half_open, 2=open) |\n| `retry_total{operation, outcome}` | Counter | Retry attempts and outcomes |\n| `recovery_fallback_total{name}` | Counter | How often fallback paths are used |\n\n### Observability Checklist\n\n```markdown\n- [ ] All errors logged with structured JSON\n- [ ] Correlation IDs flow through all service calls\n- [ ] Log levels match severity (not everything is ERROR)\n- [ ] Sensitive data (passwords, tokens, PII) stripped from logs\n- [ ] Error metrics exported for dashboards and alerting\n- [ ] Alert thresholds defined for error rate spikes\n- [ ] Circuit breaker state changes logged\n- [ ] Retry attempts and outcomes logged\n```\n\n## Step 6: Craft User-Facing Messages\n\nUsers deserve helpful error messages. Internal details must never leak.\n\n### Message Guidelines\n\n| Principle | Bad | Good |\n|-----------|-----|------|\n| **Be specific** | \"An error occurred\" | \"We could not process your payment\" |\n| **Be actionable** | \"Error 500\" | \"Please try again in a few minutes\" |\n| **No internals** | \"NullReferenceException at line 42\" | \"Something went wrong on our end\" |\n| **No blame** | \"You entered an invalid email\" | \"Please enter a valid email address\" |\n| **Suggest next steps** | \"Payment failed\" | \"Payment failed. Please check your card details or try a different payment method.\" |\n\n### Error Message Map\n\n```typescript\n// src/errors/userMessages.ts\n\nconst USER_MESSAGES: Record<string, { title: string; message: string; action?: string }> = {\n  VALIDATION_ERROR: {\n    title: 'Invalid input',\n    message: 'Please check the highlighted fields and try again.',\n  },\n  NOT_FOUND: {\n    title: 'Not found',\n    message: 'The item you are looking for does not exist or has been removed.',\n  },\n  FORBIDDEN: {\n    title: 'Access denied',\n    message: 'You do not have permission to perform this action.',\n    action: 'Contact your administrator if you believe this is an error.',\n  },\n  PAYMENT_FAILED: {\n    title: 'Payment failed',\n    message: 'We could not process your payment.',\n    action: 'Please check your card details or try a different payment method.',\n  },\n  RATE_LIMITED: {\n    title: 'Too many requests',\n    message: 'You are making requests too quickly.',\n    action: 'Please wait a moment and try again.',\n  },\n  EXTERNAL_SERVICE_ERROR: {\n    title: 'Service temporarily unavailable',\n    message: 'One of our services is experiencing issues.',\n    action: 'Please try again in a few minutes.',\n  },\n  TIMEOUT: {\n    title: 'Request timed out',\n    message: 'The operation took too long to complete.',\n    action: 'Please try again. If the problem persists, contact support.',\n  },\n  INTERNAL_ERROR: {\n    title: 'Something went wrong',\n    message: 'An unexpected error occurred on our end.',\n    action: 'Please try again. If the problem persists, contact support.',\n  },\n};\n\nexport function getUserMessage(code: string): { title: string; message: string; action?: string } {\n  return USER_MESSAGES[code] ?? USER_MESSAGES['INTERNAL_ERROR'];\n}\n```\n\n### API Error Response Format\n\n```typescript\n// Standard error response shape\ninterface ApiErrorResponse {\n  error: {\n    code: string;            // Machine-readable: 'VALIDATION_ERROR'\n    message: string;         // User-safe message\n    action?: string;         // Suggested next step\n    fields?: Record<string, string[]>;  // Field-level validation errors\n    requestId?: string;      // Correlation ID for support reference\n  };\n}\n\n// Example response:\n// HTTP 422\n// {\n//   \"error\": {\n//     \"code\": \"VALIDATION_ERROR\",\n//     \"message\": \"Please check the highlighted fields and try again.\",\n//     \"fields\": {\n//       \"email\": [\"Must be a valid email address\"],\n//       \"quantity\": [\"Must be between 1 and 9999\"]\n//     },\n//     \"requestId\": \"req_abc123\"\n//   }\n// }\n```\n\n## Step 7: Handle Boundary Errors\n\nEach system boundary (API, UI, background jobs) needs its own error handling strategy.\n\n### API Error Middleware\n\n```typescript\n// src/middleware/errorHandler.ts\n\nexport function errorHandler(\n  error: Error,\n  req: Request,\n  res: Response,\n  _next: NextFunction,\n): void {\n  // 1. Log the error\n  const correlationId = req.correlationId ?? 'unknown';\n\n  if (error instanceof AppError) {\n    const logLevel = error.isOperational ? 'warn' : 'error';\n    logger[logLevel]('Request error', error, {\n      correlationId,\n      method: req.method,\n      path: req.path,\n      userId: req.user?.id,\n    });\n\n    // 2. Send appropriate response\n    const userMessage = getUserMessage(error.code);\n    res.status(error.statusCode).json({\n      error: {\n        code: error.code,\n        message: userMessage.message,\n        action: userMessage.action,\n        ...(error instanceof ValidationError ? { fields: error.fieldErrors } : {}),\n        requestId: correlationId,\n      },\n    });\n  } else {\n    // Unclassified error -- treat as programmer error\n    logger.error('Unhandled error', error, {\n      correlationId,\n      method: req.method,\n      path: req.path,\n    });\n\n    res.status(500).json({\n      error: {\n        code: 'INTERNAL_ERROR',\n        message: 'An unexpected error occurred on our end.',\n        action: 'Please try again. If the problem persists, contact support.',\n        requestId: correlationId,\n      },\n    });\n  }\n}\n\n// Register as the LAST middleware\napp.use(errorHandler);\n```\n\n### Process-Level Error Handlers\n\n```typescript\n// src/bootstrap.ts\n\n// Catch unhandled promise rejections\nprocess.on('unhandledRejection', (reason: unknown) => {\n  logger.error('Unhandled promise rejection', reason as Error, {\n    type: 'unhandledRejection',\n  });\n  // In production: trigger graceful shutdown\n  // process.exit(1);\n});\n\n// Catch uncaught exceptions\nprocess.on('uncaughtException', (error: Error) => {\n  logger.error('Uncaught exception', error, {\n    type: 'uncaughtException',\n  });\n  // MUST exit -- process is in undefined state\n  process.exit(1);\n});\n\n// Graceful shutdown on SIGTERM\nprocess.on('SIGTERM', () => {\n  logger.info('SIGTERM received, starting graceful shutdown');\n  // Close server, drain connections, flush logs\n  server.close(() => {\n    logger.info('Server closed');\n    process.exit(0);\n  });\n  // Force exit after timeout\n  setTimeout(() => {\n    logger.error('Forced shutdown after timeout', new Error('Shutdown timeout'));\n    process.exit(1);\n  }, 30_000);\n});\n```\n\n### UI Error Boundaries (React)\n\n```typescript\n// src/components/ErrorBoundary.tsx\nimport { Component, type ReactNode, type ErrorInfo } from 'react';\n\ninterface ErrorBoundaryProps {\n  children: ReactNode;\n  fallback?: ReactNode | ((error: Error, reset: () => void) => ReactNode);\n  onError?: (error: Error, errorInfo: ErrorInfo) => void;\n}\n\ninterface ErrorBoundaryState {\n  error: Error | null;\n}\n\nexport class ErrorBoundary extends Component<ErrorBoundaryProps, ErrorBoundaryState> {\n  state: ErrorBoundaryState = { error: null };\n\n  static getDerivedStateFromError(error: Error): ErrorBoundaryState {\n    return { error };\n  }\n\n  componentDidCatch(error: Error, errorInfo: ErrorInfo): void {\n    // Log to error tracking service\n    logger.error('React error boundary caught error', error, {\n      componentStack: errorInfo.componentStack,\n    });\n    this.props.onError?.(error, errorInfo);\n  }\n\n  reset = (): void => {\n    this.setState({ error: null });\n  };\n\n  render(): ReactNode {\n    if (this.state.error) {\n      if (typeof this.props.fallback === 'function') {\n        return this.props.fallback(this.state.error, this.reset);\n      }\n      return this.props.fallback ?? <DefaultErrorFallback error={this.state.error} onRetry={this.reset} />;\n    }\n    return this.props.children;\n  }\n}\n\nfunction DefaultErrorFallback({ error, onRetry }: { error: Error; onRetry: () => void }) {\n  return (\n    <div role=\"alert\" className=\"error-fallback\">\n      <h2>Something went wrong</h2>\n      <p>We are sorry for the inconvenience. Please try again.</p>\n      <button onClick={onRetry}>Try again</button>\n    </div>\n  );\n}\n\n// Usage: Wrap at feature boundaries, not just the app root\n// <ErrorBoundary fallback={<CheckoutErrorView />}>\n//   <CheckoutFlow />\n// </ErrorBoundary>\n```\n\n### Background Job Error Handling\n\n```typescript\n// src/jobs/baseJob.ts\n\nexport abstract class BaseJob<T> {\n  abstract readonly name: string;\n  abstract readonly maxRetries: number;\n\n  async run(payload: T, attempt: number): Promise<void> {\n    try {\n      await this.execute(payload);\n    } catch (error) {\n      const appError = error instanceof AppError\n        ? error\n        : new InternalError('Job execution failed', { cause: error as Error });\n\n      logger.error(`Job ${this.name} failed`, appError, {\n        attempt,\n        maxRetries: this.maxRetries,\n        payload: this.sanitizePayload(payload),\n      });\n\n      if (attempt < this.maxRetries && this.isRetryable(appError)) {\n        // Re-enqueue with backoff\n        const delay = Math.min(1000 * Math.pow(2, attempt), 60_000);\n        await this.enqueue(payload, attempt + 1, delay);\n        logger.info(`Job ${this.name} re-enqueued`, { attempt: attempt + 1, delay });\n      } else {\n        // Send to dead letter queue for manual investigation\n        await this.sendToDeadLetterQueue(payload, appError, attempt);\n        logger.error(`Job ${this.name} sent to DLQ after ${attempt} attempts`, appError);\n      }\n    }\n  }\n\n  protected abstract execute(payload: T): Promise<void>;\n  protected abstract enqueue(payload: T, attempt: number, delayMs: number): Promise<void>;\n  protected abstract sendToDeadLetterQueue(payload: T, error: AppError, attempts: number): Promise<void>;\n\n  protected isRetryable(error: AppError): boolean {\n    return error.isOperational;\n  }\n\n  protected sanitizePayload(payload: T): unknown {\n    return payload; // Override to strip sensitive fields\n  }\n}\n```\n\n## Output Formats\n\n### Quick Format (Single Feature)\n\n```markdown\n## Error Handling: [Feature Name]\n\n### Error Classification\n| Error Code | Type | Transient? | Recovery |\n|------------|------|------------|----------|\n| [CODE] | operational/programmer | yes/no | retry/fallback/fail |\n\n### Custom Errors Added\n- `[ErrorClass]` -- [when thrown]\n\n### Recovery Strategy\n- [Pattern used and configuration]\n\n### User Messages\n| Code | Message | Action |\n|------|---------|--------|\n| [CODE] | [message] | [action] |\n```\n\n### Full Format (System-Wide)\n\n```markdown\n## Error Handling Strategy: [System Name]\n\n### Error Hierarchy\n[Class diagram or list of all custom errors]\n\n### Error Classification Matrix\n[Complete table of all error codes, types, severities, and recovery strategies]\n\n### Recovery Configuration\n[Retry policies, circuit breaker settings, fallback chains]\n\n### Boundary Handlers\n- **API**: [middleware description]\n- **UI**: [error boundary strategy]\n- **Jobs**: [retry and DLQ strategy]\n\n### Logging & Observability\n- [Structured logging format]\n- [Correlation ID flow]\n- [Metrics and alerting thresholds]\n\n### User Message Catalog\n[Complete mapping of error codes to user-safe messages]\n\n### Testing Strategy\n[How error handling is tested -- fault injection, chaos testing]\n```\n\n## Common Patterns\n\n### Pattern 1: API Error Handling\n\nStandard pattern for REST API error handling with middleware, validation, and consistent response format.\n\n```typescript\n// Validation at API boundary\napp.post('/orders', validate(createOrderSchema), async (req, res, next) => {\n  try {\n    const order = await orderService.create(req.body);\n    res.status(201).json({ data: order });\n  } catch (error) {\n    next(error); // Global error handler formats response\n  }\n});\n\n// Global handler produces consistent error responses\n// See Step 7: API Error Middleware\n```\n\n**Key elements:** Input validation at boundary, try/catch delegates to global handler, consistent JSON error shape, correlation IDs in every response.\n\n### Pattern 2: Background Job Errors\n\nJobs need retry with backoff, dead letter queues, and idempotency guarantees.\n\n```typescript\n// Idempotent job execution\nasync execute(payload: { orderId: string }): Promise<void> {\n  const lock = await this.acquireLock(`process-order:${payload.orderId}`);\n  if (!lock) {\n    logger.info('Job already processing, skipping', { orderId: payload.orderId });\n    return; // Idempotent -- safe to skip\n  }\n  try {\n    await this.processOrder(payload.orderId);\n  } finally {\n    await this.releaseLock(lock);\n  }\n}\n```\n\n**Key elements:** Idempotency via distributed locks, exponential backoff between retries, dead letter queue after max attempts, sanitized payload logging.\n\n### Pattern 3: UI Error Boundaries\n\nIsolate errors to feature boundaries so one broken component does not crash the entire page.\n\n```typescript\n// Feature-level isolation\n<Layout>\n  <ErrorBoundary fallback={<NavError />}>\n    <Navigation />\n  </ErrorBoundary>\n\n  <ErrorBoundary fallback={<ContentError onRetry={refetch} />}>\n    <MainContent />\n  </ErrorBoundary>\n\n  <ErrorBoundary fallback={<SidebarFallback />}>\n    <Sidebar />\n  </ErrorBoundary>\n</Layout>\n```\n\n**Key elements:** Boundaries at feature level (not just app root), meaningful fallback UI per boundary, retry mechanism, error logging to tracking service.\n\n### Pattern 4: Distributed System Errors\n\nCross-service errors need correlation, circuit breakers, and graceful degradation.\n\n```typescript\n// Composed resilience: circuit breaker + retry + fallback + timeout\nasync function getProductRecommendations(userId: string): Promise<Product[]> {\n  return withFallback(\n    () => recommendationCircuit.execute(\n      () => withRetry(\n        () => withTimeout(\n          () => recommendationApi.getForUser(userId),\n          { timeoutMs: 2000, operation: 'get-recommendations' },\n        ),\n        {\n          maxAttempts: 2,\n          baseDelayMs: 200,\n          maxDelayMs: 1000,\n          isRetryable: (e) => e instanceof TimeoutError || e instanceof ExternalServiceError,\n        },\n      ),\n    ),\n    () => cache.get<Product[]>(`recommendations:${userId}`) ?? [],\n    {\n      onFallback: (error) => {\n        logger.warn('Recommendations unavailable, serving cached', {\n          userId,\n          error: error.message,\n          correlationId: getCorrelationId(),\n        });\n      },\n    },\n  );\n}\n```\n\n**Key elements:** Timeout wraps the innermost call, retry wraps timeout, circuit breaker wraps retry, fallback wraps everything. Correlation ID flows through all layers. Degraded experience is better than no experience.\n\n## Relationship to Other Skills\n\n| Skill | Relationship |\n|-------|--------------|\n| `implement` | Error handling is integral to implementation -- every service needs classified errors, propagation strategy, and recovery |\n| `code-verification` | Verification confirms error handling exists at all boundaries and follows the defined hierarchy |\n| `test-generation` | Tests must cover every error path -- happy path is not enough without error case coverage |\n| `deploy` | Deployment must configure monitoring, alerting thresholds, and circuit breaker settings per environment |\n\n## Key Principles\n\n**Classify before you catch.** Know whether an error is operational or a bug, transient or permanent. The classification determines the handling strategy.\n\n**Errors are data, not just strings.** Every error should carry a code, severity, context, and correlation ID. Strings alone cannot drive automated recovery or alerting.\n\n**Recover where possible, fail fast where necessary.** Transient failures deserve retry and fallback. Programmer errors should crash the process -- do not mask bugs with catch blocks.\n\n**Never swallow, never expose.** Every catch block must either recover, log and rethrow, or report to the user. But never expose stack traces, internal codes, or infrastructure details to end users.\n\n**Contain the blast radius.** Use circuit breakers, bulkheads, and error boundaries to prevent one failure from cascading across the system. A broken recommendation engine should not prevent checkout.\n\n**Observe everything.** Errors without structured logging, correlation IDs, and metrics are invisible. You cannot fix what you cannot see.\n\n## References\n\n- `references/error-classification.md`: Operational vs programmer error taxonomy, severity levels, and classification decision trees\n- `references/recovery-strategies.md`: Retry, fallback, circuit breaker, and bulkhead patterns with configuration guidance\n- `references/logging-standards.md`: Structured logging format, log levels, sensitive data handling, and correlation ID propagation\n- `references/retry-patterns.md`: Exponential backoff with jitter, idempotency requirements, and retry budget management\n- `references/circuit-breaker-patterns.md`: Circuit breaker state machine, configuration tuning, and bulkhead isolation strategies",
  "references": [
    {
      "name": "circuit-breaker-patterns.md",
      "path": "references/circuit-breaker-patterns.md",
      "content": "# Circuit Breaker Patterns Reference\n\n## State Machine\n\nA circuit breaker wraps calls to an external dependency and tracks failures\nto prevent cascading outages. It has three states:\n\n```\n                 failure threshold\n                    exceeded\n    +--------+    ----------->    +--------+\n    | CLOSED |                    |  OPEN  |\n    +--------+    <-----------    +--------+\n         ^         probe fails         |\n         |                             | timeout expires\n         |                             v\n         |                       +-----------+\n         +-------- success ------|  HALF-OPEN |\n                   threshold     +-----------+\n```\n\n### CLOSED (Normal Operation)\n- All requests pass through to the dependency\n- Failures are counted within a sliding window\n- Transitions to OPEN when failure threshold is exceeded\n\n### OPEN (Failing Fast)\n- All requests immediately rejected without calling the dependency\n- Returns a fallback response or error\n- Transitions to HALF-OPEN when the timeout expires\n\n### HALF-OPEN (Testing Recovery)\n- A limited number of probe requests are allowed through\n- If probes succeed: transition to CLOSED (dependency recovered)\n- If probes fail: transition back to OPEN (still broken)\n\n## Configuration Parameters\n\n### Failure Threshold\n\n| Parameter | Typical value | Notes |\n|-----------|--------------|-------|\n| failureRate | 50% | Percentage of calls that failed |\n| slidingWindow | 60 seconds | Time window for counting failures |\n| minimumCalls | 10 | Don't open on low volume (avoid false positives) |\n\n**Recommendation:** Use failure rate with a minimum call count. 3 failures out of 5\ncalls is concerning; 3 failures out of 5000 is noise.\n\n### Open State Timeout\n\n| Dependency type | Timeout | Rationale |\n|----------------|---------|-----------|\n| Internal microservice | 15-30s | Fast restart, container orchestration |\n| External API | 30-60s | Less control over recovery time |\n| Database | 10-20s | Failover is usually fast |\n| Third-party SaaS | 60-120s | Unpredictable recovery |\n\n### Half-Open Configuration\n\n| Parameter | Typical value | Notes |\n|-----------|--------------|-------|\n| probeCount | 3 | Number of requests to test |\n| successThreshold | 2 out of 3 | Required successes to close |\n| probeTimeout | 5s | Shorter timeout for probes |\n\n## Implementation Pattern\n\n```javascript\nclass CircuitBreaker {\n  constructor(options) {\n    this.state = 'CLOSED';\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.failureThreshold = options.failureThreshold || 5;\n    this.successThreshold = options.successThreshold || 2;\n    this.openTimeout = options.openTimeout || 30000;\n  }\n\n  async call(fn, fallback) {\n    if (this.state === 'OPEN') {\n      return fallback ? fallback() : Promise.reject(new CircuitOpenError());\n    }\n    try {\n      const result = await fn();\n      this.onSuccess();\n      return result;\n    } catch (error) {\n      this.onFailure();\n      if (fallback) return fallback();\n      throw error;\n    }\n  }\n\n  transitionTo(newState) {\n    const prev = this.state;\n    this.state = newState;\n    this.failureCount = 0;\n    this.successCount = 0;\n    if (newState === 'OPEN') {\n      setTimeout(() => this.transitionTo('HALF_OPEN'), this.openTimeout);\n    }\n    this.emit('stateChange', { from: prev, to: newState });\n  }\n}\n```\n\n## Bulkhead Isolation Strategies\n\nBulkheads prevent one dependency from consuming all shared resources.\n\n| Strategy | Mechanism | Use case |\n|----------|-----------|----------|\n| Semaphore | Limit concurrent calls per dependency | Lightweight isolation |\n| Connection pool | Separate pools per downstream | Database access |\n| Queue | Separate queues per priority/tenant | Async processing |\n| Process | Separate processes per workload | Maximum isolation |\n\n**Sizing rule:** Base pool size on the dependency's capacity, not the caller's demand.\n\n## Monitoring and Alerting\n\n### Metrics to Track\n\n| Metric | Alert threshold | Meaning |\n|--------|----------------|---------|\n| Circuit state | Any OPEN event | Dependency failure detected |\n| Time in OPEN | > 5 minutes | Sustained outage, needs intervention |\n| State transitions/hour | > 10 | Flapping, intermittent failure |\n| Rejected requests | Rate > baseline | Users are impacted |\n| Fallback rate | > 20% of requests | Degraded service quality |\n\n### Dashboard Essentials\n\nEvery circuit breaker should expose:\n1. Current state (CLOSED / OPEN / HALF-OPEN) with color indicator\n2. Failure rate over sliding window (line chart)\n3. State transition history (timeline)\n4. Fallback activation rate (percentage)\n5. P99 latency (should decrease when circuit is open)\n\n## Anti-Patterns\n\n1. **Single global circuit breaker**: One circuit for all dependencies; one failure opens everything\n2. **Too sensitive threshold**: Opens on 2 failures; flaps on normal error rates\n3. **Too long open timeout**: 10-minute timeout delays recovery detection\n4. **No fallback on OPEN**: Returns raw errors to users instead of degraded response\n5. **No monitoring**: Circuit silently opens and closes; operators never know\n"
    },
    {
      "name": "error-classification.md",
      "path": "references/error-classification.md",
      "content": "# Error Classification Reference\n\n## Two Fundamental Categories\n\n### Operational Errors\nRuntime problems that occur in correctly written programs. These are expected failure modes\nthat the system must handle gracefully.\n\n| Type | Examples | Response |\n|------|----------|----------|\n| Network | Timeout, DNS failure, connection refused | Retry with backoff |\n| Resource | Disk full, memory exhausted, file locked | Alert + degrade gracefully |\n| Dependency | Database down, API 503, queue full | Circuit breaker + fallback |\n| Input | Malformed request, invalid JSON, missing field | Validate + reject with details |\n| Permission | Auth expired, insufficient privileges, rate limited | Re-auth or propagate 403/429 |\n\n### Programmer Errors\nBugs in the code itself. These should never be \"handled\" -- they should be fixed.\n\n| Type | Examples | Response |\n|------|----------|----------|\n| Type | Calling method on undefined, wrong argument type | Crash + fix code |\n| Logic | Off-by-one, infinite loop, race condition | Crash + fix code |\n| Contract | Violating API contract, missing required config | Crash + fix code |\n| Assertion | Failed invariant, impossible state reached | Crash + fix code |\n\n**Key rule:** Never retry programmer errors. Never add fallbacks for bugs. Fix them.\n\n## Severity Levels\n\n### Critical (Immediate action required)\n- System is down or data integrity is at risk\n- Revenue-impacting failure in production\n- Security breach or credential exposure\n- Escalation: Page on-call immediately\n- Example: Database corruption, payment processing failure, auth bypass\n\n### High (Action required within 1 hour)\n- Major feature is broken for all users\n- Dependency failure with no fallback\n- Error rate exceeds 5% of requests\n- Escalation: Alert team channel, assign owner\n- Example: Search service down, email delivery failing, cache cluster unreachable\n\n### Medium (Action required within 24 hours)\n- Feature degraded but functional\n- Fallback is active and handling load\n- Error rate between 1-5% of requests\n- Escalation: Create ticket, schedule fix\n- Example: Slow query degrading response time, non-critical API returning stale data\n\n### Low (Track and batch)\n- Cosmetic or minor UX impact\n- Single user affected, workaround exists\n- Error rate below 1%\n- Escalation: Log for weekly review\n- Example: Tooltip not rendering, pagination off-by-one on edge case\n\n## Classification Decision Tree\n\n```\nError occurred\n  |\n  +-- Is this a bug in our code?\n  |     YES -> Programmer Error -> Crash, log, fix\n  |     NO  -> Operational Error\n  |              |\n  |              +-- Is the system usable?\n  |              |     NO  -> Is data at risk?\n  |              |     |       YES -> CRITICAL\n  |              |     |       NO  -> HIGH\n  |              |     YES -> Is a fallback active?\n  |              |             YES -> MEDIUM\n  |              |             NO  -> Is user impacted?\n  |              |                     YES -> HIGH\n  |              |                     NO  -> LOW\n```\n\n## Transient vs Persistent Classification\n\n**Transient errors** resolve on their own. Retry is appropriate.\n- Network timeout (server was momentarily busy)\n- HTTP 429 (rate limit will reset)\n- HTTP 503 (service restarting)\n- Lock contention (other transaction will complete)\n\n**Persistent errors** will not resolve without intervention. Do not retry.\n- HTTP 400 (bad request won't become valid)\n- HTTP 401/403 (credentials won't fix themselves)\n- HTTP 404 (resource doesn't exist)\n- Schema validation failure (data shape is wrong)\n- Disk full (needs cleanup, not retries)\n\n## Error Code Conventions\n\nUse structured error codes for programmatic handling:\n\n```\nFormat: DOMAIN.CATEGORY.SPECIFIC\n\nExamples:\n  AUTH.TOKEN.EXPIRED      -> Re-authenticate\n  AUTH.TOKEN.INVALID      -> Reject, do not retry\n  DB.CONNECTION.TIMEOUT   -> Retry with backoff\n  DB.QUERY.SYNTAX         -> Programmer error, fix query\n  API.RATE.EXCEEDED       -> Back off, respect Retry-After\n  API.INPUT.VALIDATION    -> Return 400 with field details\n  STORAGE.DISK.FULL       -> Alert ops, degrade gracefully\n  QUEUE.PUBLISH.TIMEOUT   -> Retry, then dead-letter\n```\n\n## Classification Anti-Patterns\n\n1. **Catch-all suppression**: `catch(e) {}` -- silently swallowing errors hides bugs\n2. **Over-retrying**: Retrying 404s or validation errors wastes resources\n3. **Wrong severity**: Logging a database outage as \"warning\" delays response\n4. **Missing context**: Logging \"error occurred\" without the error object or stack trace\n5. **Treating bugs as operational**: Adding retry logic around null pointer exceptions\n"
    },
    {
      "name": "logging-standards.md",
      "path": "references/logging-standards.md",
      "content": "# Logging Standards Reference\n\n## Structured Logging Format\n\nAll logs MUST be structured JSON. Unstructured string logs are not searchable,\nnot parseable, and not useful at scale.\n\n**Required fields in every log entry:**\n\n```json\n{\n  \"timestamp\": \"2025-01-15T14:32:01.123Z\",\n  \"level\": \"error\",\n  \"message\": \"Database connection timeout\",\n  \"service\": \"user-api\",\n  \"correlationId\": \"req-a1b2c3d4\",\n  \"environment\": \"production\"\n}\n```\n\n**Additional fields for errors:** Include `error` object (name, message, code, stack)\nand `context` object (relevant parameters like database name, attempt number, timeout).\n\n## Log Level Guidelines\n\n### FATAL\nSystem cannot continue. Process will exit.\n- Unrecoverable startup failure (missing required config, port in use)\n- Data corruption detected\n- **Action:** Pages on-call, process restarts\n\n### ERROR\nOperation failed. Requires attention but system continues.\n- Dependency call failed after all retries exhausted\n- Circuit breaker opened\n- **Action:** Alert team channel, create incident if sustained\n\n### WARN\nUnexpected situation that the system handled. May indicate a growing problem.\n- Fallback activated, retry attempt, approaching resource limits\n- **Action:** Review in daily triage\n\n### INFO\nNormal operational events. The narrative of what the system is doing.\n- Request completed (with duration), service started/stopped, job finished\n- **Action:** None. Used for auditing and debugging.\n\n### DEBUG\nDetailed information for development and troubleshooting.\n- SQL queries, HTTP bodies, cache hit/miss, state transitions\n- **Action:** None. Disabled in production by default.\n\n**Rule of thumb:** If you need to add a log to debug production, it should be\nINFO or WARN, not DEBUG.\n\n## Sensitive Data Handling\n\n### Never log directly:\n- Passwords, API keys, tokens, secrets\n- Credit card numbers, SSNs, government IDs\n- Full email addresses, phone numbers (GDPR contexts)\n- Session tokens, JWTs (log a hash or last 4 chars)\n\n### PII Redaction\n\n```\n// Before: NEVER do this\nlog.info({ email: user.email, token: authToken }, 'User authenticated');\n\n// After: Redact sensitive fields\nlog.info({\n  email: redact(user.email),        // \"j***@example.com\"\n  tokenSuffix: authToken.slice(-4), // \"...x7f2\"\n  userId: user.id                   // Use opaque ID instead\n}, 'User authenticated');\n```\n\n### Credential Masking\n\nImplement at the logger level, not at each call site:\n\n```\nconst SENSITIVE_KEYS = [\n  'password', 'secret', 'token', 'apiKey', 'authorization',\n  'cookie', 'ssn', 'creditCard', 'cardNumber'\n];\n// Deep clone and replace sensitive values with '[REDACTED]'\n// Applied automatically by the logger before serialization\n```\n\n## Correlation ID Propagation\n\nEvery request gets a correlation ID at the edge. It flows through every service call.\n\n```\nClient -> API Gateway -> Service A -> Service B -> Database\n           |                |            |\n           correlationId    correlationId correlationId\n           (generated)      (propagated)  (propagated)\n```\n\n**Implementation rules:**\n1. Generate at the edge: `X-Correlation-ID: ${uuid()}` if not present\n2. Extract from incoming headers in every service\n3. Include in all outgoing HTTP calls, queue messages, and log entries\n4. Store in async context (AsyncLocalStorage in Node.js, Context in Go)\n5. Return in response headers for client-side debugging\n\n## Contextual Logging\n\nAttach request context to every log within a request lifecycle:\n\n```\nconst requestLogger = logger.child({\n  correlationId: req.headers['x-correlation-id'],\n  method: req.method,\n  path: req.path,\n  userId: req.auth?.userId\n});\n// All subsequent logs include the context automatically\nrequestLogger.info('Processing order');\n```\n\n## Log Retention and Rotation\n\n| Environment | Retention | Rotation |\n|-------------|-----------|----------|\n| Development | 7 days | 100MB per file |\n| Staging | 14 days | Daily |\n| Production (INFO+) | 30 days hot, 90 days cold | Hourly |\n| Production (ERROR+) | 90 days hot, 1 year cold | Daily |\n| Audit logs | 7 years | Daily, immutable storage |\n\n**Rotation rules:** Rotate on size (100MB) OR time (daily), whichever first.\nCompress rotated files. Delete after retention period automatically.\n\n## Anti-Patterns\n\n1. **Log and throw**: Logging then re-throwing causes duplicate entries\n2. **String concatenation**: `log('Error: ' + err)` loses structure and context\n3. **Logging in loops**: Thousands of identical logs overwhelm aggregators\n4. **Missing error object**: `log.error('Failed')` without the actual error\n5. **Logging secrets**: Even at DEBUG level, secrets in logs are a security incident\n"
    },
    {
      "name": "recovery-strategies.md",
      "path": "references/recovery-strategies.md",
      "content": "# Recovery Strategies Reference\n\n## Overview of Recovery Patterns\n\nFour core patterns form the foundation of resilient systems. Each addresses a different\nfailure mode. Most production systems combine multiple patterns.\n\n## Pattern 1: Retry\n\nRe-attempt a failed operation, assuming the failure is transient.\n\n**When to use:**\n- Network timeouts or temporary connectivity loss\n- HTTP 429, 503, or 502 responses\n- Database connection pool exhaustion\n- Queue publish failures\n\n**When NOT to use:**\n- HTTP 400, 401, 403, 404 (non-transient)\n- Validation errors or malformed input\n- Business logic violations\n- Any programmer error\n\n**Configuration:**\n- Max retries: 3-5 for network calls, 1-2 for expensive operations\n- Backoff: Exponential with jitter (see retry-patterns.md)\n- Timeout per attempt: Set lower than total operation budget\n- Idempotency: Required for any retried write operation\n\n## Pattern 2: Fallback\n\nProvide degraded but functional service when the primary path fails.\n\n**When to use:**\n- Non-critical features that enhance but aren't essential\n- Read operations where stale data is acceptable\n- Services with multiple data sources\n- UI features that can gracefully degrade\n\n**Fallback hierarchy (try in order):**\n1. Cache -- return last known good value\n2. Default -- return a safe static default\n3. Alternative service -- call a backup provider\n4. Degraded mode -- disable the feature, inform the user\n\n**Configuration:**\n- Cache TTL: Match staleness tolerance (5min for prices, 1hr for profiles)\n- Default values: Must be safe and obvious (empty list, not null)\n- Fallback timeout: Should be fast (100-500ms), otherwise defeats purpose\n- Feature flags: Use flags to toggle fallback behavior without deploys\n\n**Example:**\n```\nasync function getRecommendations(userId) {\n  try {\n    return await recommendationService.fetch(userId);    // Primary\n  } catch (err) {\n    log.warn({ err, userId }, 'Recommendation service failed, using fallback');\n    const cached = await cache.get(`recs:${userId}`);\n    if (cached) return cached;                           // Cache fallback\n    return DEFAULT_RECOMMENDATIONS;                      // Static fallback\n  }\n}\n```\n\n## Pattern 3: Circuit Breaker\n\nStop calling a failing service to let it recover, and to protect your system from\ncascading failures. (See circuit-breaker-patterns.md for detailed state machine.)\n\n**When to use:**\n- Calling external services or APIs\n- Database queries under heavy load\n- Any dependency that can fail repeatedly\n- Protecting against cascading failures across services\n\n**When NOT to use:**\n- In-process function calls (overhead not justified)\n- Operations that must succeed (use retry + queue instead)\n- Idempotent reads where stale cache suffices (use fallback)\n\n**Configuration:**\n- Failure threshold: 5 failures in 60 seconds (tune to your SLA)\n- Open duration: 30-60 seconds (long enough for recovery)\n- Half-open probe count: 1-3 requests to test recovery\n- Monitoring: Alert when circuit opens, track open duration\n\n## Pattern 4: Bulkhead\n\nIsolate components so one failure doesn't exhaust shared resources.\n\n**When to use:**\n- Multiple services sharing a thread pool or connection pool\n- One slow dependency dragging down all requests\n- Multi-tenant systems where one tenant's load shouldn't affect others\n- Any shared resource that can be saturated\n\n**Isolation strategies:**\n\n| Strategy | Mechanism | Use case |\n|----------|-----------|----------|\n| Thread pool | Separate pools per dependency | JVM-based systems |\n| Connection pool | Separate DB pools per service | Database access |\n| Semaphore | Limit concurrent calls to a service | Lightweight isolation |\n| Queue | Separate queues per priority/tenant | Async processing |\n| Process | Separate processes per workload | Maximum isolation |\n\n**Configuration:**\n- Pool size: Based on dependency's capacity, not caller's demand\n- Queue depth: Reject early rather than queue indefinitely\n- Timeout: Per-bulkhead timeout prevents hung requests from consuming slots\n\n## Combining Patterns\n\nPatterns compose. The standard combination for external service calls:\n\n```\nRequest -> [Bulkhead] -> [Circuit Breaker] -> [Retry] -> [Fallback] -> Response\n```\n\n**Order matters.** Bulkhead outermost (resource protection), fallback innermost (response guarantee).\n\n## Recovery Strategy Selection Matrix\n\n| Failure type | Primary strategy | Secondary strategy |\n|-------------|------------------|-------------------|\n| Transient network error | Retry with backoff | Fallback to cache |\n| Service outage | Circuit breaker | Fallback to default |\n| Resource exhaustion | Bulkhead isolation | Shed load (429) |\n| Slow dependency | Timeout + circuit breaker | Fallback to cache |\n| Data corruption | Stop processing | Alert + manual review |\n| Configuration error | Fail fast | Use last known good config |\n\n## Anti-Patterns\n\n1. **Retry storms**: All clients retry simultaneously, amplifying the outage\n2. **Fallback to same failure**: Fallback calls the same broken dependency\n3. **No timeout on fallback**: Fallback itself hangs, defeating the purpose\n4. **Ignoring partial failure**: Returning empty results without indicating degradation\n5. **Recovery without observability**: Recovering silently so operators never know it happened\n"
    },
    {
      "name": "retry-patterns.md",
      "path": "references/retry-patterns.md",
      "content": "# Retry Patterns Reference\n\n## Exponential Backoff with Jitter\n\nThe standard retry algorithm for distributed systems. Simple exponential backoff causes\n\"thundering herd\" when many clients retry in lockstep. Jitter breaks synchronization.\n\n### Formula\n\n```\ndelay = min(maxDelay, baseDelay * 2^attempt) * random(0.5, 1.5)\n```\n\n- `baseDelay`: Starting delay (typically 100-500ms)\n- `attempt`: Zero-indexed retry count (0, 1, 2, ...)\n- `maxDelay`: Cap to prevent unbounded waits (typically 30-60s)\n- `random(0.5, 1.5)`: Jitter factor, randomizes +/- 50%\n\n### Example Delay Sequence (baseDelay=200ms, maxDelay=30s)\n\n| Attempt | Base delay | With jitter (range) |\n|---------|-----------|---------------------|\n| 0 | 200ms | 100-300ms |\n| 1 | 400ms | 200-600ms |\n| 2 | 800ms | 400-1200ms |\n| 3 | 1600ms | 800-2400ms |\n| 4 | 3200ms | 1600-4800ms |\n| 5 | 6400ms | 3200-9600ms |\n\n### Implementation\n\n```javascript\nasync function retryWithBackoff(fn, options = {}) {\n  const {\n    maxRetries = 3, baseDelay = 200,\n    maxDelay = 30000, retryableCheck = () => true,\n  } = options;\n\n  for (let attempt = 0; attempt <= maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      if (attempt === maxRetries || !retryableCheck(error)) throw error;\n      const delay = Math.min(maxDelay, baseDelay * Math.pow(2, attempt));\n      await sleep(delay * (0.5 + Math.random()));\n    }\n  }\n}\n```\n\n## Idempotency Requirements\n\n**Any retried operation MUST be idempotent.** A retry might arrive after the original\nsucceeded (network timeout on response, not on request).\n\n| Operation | Strategy | Implementation |\n|-----------|----------|----------------|\n| Create resource | Idempotency key | Client sends unique key; server deduplicates |\n| Update resource | Version/ETag | Conditional update with `If-Match` header |\n| Delete resource | Naturally idempotent | Deleting already-deleted returns success |\n| Payment/transfer | Idempotency key | Store key->result mapping; return cached result |\n| Queue publish | Message dedup ID | Consumer deduplicates by message ID |\n\n### Idempotency Key Pattern\n\n```\nClient generates: idempotencyKey = uuid()\n\nPOST /orders  |  Idempotency-Key: abc-123\n\nServer: check if abc-123 exists in store\n  -> Yes: return stored response (do NOT re-execute)\n  -> No:  execute, store response keyed by abc-123 (expires 24-48h)\n```\n\n**Critical:** The key must be generated by the caller. The caller retries with the SAME key.\n\n## Retry Budget Management\n\nUnbounded retries amplify failures. Use budgets to limit total retry load.\n\n### Per-Request Budget\n\n```\nTotal timeout: 10s  |  Max retries: 3  |  Per-attempt timeout: 2s\nBudget remaining = totalTimeout - elapsed\n```\n\nStop retrying when budget is exhausted, even if max retries not reached.\n\n### System-Wide Retry Budget\n\nLimit retries to 20% of total requests across the service. If retry rate exceeds\nbudget, fail fast. This prevents retry storms from amplifying outages.\n\n### Circuit Breaker Integration\n\nRetries feed into the circuit breaker. When retries consistently fail, the circuit\nopens and stops all calls for a cooldown period.\n\n## When NOT to Retry\n\n### Non-Transient Errors (never retry)\n\n| HTTP Status | Meaning | Why no retry |\n|-------------|---------|--------------|\n| 400 | Bad Request | Payload won't fix itself |\n| 401 | Unauthorized | Token is invalid |\n| 403 | Forbidden | Permission won't change |\n| 404 | Not Found | Resource doesn't exist |\n| 409 | Conflict | Requires resolution, not retry |\n| 422 | Unprocessable | Validation failure |\n\n### Retryable Errors (retry with backoff)\n\n| HTTP Status | Meaning | Retry guidance |\n|-------------|---------|----------------|\n| 408 | Request Timeout | Retry immediately |\n| 429 | Too Many Requests | Respect `Retry-After` header |\n| 502 | Bad Gateway | Retry, upstream may recover |\n| 503 | Service Unavailable | Retry, service restarting |\n| 504 | Gateway Timeout | Retry with longer timeout |\n\n### Other Non-Retryable Conditions\n\n- **Mutation already applied**: POST succeeded but response timed out; retry creates duplicates\n- **Business logic rejection**: \"Insufficient funds\" won't change on retry\n- **Authentication failures**: Re-authenticate; don't retry the same bad token\n\n## Retry Observability\n\nLog every retry for debugging and tuning:\n\n```json\n{\n  \"level\": \"warn\",\n  \"message\": \"Retrying failed operation\",\n  \"operation\": \"fetchUserProfile\",\n  \"attempt\": 2,\n  \"maxAttempts\": 3,\n  \"delayMs\": 847,\n  \"error\": { \"code\": \"ETIMEDOUT\" },\n  \"correlationId\": \"req-a1b2c3d4\"\n}\n```\n\nTrack retry rate as a metric. A rising retry rate is an early warning signal.\n"
    }
  ],
  "tags": [
    "implementation",
    "quality",
    "patterns",
    "reliability"
  ],
  "dependsOn": [
    "implement"
  ]
}