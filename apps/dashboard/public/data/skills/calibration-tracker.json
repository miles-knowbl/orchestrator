{
  "id": "calibration-tracker",
  "name": "calibration-tracker",
  "version": "1.0.0",
  "description": "Tracks estimate accuracy over time and adjusts future estimates based on historical data. Compares estimated vs actual effort, identifies systematic biases, and generates calibration adjustments. Enables increasingly accurate estimates as more data accumulates.",
  "phase": "COMPLETE",
  "category": "operations",
  "content": "# Calibration Tracker\n\nImprove estimates through feedback.\n\n## When to Use\n\n- **After journey completion** — Record actual vs estimated\n- **Before estimating** — Load calibration adjustments\n- **Periodically** — Analyze patterns across domains\n- **When estimates consistently off** — Diagnose and adjust\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `calibration-formulas.md` | Statistical methods for adjustment |\n| `variance-analysis.md` | Root cause patterns for estimate variance |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `confidence-levels.md` | When interpreting sample size confidence |\n\n**Verification:** Ensure calibration.json is updated with new data point.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| `calibration.json` | `domain-memory/{domain}/learning/` | Always (create or update) |\n\n## Core Concept\n\nCalibration Tracker answers: **\"How can we estimate more accurately next time?\"**\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                      CALIBRATION FEEDBACK LOOP                               │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  ESTIMATE ──────────────────────────────────────────▶ ACTUAL                │\n│     │                                                    │                  │\n│     │                                                    │                  │\n│     │          ┌─────────────────────────┐               │                  │\n│     │          │   Calibration Tracker   │               │                  │\n│     │          │                         │               │                  │\n│     └─────────▶│   Compare & Analyze     │◀──────────────┘                  │\n│                │   Generate Adjustments  │                                  │\n│                │   Store History         │                                  │\n│                └────────────┬────────────┘                                  │\n│                             │                                               │\n│                             ▼                                               │\n│                    FUTURE ESTIMATES                                         │\n│                    (with adjustments)                                       │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n## Calibration Data Model\n\n### Historical Record\n\n```json\n{\n  \"domain\": \"skills-library-mcp\",\n  \"records\": [\n    {\n      \"id\": \"rec-001\",\n      \"system\": \"Skills Library MCP\",\n      \"date\": \"2025-01-17\",\n      \"estimated\": {\n        \"complexity\": \"M\",\n        \"effortHours\": 26,\n        \"durationDays\": 2,\n        \"riskMultiplier\": 1.2,\n        \"confidence\": \"high\",\n        \"breakdown\": {\n          \"foundation\": 4,\n          \"state\": 7.25,\n          \"memory\": 3.5,\n          \"github\": 3.5,\n          \"polish\": 5.5\n        }\n      },\n      \"actual\": {\n        \"effortHours\": 4.5,\n        \"durationDays\": 0.5,\n        \"breakdown\": {\n          \"foundation\": 0.5,\n          \"state\": 1.5,\n          \"memory\": 0.75,\n          \"github\": 0.5,\n          \"polish\": 1.25\n        }\n      },\n      \"ratio\": 0.17,\n      \"factors\": {\n        \"agenticExecution\": true,\n        \"existingPatterns\": true,\n        \"clearRequirements\": true,\n        \"noBlockers\": true\n      },\n      \"notes\": \"Agentic continuous execution far faster than estimated human sprints\"\n    }\n  ]\n}\n```\n\n### Adjustment Model\n\n```json\n{\n  \"domain\": \"skills-library-mcp\",\n  \"lastUpdated\": \"2025-01-17\",\n  \"sampleSize\": 1,\n  \"adjustments\": {\n    \"global\": {\n      \"agenticMultiplier\": 0.3,\n      \"confidence\": \"low\",\n      \"basedOn\": 1\n    },\n    \"byComplexity\": {\n      \"S\": { \"multiplier\": 1.0, \"samples\": 0 },\n      \"M\": { \"multiplier\": 0.3, \"samples\": 1 },\n      \"L\": { \"multiplier\": 1.0, \"samples\": 0 },\n      \"XL\": { \"multiplier\": 1.0, \"samples\": 0 }\n    },\n    \"byCategory\": {\n      \"mcp\": { \"multiplier\": 0.8, \"samples\": 1 },\n      \"typescript\": { \"multiplier\": 0.9, \"samples\": 1 },\n      \"fileOperations\": { \"multiplier\": 0.7, \"samples\": 1 }\n    },\n    \"byPhase\": {\n      \"INIT\": { \"multiplier\": 0.5, \"samples\": 1 },\n      \"SCAFFOLD\": { \"multiplier\": 0.3, \"samples\": 1 },\n      \"IMPLEMENT\": { \"multiplier\": 0.3, \"samples\": 1 },\n      \"TEST\": { \"multiplier\": 0.4, \"samples\": 1 },\n      \"VERIFY\": { \"multiplier\": 0.3, \"samples\": 1 },\n      \"VALIDATE\": { \"multiplier\": 0.3, \"samples\": 1 },\n      \"DOCUMENT\": { \"multiplier\": 0.4, \"samples\": 1 },\n      \"REVIEW\": { \"multiplier\": 0.3, \"samples\": 1 },\n      \"SHIP\": { \"multiplier\": 0.3, \"samples\": 1 }\n    }\n  }\n}\n```\n\n## The Calibration Process\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                      CALIBRATION PROCESS                                     │\n│                                                                             │\n│  RECORD PHASE (After journey)                                               │\n│  ────────────────────────────                                               │\n│                                                                             │\n│  1. CAPTURE ACTUALS                                                         │\n│     └─→ Total hours from journey tracer                                     │\n│     └─→ Hours by phase                                                      │\n│     └─→ Hours by skill                                                      │\n│                                                                             │\n│  2. COMPARE TO ESTIMATE                                                     │\n│     └─→ Overall ratio (actual / estimated)                                  │\n│     └─→ Phase-level ratios                                                  │\n│     └─→ Identify largest variances                                          │\n│                                                                             │\n│  3. ANALYZE FACTORS                                                         │\n│     └─→ What contributed to variance?                                       │\n│     └─→ Agentic vs human execution?                                         │\n│     └─→ Clear requirements vs ambiguity?                                    │\n│     └─→ Existing patterns vs novel?                                         │\n│                                                                             │\n│  4. UPDATE ADJUSTMENTS                                                      │\n│     └─→ Weighted average with history                                       │\n│     └─→ Update confidence based on sample size                              │\n│     └─→ Flag anomalies for review                                           │\n│                                                                             │\n│  APPLY PHASE (Before estimating)                                            │\n│  ───────────────────────────────                                            │\n│                                                                             │\n│  5. LOAD ADJUSTMENTS                                                        │\n│     └─→ Read domain calibration data                                        │\n│     └─→ Check sample sizes for confidence                                   │\n│                                                                             │\n│  6. APPLY TO ESTIMATE                                                       │\n│     └─→ Start with base estimate                                            │\n│     └─→ Apply relevant multipliers                                          │\n│     └─→ Document adjustments made                                           │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n## Recording Actuals\n\n### Source of Truth: skillsLog\n\nThe actual durations come from `loop-state.json`'s `skillsLog` field. Each skill invocation records:\n\n```json\n{\n  \"skill\": \"implement\",\n  \"reason\": \"Implement C1: Record metric events\",\n  \"startedAt\": \"2025-01-17T22:30:00Z\",\n  \"completedAt\": \"2025-01-17T23:15:00Z\",\n  \"durationMs\": 2700000,\n  \"status\": \"complete\"\n}\n```\n\n**Extracting actuals:**\n\n```javascript\n// Sum all skill durations for total actual time\nconst totalMs = skillsLog.reduce((sum, entry) => {\n  const entryMs = entry.durationMs || 0;\n  const childMs = (entry.children || []).reduce((s, c) => s + (c.durationMs || 0), 0);\n  return sum + entryMs + childMs;\n}, 0);\nconst totalHours = totalMs / 3600000;\n\n// Group by skill for per-skill calibration\nconst bySkill = {};\nskillsLog.forEach(entry => {\n  bySkill[entry.skill] = (bySkill[entry.skill] || 0) + entry.durationMs;\n});\n\n// Group by capability (from reason field) for per-capability calibration\nconst byCapability = {};\nskillsLog.filter(e => e.reason.includes('C1:') || e.reason.includes('C2:')).forEach(...);\n```\n\n**What counts:**\n- Skill execution time only (durationMs)\n- Nested child skills are counted separately, not double-counted in parent\n- Gate wait time is NOT included\n- Human review time is NOT included\n\n### After Journey Completion\n\n```markdown\n## Calibration Record: [System Name]\n\n**Date:** [Date]\n**Domain:** [Domain]\n\n### Estimate (from ESTIMATE.md)\n\n| Dimension | Value |\n|-----------|-------|\n| Complexity | [S/M/L/XL] |\n| Effort | [X hours] |\n| Duration | [Y days] |\n| Risk Multiplier | [Z]x |\n| Confidence | [High/Medium/Low] |\n\n### Actual (from skillsLog)\n\n| Dimension | Value |\n|-----------|-------|\n| Effort | [X hours] |\n| Duration | [Y days] |\n| Rework Cycles | [N] |\n\n### Comparison\n\n| Metric | Estimated | Actual | Ratio |\n|--------|-----------|--------|-------|\n| Total Hours | 26 | 4.5 | 0.17 |\n| INIT Phase | 6 | 1.5 | 0.25 |\n| SCAFFOLD Phase | 4 | 1 | 0.25 |\n| IMPLEMENT Phase | 8 | 1.5 | 0.19 |\n| TEST Phase | 2 | 0.5 | 0.25 |\n| VERIFY Phase | 2 | 0.25 | 0.125 |\n| VALIDATE Phase | 1 | 0.25 | 0.25 |\n| DOCUMENT Phase | 1 | 0.5 | 0.5 |\n| REVIEW Phase | 1 | 0.25 | 0.25 |\n| SHIP Phase | 1 | 0.25 | 0.25 |\n\n### Per-Skill Comparison\n\n| Skill | Estimated | Actual | Ratio |\n|-------|-----------|--------|-------|\n| spec | 30m | 3m | 0.10 |\n| estimation | 15m | 1m | 0.07 |\n| architect | 60m | ? | ? |\n| scaffold | 30m | ? | ? |\n| implement | 300m | ? | ? |\n| test-generation | 120m | ? | ? |\n| code-verification | 30m | ? | ? |\n\n### Contributing Factors\n\n- [x] Agentic execution (continuous, no context switching)\n- [x] Clear requirements (single system, well-defined)\n- [x] Existing patterns (MCP SDK, TypeScript)\n- [x] No blockers (no external dependencies)\n- [ ] Novel domain (had prior knowledge)\n- [ ] Complex integrations (simple file-based)\n\n### Anomalies\n\n- Estimate was for human developer with sprints\n- Actual was agentic continuous execution\n- Need separate calibration tracks for human vs agentic\n\n### Adjustment Recommendation\n\n| Factor | Current | Recommended | Confidence |\n|--------|---------|-------------|------------|\n| Agentic Global | N/A | 0.3x | Low (n=1) |\n| Medium Complexity | 1.0x | 0.3x | Low (n=1) |\n| MCP Category | N/A | 0.8x | Low (n=1) |\n```\n\n## Applying Calibration\n\n### Before Estimating\n\n```markdown\n## Calibration Check: [New System]\n\n**Domain:** [Domain]\n**Date:** [Date]\n\n### Available Calibration Data\n\n| Factor | Samples | Adjustment | Confidence |\n|--------|---------|------------|------------|\n| Global (Agentic) | 1 | 0.3x | Low |\n| Complexity (M) | 1 | 0.3x | Low |\n| Category (MCP) | 1 | 0.8x | Low |\n\n### Raw Estimate\n\n[From estimation skill]\n- Base effort: 40 hours\n- Risk multiplier: 1.2x\n- **Raw total: 48 hours**\n\n### Calibrated Estimate\n\n[Apply adjustments]\n- Raw: 48 hours\n- Agentic adjustment: × 0.3 = 14.4 hours\n- MCP adjustment: × 0.8 = 11.5 hours\n- **Calibrated total: 12-15 hours**\n\n### Confidence Note\n\nSample size is low (n=1). Calibrated estimate has high uncertainty.\nRecommend tracking actuals closely to improve calibration.\n```\n\n## Confidence Levels\n\n| Sample Size | Confidence | Action |\n|-------------|------------|--------|\n| 0 | None | Use default multiplier (1.0x) |\n| 1-2 | Low | Use with caution, wide range |\n| 3-5 | Medium | Apply but verify |\n| 6-10 | Good | Reliable for similar contexts |\n| 10+ | High | Stable estimate |\n\n## Variance Analysis\n\nWhen ratio is significantly off:\n\n### Underestimate (Actual > Estimated)\n\n| Cause | Indicator | Fix |\n|-------|-----------|-----|\n| Hidden complexity | Many unknowns discovered | Add discovery phase |\n| Scope creep | Requirements changed | Better requirements |\n| Integration issues | External dependencies | Add integration buffer |\n| Rework | Multiple iterations | Improve first-pass quality |\n\n### Overestimate (Actual < Estimated)\n\n| Cause | Indicator | Fix |\n|-------|-----------|-----|\n| Agentic efficiency | Continuous execution | Agentic multiplier |\n| Familiar patterns | Reused previous work | Pattern multiplier |\n| Clear requirements | No ambiguity | Reduce uncertainty buffer |\n| Good tooling | MCP, IDE support | Tool productivity factor |\n\n## File Locations\n\n| File | Location | Purpose |\n|------|----------|---------|\n| Calibration data | `domain-memory/{domain}/learning/calibration.json` | Historical records |\n| Record template | `domain-memory/{domain}/learning/calibration-records/` | Individual records |\n\n## Integration\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                      CALIBRATION INTEGRATION                                 │\n│                                                                             │\n│  estimation skill                                                           │\n│       │                                                                     │\n│       ├──▶ Reads calibration.json                                           │\n│       │    └─→ Applies relevant adjustments                                 │\n│       │    └─→ Documents adjustments in ESTIMATE.md                         │\n│       │                                                                     │\n│  journey-tracer                                                             │\n│       │                                                                     │\n│       └──▶ Provides actual hours                                            │\n│            └─→ Total and by-phase breakdown                                 │\n│                                                                             │\n│  retrospective skill                                                        │\n│       │                                                                     │\n│       └──▶ Triggers calibration update                                      │\n│            └─→ Calls calibration-tracker with journey data                  │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n## Calibration Tracker Verification Checklist\n\n```markdown\n## calibration-tracker Verification\n\n### Record Phase\n- [ ] Actual hours captured from journey\n- [ ] Comparison to estimate documented\n- [ ] Contributing factors identified\n- [ ] Anomalies flagged\n- [ ] Adjustment recommendations made\n\n### Update Phase\n- [ ] calibration.json updated\n- [ ] Sample sizes incremented\n- [ ] Confidence levels updated\n- [ ] Anomalies documented for review\n\n### Apply Phase\n- [ ] Calibration data loaded before estimate\n- [ ] Relevant adjustments applied\n- [ ] Adjustments documented in estimate\n- [ ] Confidence level noted\n```\n\n## Mode-Specific Behavior\n\nCalibration tracking behavior differs by orchestrator mode:\n\n### Greenfield Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Full system builds from scratch |\n| **Approach** | Track all phases comprehensively |\n| **Patterns** | Establish baseline calibration data |\n| **Deliverables** | Phase timing, capability estimates |\n| **Validation** | Compare estimated vs actual per system |\n| **Constraints** | Minimal—building initial dataset |\n\n### Brownfield-Polish Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Gap closure iterations, polish cycles |\n| **Approach** | Track per-gap-type timing patterns |\n| **Patterns** | Should match existing gap categories |\n| **Deliverables** | Gap type baselines, rework frequency |\n| **Validation** | Compare estimated vs actual per gap |\n| **Constraints** | Must track by gap category |\n\n**Polish considerations:**\n- Track dark mode and responsive design time\n- Measure deployment configuration overhead\n- Account for test coverage improvement time\n- Record rework frequency per gap type\n\n### Brownfield-Enterprise Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Surgical changes, pattern conformance |\n| **Approach** | Track change size vs time ratios |\n| **Patterns** | Must include enterprise overhead factors |\n| **Deliverables** | Enterprise multiplier, review cycle time |\n| **Validation** | Compare change estimate vs actual |\n| **Constraints** | Include approval wait time in estimates |\n\n**Enterprise constraints:**\n- Account for codebase analysis overhead\n- Track pattern conformance verification time\n- Measure multi-team coordination delays\n- Include compliance/security review cycles\n\n### Mode-Specific Calibration Data\n\nStore separate calibration tracks per mode:\n\n```json\n{\n  \"domain\": \"example-domain\",\n  \"byMode\": {\n    \"greenfield\": {\n      \"samples\": 3,\n      \"avgRatio\": 0.35,\n      \"confidence\": \"medium\"\n    },\n    \"brownfield-polish\": {\n      \"samples\": 5,\n      \"avgRatio\": 0.45,\n      \"confidence\": \"medium\"\n    },\n    \"brownfield-enterprise\": {\n      \"samples\": 2,\n      \"avgRatio\": 0.60,\n      \"confidence\": \"low\"\n    }\n  }\n}\n```\n\n---\n\n→ See `references/calibration-formulas.md` for statistical methods\n→ See `references/variance-analysis.md` for root cause patterns\n\n## References\n\n| Reference | Description |\n|-----------|-------------|\n| [`calibration-formulas.md`](references/calibration-formulas.md) | Statistical methods for calculating calibration adjustments |\n| [`confidence-levels.md`](references/confidence-levels.md) | Interpreting sample size and confidence levels |\n| [`variance-analysis.md`](references/variance-analysis.md) | Root cause patterns for estimate variance |",
  "references": [
    {
      "name": "calibration-formulas.md",
      "path": "references/calibration-formulas.md",
      "content": "# Calibration Formulas\n\nStatistical methods for improving estimate accuracy.\n\n---\n\n## Core Concept\n\nCalibration adjusts future estimates based on historical accuracy:\n\n```\nAdjusted Estimate = Base Estimate × Calibration Multiplier\n```\n\n---\n\n## Basic Ratio Calculation\n\nFor each completed system:\n\n```\nRatio = Actual Hours / Estimated Hours\n```\n\nExamples:\n- Estimated 26h, Actual 4.5h → Ratio = 0.17\n- Estimated 10h, Actual 15h → Ratio = 1.5\n- Estimated 20h, Actual 20h → Ratio = 1.0\n\n### Interpretation\n\n| Ratio | Meaning |\n|-------|---------|\n| < 0.5 | Significantly overestimated |\n| 0.5-0.8 | Moderately overestimated |\n| 0.8-1.2 | Accurate |\n| 1.2-1.5 | Moderately underestimated |\n| > 1.5 | Significantly underestimated |\n\n---\n\n## Calibration Multiplier\n\n### Simple Average\n\nFor small sample sizes (< 10):\n\n```\nMultiplier = Sum(Ratios) / Count(Ratios)\n```\n\n### Weighted Average\n\nWeight recent data more heavily:\n\n```\nMultiplier = Σ(Weight_i × Ratio_i) / Σ(Weight_i)\n\nWhere Weight_i = 1 / (age_in_days + 1)\n```\n\n### Exponential Moving Average\n\nFor continuous adjustment:\n\n```\nNew_Multiplier = α × Latest_Ratio + (1 - α) × Previous_Multiplier\n\nWhere α = 0.3 (typical smoothing factor)\n```\n\n---\n\n## Confidence Levels\n\nHow much to trust the multiplier:\n\n| Samples | Confidence | Recommended Action |\n|---------|------------|-------------------|\n| 0 | None | Use 1.0x (no adjustment) |\n| 1-2 | Very Low | Apply with wide range (±50%) |\n| 3-5 | Low | Apply cautiously (±30%) |\n| 6-10 | Medium | Apply normally (±20%) |\n| 10-20 | Good | Apply confidently (±10%) |\n| 20+ | High | Apply with tight range (±5%) |\n\n### Confidence Formula\n\n```\nConfidence Score = min(samples / 20, 1.0)\n\nRange = Base × (1 - Confidence Score × 0.45)\n```\n\n---\n\n## Factor-Based Adjustments\n\nDifferent contexts need different multipliers.\n\n### By Execution Mode\n\n| Mode | Typical Multiplier | Notes |\n|------|-------------------|-------|\n| Human, interrupted | 1.5-2.0x | Meetings, context switching |\n| Human, focused | 1.0x | Baseline |\n| Agentic, supervised | 0.5-0.8x | Faster but with gates |\n| Agentic, autonomous | 0.2-0.4x | Continuous execution |\n\n### By Complexity\n\n| Complexity | Adjustment |\n|------------|------------|\n| S (Small) | 0.8x (often faster than estimated) |\n| M (Medium) | 1.0x (baseline) |\n| L (Large) | 1.2x (integration overhead) |\n| XL (Extra Large) | 1.5x (unknowns accumulate) |\n\n### By Familiarity\n\n| Familiarity | Adjustment |\n|-------------|------------|\n| Novel domain | 1.5x |\n| Similar to past work | 0.8x |\n| Repeat of past work | 0.5x |\n\n---\n\n## Combined Adjustment\n\nApply multiple factors:\n\n```\nFinal Multiplier = Global × Execution × Complexity × Familiarity × Confidence\n```\n\nExample:\n- Global: 0.8 (historically overestimate by 20%)\n- Execution: 0.3 (agentic autonomous)\n- Complexity: 1.0 (medium)\n- Familiarity: 0.8 (similar to past)\n- Confidence: 0.6 (6 samples)\n\n```\nFinal = 0.8 × 0.3 × 1.0 × 0.8 × 0.6 = 0.115\n```\n\nFor 26h estimate: 26 × 0.115 ≈ 3 hours\n\n---\n\n## Variance Analysis\n\nUnderstanding why estimates were off.\n\n### Variance Breakdown\n\n```\nTotal Variance = Scope Variance + Execution Variance + External Variance\n```\n\nWhere:\n- **Scope Variance**: Requirements changed, discovered complexity\n- **Execution Variance**: Faster/slower than expected execution\n- **External Variance**: Blockers, dependencies, interruptions\n\n### Tracking Template\n\n```json\n{\n  \"record\": {\n    \"estimated\": 26,\n    \"actual\": 4.5,\n    \"ratio\": 0.17,\n    \"variance\": {\n      \"scope\": 0,\n      \"execution\": -21.5,\n      \"external\": 0\n    },\n    \"factors\": {\n      \"agenticExecution\": true,\n      \"clearSpec\": true,\n      \"existingPatterns\": true\n    }\n  }\n}\n```\n\n---\n\n## Data Structure\n\n### calibration.json\n\n```json\n{\n  \"domain\": \"skills-library-mcp\",\n  \"records\": [\n    {\n      \"system\": \"Skills Library MCP\",\n      \"date\": \"2025-01-17\",\n      \"estimated\": {\n        \"complexity\": \"M\",\n        \"effortHours\": 26\n      },\n      \"actual\": {\n        \"effortHours\": 4.5\n      },\n      \"ratio\": 0.17,\n      \"factors\": {\n        \"executionMode\": \"agentic-autonomous\",\n        \"familiarity\": \"similar\",\n        \"scopeChange\": \"none\"\n      },\n      \"notes\": \"First calibration point for agentic execution\"\n    }\n  ],\n  \"adjustments\": {\n    \"global\": {\n      \"multiplier\": 0.17,\n      \"confidence\": \"very-low\",\n      \"basedOn\": 1\n    },\n    \"byExecutionMode\": {\n      \"agentic-autonomous\": {\n        \"multiplier\": 0.17,\n        \"samples\": 1\n      }\n    },\n    \"byComplexity\": {\n      \"M\": {\n        \"multiplier\": 0.17,\n        \"samples\": 1\n      }\n    }\n  },\n  \"lastUpdated\": \"2025-01-17T12:00:00Z\"\n}\n```\n\n---\n\n## Application Protocol\n\n### Before Estimating\n\n1. Load calibration data for domain\n2. Identify relevant factors (execution mode, complexity, familiarity)\n3. Look up multipliers for each factor\n4. Calculate combined adjustment\n5. Apply to base estimate\n6. Document adjustments made\n\n### After Completing\n\n1. Record actual effort\n2. Calculate ratio\n3. Update relevant multipliers\n4. Recalculate confidence\n5. Document any unusual factors\n\n---\n\n## Gotchas\n\n### Small Sample Bias\n\nWith few samples, individual outliers have huge impact. Use wide ranges until you have 10+ samples.\n\n### Context Drift\n\nMultipliers from one context may not apply to another. Track factors and use factor-specific multipliers when possible.\n\n### Changing Conditions\n\nAs skills improve, multipliers should trend toward 1.0 for human execution, or stabilize for agentic execution. If they don't, investigate why.\n\n### Self-Fulfilling Prophecy\n\nIf you know the multiplier, you might subconsciously adjust effort to match. Record actuals honestly.\n\n---\n\n*Use these formulas to continuously improve estimate accuracy.*\n"
    },
    {
      "name": "confidence-levels.md",
      "path": "references/confidence-levels.md",
      "content": "# Confidence Levels\n\nHow to assess and communicate the reliability of calibration data.\n\n---\n\n## Purpose\n\nConfidence levels indicate how much to trust calibration multipliers and estimates. With few data points, multipliers are unstable and should be applied cautiously. As samples accumulate, confidence increases and multipliers become more reliable.\n\n**When to Use This Guide:**\n- Deciding how much to adjust estimates based on calibration\n- Communicating uncertainty in estimates to stakeholders\n- Determining when calibration data is mature enough to trust\n- Setting appropriate ranges around point estimates\n\n---\n\n## Confidence Scale\n\n| Level | Samples | Score Range | Meaning | Trust Weight |\n|-------|---------|-------------|---------|--------------|\n| **None** | 0 | 0% | No data, use defaults | 0% |\n| **Very Low** | 1-2 | 1-20% | Guess, minimal data | 10% |\n| **Low** | 3-5 | 21-40% | Limited data, high uncertainty | 25% |\n| **Medium** | 6-10 | 41-60% | Some data, moderate uncertainty | 50% |\n| **High** | 11-20 | 61-80% | Good data, low uncertainty | 75% |\n| **Very High** | 21+ | 81-100% | Extensive data, very predictable | 100% |\n\n### Visual Representation\n\n```\nSamples:     0      1-2     3-5     6-10    11-20    21+\n             │       │       │        │        │       │\nConfidence:  ├───────┼───────┼────────┼────────┼───────┤\n             None   VeryLow  Low    Medium   High   VeryHigh\n             │       │       │        │        │       │\nTrust:       0%     10%     25%     50%      75%    100%\n             │       │       │        │        │       │\nRange:       N/A    ±50%    ±30%    ±20%     ±10%    ±5%\n```\n\n---\n\n## Factors Affecting Confidence\n\n### Sample Size (Primary Factor)\n\n```\nsamples = 0     → confidence = 0%\nsamples = 1-2   → confidence -= 40%\nsamples = 3-5   → confidence -= 20%\nsamples = 6-10  → confidence += 0%\nsamples = 11-20 → confidence += 10%\nsamples = 21+   → confidence += 20%\n```\n\n### Variance Consistency\n\nHow consistent are the samples?\n\n```\nstdDev < 10%  → confidence += 20% (very consistent)\nstdDev 10-25% → confidence += 0%  (normal)\nstdDev 25-50% → confidence -= 20% (inconsistent)\nstdDev > 50%  → confidence -= 40% (highly variable)\n```\n\n**Example:**\n- 10 samples with stdDev of 8%: Base 60% + 20% = 80% (High)\n- 10 samples with stdDev of 35%: Base 60% - 20% = 40% (Low)\n\n### Recency\n\nHow old is the data?\n\n```\ndata < 1 week old   → confidence += 10%\ndata 1-4 weeks old  → confidence += 0%\ndata 1-3 months old → confidence -= 10%\ndata > 3 months old → confidence -= 20%\n```\n\n### Domain Similarity\n\nDoes the calibration data match the current work?\n\n```\nsame domain/stack     → confidence += 10%\nsimilar domain/stack  → confidence += 0%\ndifferent domain/stack → confidence -= 20%\n```\n\n---\n\n## Calculating Confidence\n\n### Basic Algorithm\n\n```typescript\ninterface ConfidenceInput {\n  samples: number;\n  variances: number[];\n  lastUpdated: Date;\n  domainMatch: 'same' | 'similar' | 'different';\n}\n\ninterface ConfidenceResult {\n  level: string;\n  score: number;\n  trustWeight: number;\n  range: number;\n  recommendation: string;\n}\n\nfunction calculateConfidence(data: ConfidenceInput): ConfidenceResult {\n  let confidence = 50; // Base starting point\n\n  // Sample size adjustment\n  if (data.samples === 0) {\n    return {\n      level: 'none',\n      score: 0,\n      trustWeight: 0,\n      range: 0,\n      recommendation: 'No calibration data - use 1.0x multiplier'\n    };\n  } else if (data.samples < 3) {\n    confidence -= 40;\n  } else if (data.samples < 6) {\n    confidence -= 20;\n  } else if (data.samples > 20) {\n    confidence += 20;\n  } else if (data.samples > 10) {\n    confidence += 10;\n  }\n\n  // Variance consistency adjustment\n  const stdDev = calculateStdDev(data.variances);\n  if (stdDev < 0.1) {\n    confidence += 20;\n  } else if (stdDev > 0.5) {\n    confidence -= 40;\n  } else if (stdDev > 0.25) {\n    confidence -= 20;\n  }\n\n  // Recency adjustment\n  const ageWeeks = (Date.now() - data.lastUpdated.getTime()) /\n                   (7 * 24 * 60 * 60 * 1000);\n  if (ageWeeks < 1) {\n    confidence += 10;\n  } else if (ageWeeks > 12) {\n    confidence -= 20;\n  } else if (ageWeeks > 4) {\n    confidence -= 10;\n  }\n\n  // Domain similarity adjustment\n  if (data.domainMatch === 'same') {\n    confidence += 10;\n  } else if (data.domainMatch === 'different') {\n    confidence -= 20;\n  }\n\n  // Clamp to valid range\n  confidence = Math.max(0, Math.min(100, confidence));\n\n  return scoreToResult(confidence);\n}\n\nfunction scoreToResult(score: number): ConfidenceResult {\n  if (score <= 0) {\n    return { level: 'none', score, trustWeight: 0, range: 0,\n             recommendation: 'No data - use defaults' };\n  } else if (score <= 20) {\n    return { level: 'very_low', score, trustWeight: 0.1, range: 0.5,\n             recommendation: 'Note pattern, don\\'t adjust yet' };\n  } else if (score <= 40) {\n    return { level: 'low', score, trustWeight: 0.25, range: 0.3,\n             recommendation: 'Apply 25% weight to adjustment' };\n  } else if (score <= 60) {\n    return { level: 'medium', score, trustWeight: 0.5, range: 0.2,\n             recommendation: 'Apply 50% weight to adjustment' };\n  } else if (score <= 80) {\n    return { level: 'high', score, trustWeight: 0.75, range: 0.1,\n             recommendation: 'Apply 75% weight, reliable data' };\n  } else {\n    return { level: 'very_high', score, trustWeight: 1.0, range: 0.05,\n             recommendation: 'Full trust in calibration' };\n  }\n}\n\nfunction calculateStdDev(values: number[]): number {\n  if (values.length === 0) return 0;\n  const mean = values.reduce((a, b) => a + b, 0) / values.length;\n  const variance = values.reduce((sum, v) =>\n    sum + Math.pow(v - mean, 2), 0) / values.length;\n  return Math.sqrt(variance);\n}\n```\n\n### Continuous Confidence Score\n\nFor smoother progression:\n\n```typescript\nfunction continuousConfidenceScore(sampleCount: number): number {\n  // Asymptotic curve approaching 1.0\n  // At 5 samples: ~0.4\n  // At 10 samples: ~0.6\n  // At 20 samples: ~0.8\n  // At 30 samples: ~0.9\n  return 1 - Math.exp(-sampleCount / 15);\n}\n```\n\n---\n\n## Applying Confidence to Estimates\n\n### Weighted Multiplier Application\n\nDon't apply the full calibration multiplier at low confidence:\n\n```typescript\nfunction applyCalibration(\n  baseEstimate: number,\n  multiplier: number,\n  confidence: ConfidenceResult\n): CalibratedEstimate {\n  // Blend between 1.0 (no adjustment) and calibrated multiplier\n  const effectiveMultiplier = 1 + (multiplier - 1) * confidence.trustWeight;\n\n  const pointEstimate = baseEstimate * effectiveMultiplier;\n  const range = pointEstimate * confidence.range;\n\n  return {\n    point: Math.round(pointEstimate * 10) / 10,\n    low: Math.round((pointEstimate - range) * 10) / 10,\n    high: Math.round((pointEstimate + range) * 10) / 10,\n    confidence: confidence.level,\n    score: confidence.score\n  };\n}\n```\n\n**Example:**\n```\nBase estimate: 10h\nCalibration multiplier: 0.5 (historically 50% faster)\nConfidence: Low (trustWeight: 0.25)\n\nEffective multiplier: 1 + (0.5 - 1) * 0.25 = 0.875\nPoint estimate: 10 * 0.875 = 8.75h\nRange (30%): 8.75 * 0.3 = 2.6h\n\nResult: 8.75h (6.1h - 11.4h), Low confidence\n```\n\n### Estimate Ranges by Confidence\n\n| Confidence | Point | Low | High | Display |\n|------------|-------|-----|------|---------|\n| None | 10h | - | - | \"~10h (no calibration)\" |\n| Very Low | 9.5h | 4.8h | 14.3h | \"10h (5-14h range)\" |\n| Low | 8.8h | 6.1h | 11.4h | \"9h (6-11h)\" |\n| Medium | 8h | 6.4h | 9.6h | \"8h (6-10h)\" |\n| High | 7.5h | 6.8h | 8.3h | \"7.5h +/- 45min\" |\n| Very High | 7h | 6.7h | 7.4h | \"7h +/- 20min\" |\n\n---\n\n## Displaying Confidence\n\n### In Estimates\n\n```markdown\n## Estimate: metric-store\n\n| Phase | Hours | Confidence | Range |\n|-------|-------|------------|-------|\n| SPEC | 1h | High (72%) | 0.9-1.1h |\n| IMPLEMENT | 6h | Medium (48%) | 4.8-7.2h |\n| TEST | 2h | Low (35%) | 1.4-2.6h |\n| **Total** | **9h** | **Medium (52%)** | **7.1-10.9h** |\n\n**Note:** Implementation estimate has higher uncertainty due to limited\nsamples for this complexity level. Test estimates are based on similar\nbut not identical projects.\n```\n\n### In Calibration Data\n\n```json\n{\n  \"calibration\": {\n    \"global\": {\n      \"multiplier\": 1.3,\n      \"confidence\": {\n        \"level\": \"high\",\n        \"score\": 65,\n        \"trustWeight\": 0.75\n      },\n      \"samples\": 12,\n      \"lastUpdated\": \"2025-01-17\"\n    },\n    \"byComplexity\": {\n      \"S\": {\n        \"multiplier\": 0.8,\n        \"confidence\": { \"level\": \"high\", \"score\": 70, \"trustWeight\": 0.75 },\n        \"samples\": 15\n      },\n      \"M\": {\n        \"multiplier\": 1.0,\n        \"confidence\": { \"level\": \"medium\", \"score\": 55, \"trustWeight\": 0.5 },\n        \"samples\": 8\n      },\n      \"L\": {\n        \"multiplier\": 1.5,\n        \"confidence\": { \"level\": \"low\", \"score\": 30, \"trustWeight\": 0.25 },\n        \"samples\": 4\n      },\n      \"XL\": {\n        \"multiplier\": 2.0,\n        \"confidence\": { \"level\": \"very_low\", \"score\": 15, \"trustWeight\": 0.1 },\n        \"samples\": 2\n      }\n    }\n  }\n}\n```\n\n### Visual Confidence Indicators\n\n```\nVery High: █████████████████████ (81-100%)\nHigh:      ████████████████░░░░░ (61-80%)\nMedium:    ███████████░░░░░░░░░░ (41-60%)\nLow:       █████░░░░░░░░░░░░░░░░ (21-40%)\nVery Low:  ██░░░░░░░░░░░░░░░░░░░ (1-20%)\nNone:      ░░░░░░░░░░░░░░░░░░░░░ (0%)\n```\n\n### Dashboard Display\n\n```markdown\n## Calibration Dashboard\n\n### Global Calibration\n**Multiplier:** 0.8x (tasks complete 20% faster)\n**Confidence:** ████████████████░░░░░ High (65%)\n**Based on:** 12 samples\n**Range:** 0.72x - 0.88x\n\n### By Execution Mode\n| Mode | Multiplier | Confidence | Samples |\n|------|------------|------------|---------|\n| Agentic Autonomous | 0.20x | ██░░░░░░░░ Very Low | 2 |\n| Agentic Supervised | 0.45x | █████░░░░░ Low | 4 |\n| Human Focused | 1.00x | ████████████████░░░░░ High | 12 |\n\n### By Complexity\n| Size | Multiplier | Confidence | Samples |\n|------|------------|------------|---------|\n| S | 0.80x | ████████████████░░░░░ High | 15 |\n| M | 1.00x | ███████████░░░░░░░░░░ Medium | 8 |\n| L | 1.50x | █████░░░░░░░░░░░░░░░░ Low | 4 |\n| XL | 2.00x | ██░░░░░░░░░░░░░░░░░░░ Very Low | 2 |\n\n### Recommendations\n- **Small tasks:** Well calibrated, trust estimates\n- **Medium tasks:** Moderate confidence, expect some variance\n- **Large tasks:** Add 30% buffer, need more samples\n- **XL tasks:** Consider breaking down, estimates unreliable\n```\n\n---\n\n## Using Confidence in Decisions\n\n| Confidence | Estimate Behavior | Commitment Level |\n|------------|-------------------|------------------|\n| None | Use 1.0x, add 50% buffer | Cannot commit |\n| Very Low | Note pattern, add 50% buffer, flag for review | Cannot commit |\n| Low | Apply 25% weight, add 30% buffer | Soft commitment |\n| Medium | Apply 50% weight, use as-is | Normal commitment |\n| High | Apply 75% weight, can commit | Firm commitment |\n| Very High | Apply 100%, tight deadlines OK | Hard commitment |\n\n### Decision Examples\n\n**Low Confidence (35%):**\n```\nBase estimate: 10h\nCalibration suggests: 7h (0.7x multiplier)\nEffective estimate: 10 * (1 + (0.7-1)*0.25) = 9.25h\nWith 30% buffer: 12h\nCommitment: \"Likely 1.5 days, could be 2\"\n```\n\n**High Confidence (75%):**\n```\nBase estimate: 10h\nCalibration suggests: 7h (0.7x multiplier)\nEffective estimate: 10 * (1 + (0.7-1)*0.75) = 7.75h\nWith 10% buffer: 8.5h\nCommitment: \"8 hours, half-day buffer\"\n```\n\n---\n\n## Improving Confidence\n\n### Strategies\n\n1. **Complete more systems** - More samples = higher confidence\n   - Target: 6+ samples for medium confidence\n   - Target: 20+ samples for very high confidence\n\n2. **Track consistently** - Use journey-tracer for every skill\n   - Record all timing data\n   - Include factors (complexity, execution mode)\n\n3. **Segment appropriately** - Group similar work together\n   - By complexity (S/M/L/XL)\n   - By execution mode (agentic vs human)\n   - By domain/technology\n\n4. **Review regularly** - Update calibration data weekly\n   - Check for data staleness\n   - Remove or flag outliers\n\n5. **Handle outliers carefully** - Don't let anomalies skew data\n   - Investigate unusual samples\n   - Keep but flag outliers\n\n### Confidence Building Timeline\n\n```markdown\n## Confidence Maturation\n\n### Week 1-2: Bootstrap (Target: Very Low → Low)\n- Complete 3-5 small systems\n- Track all timing meticulously\n- Note execution factors\n- Expected: Low confidence (3-5 samples)\n\n### Week 3-4: Validation (Target: Low → Medium)\n- Complete 3-5 more systems\n- Validate initial patterns\n- Identify consistent factors\n- Expected: Medium confidence (6-10 samples)\n\n### Week 5-8: Stabilization (Target: Medium → High)\n- Normal work pace\n- Continue tracking\n- Start segmenting by factors\n- Expected: High confidence (11-20 samples)\n\n### Week 9+: Maturity (Target: High → Very High)\n- Maintain data collection\n- Monitor for drift\n- Periodic recalibration review\n- Expected: Very High confidence (21+ samples)\n```\n\n---\n\n## Edge Cases\n\n### Mixed Confidence Factors\n\nWhen different factors have different confidence:\n\n```typescript\nfunction combinedConfidence(factors: ConfidenceResult[]): ConfidenceResult {\n  // Use minimum confidence (most conservative)\n  const minScore = Math.min(...factors.map(f => f.score));\n  return scoreToResult(minScore);\n}\n\n// Example:\n// Global: High (65%)\n// Execution Mode: Low (30%)\n// Complexity: Medium (50%)\n// Combined: Low (30%) - use lowest\n```\n\n### Confidence Decay\n\nOld data becomes less reliable:\n\n```typescript\nfunction applyDecay(\n  baseConfidence: number,\n  lastUpdated: Date,\n  halfLifeDays: number = 90\n): number {\n  const ageInDays = (Date.now() - lastUpdated.getTime()) / (1000 * 60 * 60 * 24);\n  const decayFactor = Math.pow(0.5, ageInDays / halfLifeDays);\n  return baseConfidence * decayFactor;\n}\n```\n\n### Outlier Impact\n\nOne outlier in small samples:\n\n```\n3 samples: [0.8, 0.9, 2.5]\nWith outlier: avg=1.4, stdDev=0.76 → confidence penalty\nWithout: avg=0.85, stdDev=0.05 → no penalty\n\nRecommendation: Flag but don't remove outliers until investigated\n```\n\n---\n\n## Anti-Patterns\n\n1. **Ignoring confidence** - Using calibration multipliers without considering confidence\n2. **Over-trusting early data** - Fully applying multipliers with 2-3 samples\n3. **Hiding uncertainty** - Giving point estimates without ranges\n4. **Not segmenting** - Mixing agentic and human data\n5. **Stale calibration** - Using data from 6+ months ago without decay\n\n---\n\n## See Also\n\n- [calibration-formulas.md](./calibration-formulas.md) - Statistical methods\n- [variance-analysis.md](./variance-analysis.md) - Understanding deviations\n- [estimation references](../../estimation/references/) - Estimation skill guidance\n\n---\n\n*Confidence levels ensure calibration is applied appropriately to the data quality available.*\n"
    },
    {
      "name": "variance-analysis.md",
      "path": "references/variance-analysis.md",
      "content": "# Variance Analysis\n\nHow to analyze the difference between estimated and actual effort to improve future estimates.\n\n---\n\n## Purpose\n\nVariance analysis identifies systematic patterns in estimation errors. By understanding why estimates deviate from actuals, you can calibrate future estimates and improve process efficiency.\n\n**When to Use This Guide:**\n- After completing a system or significant capability\n- During retrospectives to understand estimation patterns\n- When calibration multipliers need adjustment\n- When investigating chronic over- or under-estimation\n\n---\n\n## Basic Variance Formula\n\n```\nVariance = (Actual - Estimated) / Estimated * 100%\n\nAlternative (ratio form):\nRatio = Actual / Estimated\n```\n\n### Interpretation\n\n| Variance | Ratio | Interpretation |\n|----------|-------|----------------|\n| +50% | 1.5 | Took 50% longer (underestimate) |\n| +20% | 1.2 | Took 20% longer (slight underestimate) |\n| 0% | 1.0 | Perfect estimate |\n| -20% | 0.8 | Took 20% less (slight overestimate) |\n| -50% | 0.5 | Took 50% less (significant overestimate) |\n| -80% | 0.2 | Took 80% less (major overestimate) |\n\n### Example Calculation\n\n```\nSystem: metric-store\nEstimated: 26 hours\nActual: 4.5 hours\n\nVariance = (4.5 - 26) / 26 * 100% = -82.7%\nRatio = 4.5 / 26 = 0.17\n\nInterpretation: Significantly overestimated (took only 17% of estimate)\n```\n\n---\n\n## Variance Categories\n\n### Acceptable Variance\n\n| Range | Category | Action Required |\n|-------|----------|-----------------|\n| -20% to +20% | Acceptable | None - estimates are calibrated |\n\n### Overestimation (Completed Faster)\n\n| Range | Category | Action |\n|-------|----------|--------|\n| -20% to -35% | Moderate overestimate | Monitor pattern |\n| -35% to -50% | Significant overestimate | Reduce estimates by 25% |\n| -50% to -75% | Severe overestimate | Investigate root cause |\n| < -75% | Extreme overestimate | Full estimate process review |\n\n### Underestimation (Took Longer)\n\n| Range | Category | Action |\n|-------|----------|--------|\n| +20% to +35% | Moderate underestimate | Increase estimates by 25% |\n| +35% to +50% | Significant underestimate | Investigate scope creep |\n| +50% to +100% | Severe underestimate | Review estimation process |\n| > +100% | Critical | Post-mortem required |\n\n---\n\n## Systematic Variance Patterns\n\n### By Complexity\n\nTrack variance by estimated complexity:\n\n```json\n{\n  \"byComplexity\": {\n    \"S\": {\n      \"samples\": 12,\n      \"avgVariance\": -15,\n      \"pattern\": \"slight_overestimate\",\n      \"action\": \"S estimates are ~15% high, can reduce\"\n    },\n    \"M\": {\n      \"samples\": 18,\n      \"avgVariance\": +5,\n      \"pattern\": \"accurate\",\n      \"action\": \"M estimates are well calibrated\"\n    },\n    \"L\": {\n      \"samples\": 8,\n      \"avgVariance\": +35,\n      \"pattern\": \"underestimate\",\n      \"action\": \"L estimates should be increased 30%\"\n    },\n    \"XL\": {\n      \"samples\": 4,\n      \"avgVariance\": +80,\n      \"pattern\": \"severe_underestimate\",\n      \"action\": \"Break XL items into smaller pieces\"\n    }\n  }\n}\n```\n\n**Common Patterns:**\n- Small tasks often overestimated (padding/overhead)\n- Large tasks often underestimated (integration complexity)\n- Medium tasks most accurate (sweet spot for estimation)\n\n### By Phase\n\nTrack variance by SDLC phase:\n\n```json\n{\n  \"byPhase\": {\n    \"SPEC\": {\n      \"avgVariance\": -25,\n      \"action\": \"Reduce spec time estimates\"\n    },\n    \"SCAFFOLD\": {\n      \"avgVariance\": -40,\n      \"action\": \"Scaffolding faster than expected with templates\"\n    },\n    \"IMPLEMENT\": {\n      \"avgVariance\": +30,\n      \"action\": \"Increase implementation estimates by 25%\"\n    },\n    \"TEST\": {\n      \"avgVariance\": +60,\n      \"action\": \"Double test time estimates\"\n    },\n    \"VERIFY\": {\n      \"avgVariance\": +20,\n      \"action\": \"Slightly increase verification time\"\n    }\n  }\n}\n```\n\n**Common Patterns:**\n- Early phases (spec, scaffold) often overestimated\n- Testing consistently underestimated\n- Verification time depends on initial code quality\n\n### By Skill\n\nTrack variance by skill applied:\n\n```json\n{\n  \"bySkill\": {\n    \"architect\": {\n      \"avgVariance\": +10,\n      \"status\": \"acceptable\"\n    },\n    \"implement\": {\n      \"avgVariance\": +35,\n      \"status\": \"needs_adjustment\",\n      \"adjustment\": \"Increase by 30%\"\n    },\n    \"test-generation\": {\n      \"avgVariance\": +65,\n      \"status\": \"critical\",\n      \"adjustment\": \"Include test time in capability estimates\"\n    },\n    \"security-audit\": {\n      \"avgVariance\": +15,\n      \"status\": \"acceptable\"\n    }\n  }\n}\n```\n\n### By Execution Mode\n\nTrack variance by how work was executed:\n\n```json\n{\n  \"byExecutionMode\": {\n    \"human-interrupted\": {\n      \"avgVariance\": +50,\n      \"note\": \"Context switching adds significant overhead\"\n    },\n    \"human-focused\": {\n      \"avgVariance\": +10,\n      \"note\": \"Baseline estimation target\"\n    },\n    \"agentic-supervised\": {\n      \"avgVariance\": -40,\n      \"note\": \"Faster but with gate waits\"\n    },\n    \"agentic-autonomous\": {\n      \"avgVariance\": -80,\n      \"note\": \"Continuous execution dramatically faster\"\n    }\n  }\n}\n```\n\n---\n\n## Root Cause Categories\n\nWhen variance exceeds acceptable ranges, categorize the cause:\n\n### Scope Variance\n\nActual scope differed from estimated scope:\n\n| Pattern | Likely Cause | Solution |\n|---------|--------------|----------|\n| Hidden requirements | Incomplete spec | Better clarifying questions |\n| Scope creep | Changes during implementation | Change control process |\n| Over-scoping | Estimated features not needed | Validate requirements first |\n| Integration complexity | Dependencies not accounted | Map dependencies in spec |\n\n### Execution Variance\n\nExecution speed differed from expected:\n\n| Pattern | Likely Cause | Solution |\n|---------|--------------|----------|\n| Faster than expected | Existing code/patterns reused | Reduce estimates for similar work |\n| Slower than expected | Technical debt encountered | Add buffer for legacy code |\n| Environment issues | Setup/tooling problems | Include setup time in estimates |\n| Learning curve | New technology | Add learning time explicitly |\n\n### External Variance\n\nFactors outside the work itself:\n\n| Pattern | Likely Cause | Solution |\n|---------|--------------|----------|\n| Waiting time | Gate approvals, reviews | Track active vs waiting time |\n| Interruptions | Context switching | Use focused time blocks |\n| Blockers | Dependencies, external teams | Add blocker buffer |\n| Infrastructure | CI/CD, deployment issues | Separate infra estimates |\n\n---\n\n## Variance Breakdown Template\n\nFor each completed system:\n\n```markdown\n## Variance Analysis: [System Name]\n\n### Summary\n\n| Metric | Estimated | Actual | Variance |\n|--------|-----------|--------|----------|\n| Total | 26h | 4.5h | -82.7% |\n| SPEC | 4h | 1h | -75% |\n| SCAFFOLD | 2h | 0.25h | -87.5% |\n| IMPLEMENT | 12h | 2h | -83.3% |\n| TEST | 4h | 0.5h | -87.5% |\n| VERIFY | 2h | 0.5h | -75% |\n| DEPLOY | 2h | 0.25h | -87.5% |\n\n### Breakdown by Source\n\n| Source | Impact | Notes |\n|--------|--------|-------|\n| Scope | 0h | No scope changes |\n| Execution | -21.5h | Agentic execution much faster |\n| External | 0h | No blockers |\n\n### Factors\n\n- **Execution Mode:** agentic-autonomous\n- **Familiarity:** Similar to previous work (templates available)\n- **Complexity:** M (Medium)\n- **Clear Spec:** Yes (detailed FEATURESPEC)\n\n### Root Cause\n\nEstimates were based on human-focused execution timing.\nAgentic autonomous execution dramatically reduced all phases.\n\n### Calibration Update\n\n- Global multiplier adjustment: 0.17 (from 1.0)\n- Confidence: Very Low (first agentic data point)\n- Recommendation: Collect more agentic samples before trusting\n```\n\n---\n\n## Calculating Calibration Adjustments\n\n### From Variance to Multiplier\n\n```typescript\nfunction varianceToMultiplier(variancePercent: number): number {\n  // Variance of +30% means 1.3x multiplier\n  // Variance of -50% means 0.5x multiplier\n  return 1 + (variancePercent / 100);\n}\n\nfunction ratioToMultiplier(ratio: number): number {\n  // Ratio IS the multiplier\n  return ratio;\n}\n```\n\n### Averaging Multiple Samples\n\n```typescript\nfunction calculateAdjustment(samples: VarianceSample[]): number {\n  if (samples.length < 3) {\n    console.warn('Insufficient samples for reliable adjustment');\n    return 1.0;  // No adjustment\n  }\n\n  const avgVariance = samples.reduce((sum, s) =>\n    sum + (s.actual - s.estimated) / s.estimated, 0) / samples.length;\n\n  return 1 + avgVariance;\n}\n```\n\n### Weighted by Recency\n\n```typescript\nfunction recencyWeightedAdjustment(\n  samples: VarianceSample[],\n  halfLifeDays: number = 30\n): number {\n  const now = Date.now();\n  let weightedSum = 0;\n  let totalWeight = 0;\n\n  for (const sample of samples) {\n    const ageInDays = (now - sample.date) / (1000 * 60 * 60 * 24);\n    const weight = Math.pow(0.5, ageInDays / halfLifeDays);\n\n    const variance = (sample.actual - sample.estimated) / sample.estimated;\n    weightedSum += variance * weight;\n    totalWeight += weight;\n  }\n\n  return 1 + (weightedSum / totalWeight);\n}\n```\n\n---\n\n## Confidence by Sample Size\n\nThe reliability of variance analysis depends on sample size:\n\n| Samples | Confidence | Recommendation |\n|---------|------------|----------------|\n| 1-2 | Very Low | Note pattern, don't adjust yet |\n| 3-5 | Low | Apply 25% weight to adjustment |\n| 6-10 | Medium | Apply 50% weight |\n| 11-20 | High | Apply 75% weight |\n| 21+ | Very High | Apply full adjustment |\n\n### Confidence Calculation\n\n```typescript\nfunction calculateConfidence(sampleCount: number): {\n  level: string;\n  weight: number;\n  margin: number;\n} {\n  if (sampleCount <= 2) {\n    return { level: 'very_low', weight: 0, margin: 0.5 };\n  } else if (sampleCount <= 5) {\n    return { level: 'low', weight: 0.25, margin: 0.3 };\n  } else if (sampleCount <= 10) {\n    return { level: 'medium', weight: 0.5, margin: 0.2 };\n  } else if (sampleCount <= 20) {\n    return { level: 'high', weight: 0.75, margin: 0.1 };\n  } else {\n    return { level: 'very_high', weight: 1.0, margin: 0.05 };\n  }\n}\n```\n\n---\n\n## Example Analysis\n\n### Full Example\n\n```markdown\n## Variance Analysis Report\n\n### System: metric-store\n**Domain:** skills-library-mcp\n**Date:** 2025-01-17\n**Analyst:** agent-001\n\n### Time Summary\n\n| Phase | Est (h) | Act (h) | Var % | Notes |\n|-------|---------|---------|-------|-------|\n| SPEC | 4 | 1 | -75% | Detailed template helped |\n| SCAFFOLD | 2 | 0.25 | -88% | Standard MCP structure |\n| IMPLEMENT | 12 | 2 | -83% | Clear spec, reused patterns |\n| TEST | 4 | 0.5 | -88% | Generated from spec |\n| VERIFY | 2 | 0.5 | -75% | All tests passed |\n| DEPLOY | 2 | 0.25 | -88% | Standard npm publish |\n| **TOTAL** | **26** | **4.5** | **-83%** | |\n\n### Variance Breakdown\n\n**By Source:**\n- Scope: 0% (no scope change)\n- Execution: -83% (agentic autonomous)\n- External: 0% (no blockers)\n\n**Contributing Factors:**\n1. Agentic autonomous execution (primary - ~70% of variance)\n2. Clear detailed spec (secondary - ~15% of variance)\n3. Reusable patterns available (tertiary - ~15% of variance)\n\n### Root Cause Analysis\n\n**Why 83% faster than estimated?**\n\n1. **Why was it faster?**\n   → Agentic execution without context switching\n\n2. **Why did agentic execution help so much?**\n   → No meetings, no interruptions, continuous focus\n\n3. **Why were estimates so high?**\n   → Based on historical human-paced development\n\n4. **Why hadn't this been calibrated before?**\n   → First agentic execution for this domain\n\n**Root Cause:** Estimates calibrated for human execution, agentic execution fundamentally different.\n\n### Calibration Impact\n\n**Before:**\n```json\n{\n  \"global\": { \"multiplier\": 1.0, \"confidence\": \"none\" }\n}\n```\n\n**After:**\n```json\n{\n  \"global\": { \"multiplier\": 0.17, \"confidence\": \"very_low\" },\n  \"byExecutionMode\": {\n    \"agentic-autonomous\": { \"multiplier\": 0.17, \"samples\": 1 }\n  }\n}\n```\n\n### Recommendations\n\n1. **Collect more samples** before trusting 0.17x multiplier\n2. **Track execution mode** separately from global calibration\n3. **Create agentic-specific** estimation guidelines\n4. **Preserve human estimates** for comparison/rollback\n\n### Next Steps\n\n- [ ] Apply to next 2 systems and observe\n- [ ] If pattern holds, formalize agentic calibration\n- [ ] Update estimation skill with execution mode factor\n```\n\n---\n\n## Tracking Template\n\n### Per-System Tracking\n\n```json\n{\n  \"systemId\": \"sys-001\",\n  \"name\": \"metric-store\",\n  \"date\": \"2025-01-17\",\n  \"estimate\": {\n    \"total\": 26,\n    \"breakdown\": {\n      \"SPEC\": 4,\n      \"SCAFFOLD\": 2,\n      \"IMPLEMENT\": 12,\n      \"TEST\": 4,\n      \"VERIFY\": 2,\n      \"DEPLOY\": 2\n    },\n    \"complexity\": \"M\",\n    \"assumptions\": [\n      \"Human-paced development\",\n      \"Some context switching expected\"\n    ]\n  },\n  \"actual\": {\n    \"total\": 4.5,\n    \"breakdown\": {\n      \"SPEC\": 1,\n      \"SCAFFOLD\": 0.25,\n      \"IMPLEMENT\": 2,\n      \"TEST\": 0.5,\n      \"VERIFY\": 0.5,\n      \"DEPLOY\": 0.25\n    }\n  },\n  \"variance\": {\n    \"total\": -82.7,\n    \"breakdown\": {\n      \"SPEC\": -75,\n      \"SCAFFOLD\": -87.5,\n      \"IMPLEMENT\": -83.3,\n      \"TEST\": -87.5,\n      \"VERIFY\": -75,\n      \"DEPLOY\": -87.5\n    }\n  },\n  \"factors\": {\n    \"executionMode\": \"agentic-autonomous\",\n    \"scopeChange\": \"none\",\n    \"blockers\": [],\n    \"reuse\": \"high\"\n  },\n  \"rootCause\": \"Estimates based on human execution, agentic dramatically faster\"\n}\n```\n\n---\n\n## Anti-Patterns\n\n### What to Avoid\n\n1. **Adjusting after single sample**\n   - Wait for 3+ samples before adjusting\n   - Single outliers can mislead\n\n2. **Ignoring context**\n   - Don't apply agentic multiplier to human work\n   - Track factors separately\n\n3. **Over-correcting**\n   - Apply adjustments gradually\n   - Use weighted averages\n\n4. **Hiding variance**\n   - Don't pad estimates to hide uncertainty\n   - Track and communicate uncertainty explicitly\n\n5. **Averaging unlike samples**\n   - Don't mix execution modes in same average\n   - Segment by relevant factors\n\n---\n\n## See Also\n\n- [calibration-formulas.md](./calibration-formulas.md) - Statistical methods\n- [confidence-levels.md](./confidence-levels.md) - Sample size guidance\n- [retrospective-templates.md](../../../retrospective/references/retrospective-templates.md) - Full analysis templates\n\n---\n\n*Use variance analysis consistently to continuously improve estimation accuracy.*\n"
    }
  ],
  "tags": [
    "meta",
    "calibration",
    "estimation",
    "metrics"
  ],
  "dependsOn": []
}