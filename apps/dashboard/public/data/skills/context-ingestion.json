{
  "id": "context-ingestion",
  "name": "context-ingestion",
  "version": "2.0.0",
  "description": "Gather and organize context from multiple sources into a structured format. Handles documents, URLs, conversations, and notes. Produces a verified source registry and compiled context corpus ready for downstream analysis.",
  "phase": "INIT",
  "category": "engineering",
  "content": "# Context Ingestion\n\nGather and organize context from multiple sources into a structured format.\n\n## When to Use\n\n- **New proposal or project kickoff** -- Need to collect and structure background information before analysis begins\n- **Client or domain onboarding** -- Entering unfamiliar territory and need to build a knowledge base quickly\n- **Research intake** -- Multiple documents, links, or conversations need to be cataloged and made searchable\n- **Scattered source material** -- Information exists across different formats and locations, needs consolidation\n- **Pre-analysis preparation** -- Downstream skills (context-cultivation, priority-matrix) need a clean, structured input\n- When you say: \"gather context\", \"collect sources\", \"ingest this\", \"pull together the background\", \"what do we know?\"\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `source-evaluation.md` | Criteria for assessing source reliability and relevance |\n| `extraction-patterns.md` | Standard patterns for pulling content from different source types |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `metadata-schema.md` | When extending default metadata fields for a domain |\n| `deduplication-rules.md` | When sources overlap or repeat across formats |\n| `url-fetch-guidelines.md` | When ingesting web content at scale |\n\n**Verification:** Ensure CONTEXT-SOURCES.md contains at least one entry per source provided, each with a completed metadata block and reliability rating.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| `CONTEXT-SOURCES.md` | Project root | Always -- registry of all sources with metadata |\n| `RAW-CONTEXT.md` | Project root | Always -- extracted content organized by source |\n| `INGESTION-LOG.md` | Project root | When 5+ sources -- processing notes and decisions |\n\n## Core Concept\n\nContext Ingestion answers: **\"What do we know, where did it come from, and how reliable is it?\"**\n\nContext ingestion is:\n- **Systematic** -- Every source is processed through the same evaluation and extraction pipeline\n- **Traceable** -- Every fact in the compiled output links back to a specific source\n- **Evaluative** -- Sources are rated for reliability, recency, and relevance, not treated as equally valid\n- **Comprehensive** -- Actively seeks breadth across source types to avoid blind spots\n- **Non-interpretive** -- Captures what sources say without adding analysis (that is context-cultivation's job)\n\nContext ingestion is NOT:\n- Analysis or synthesis (that is `context-cultivation`)\n- Priority setting or ranking (that is `priority-matrix`)\n- Proposal writing (that is `proposal-builder`)\n- Making recommendations based on what was found\n- Summarizing to the point of losing source fidelity\n\n## The Context Ingestion Process\n\n```\n┌──────────────────────────────────────────────────────────────────┐\n│                    CONTEXT INGESTION PROCESS                     │\n│                                                                  │\n│  1. SOURCE DISCOVERY                                             │\n│     └─> Inventory all available and findable sources             │\n│                                                                  │\n│  2. SOURCE TRIAGE                                                │\n│     └─> Evaluate relevance and reliability, prioritize intake    │\n│                                                                  │\n│  3. CONTENT EXTRACTION                                           │\n│     └─> Pull structured content from each source type            │\n│                                                                  │\n│  4. METADATA TAGGING                                             │\n│     └─> Attach provenance, dates, authors, reliability scores    │\n│                                                                  │\n│  5. DEDUPLICATION & CROSS-REFERENCING                            │\n│     └─> Identify overlaps, flag contradictions                   │\n│                                                                  │\n│  6. REGISTRY ASSEMBLY                                            │\n│     └─> Build CONTEXT-SOURCES.md with full metadata              │\n│                                                                  │\n│  7. CORPUS COMPILATION                                           │\n│     └─> Build RAW-CONTEXT.md organized by topic and source       │\n│                                                                  │\n│  8. COMPLETENESS CHECK                                           │\n│     └─> Verify coverage, flag gaps, log decisions                │\n└──────────────────────────────────────────────────────────────────┘\n```\n\n## Step 1: Source Discovery\n\nIdentify every available source before processing any of them. Cast a wide net.\n\n### Source Inventory Checklist\n\n```markdown\n- [ ] User-provided documents (PDFs, Word docs, slides, spreadsheets)\n- [ ] URLs and web pages explicitly shared\n- [ ] Conversation transcripts or chat logs\n- [ ] Meeting notes or recordings\n- [ ] Freeform notes or braindumps\n- [ ] Existing project files (README, specs, prior proposals)\n- [ ] Codebase context (if technical project)\n- [ ] Email threads or correspondence\n- [ ] Competitor or market materials\n- [ ] Regulatory or compliance documents\n```\n\n### Discovery Strategies\n\n| Strategy | Description | When to Apply |\n|----------|-------------|---------------|\n| **Explicit collection** | Gather everything the user has directly provided | Always -- first pass |\n| **Adjacency search** | Look for related files near provided sources | When sources reference other documents |\n| **Gap-driven search** | Identify missing perspectives and ask for them | After initial inventory reveals blind spots |\n| **Domain scan** | Search for standard artifacts in the domain | When onboarding to a new industry or codebase |\n| **Stakeholder mapping** | Identify who else might have relevant information | When building proposals or strategies |\n\n### Source Discovery Output\n\n```markdown\n### Source Inventory\n\n| # | Source Name | Type | Status | Notes |\n|---|------------|------|--------|-------|\n| 1 | Client brief.pdf | Document | Pending extraction | Primary input |\n| 2 | https://example.com/about | URL | Pending fetch | Company background |\n| 3 | Kickoff call notes | Conversation | Pending extraction | Key decisions made |\n| 4 | Competitor analysis spreadsheet | Document | Pending extraction | Market context |\n| 5 | [GAP] Technical requirements | Unknown | Not yet provided | Need to request |\n```\n\n## Step 2: Source Triage\n\nNot all sources deserve equal attention. Evaluate before extracting.\n\n### Reliability Assessment\n\nRate each source on a 1-5 scale across three dimensions:\n\n| Dimension | 1 (Low) | 3 (Medium) | 5 (High) |\n|-----------|---------|------------|-----------|\n| **Authority** | Unknown author, no credentials | Known author, some expertise | Domain expert, official source |\n| **Recency** | Over 2 years old | 6-24 months old | Less than 6 months old |\n| **Specificity** | Generic, tangentially related | Partially relevant | Directly addresses the topic |\n\n### Composite Reliability Score\n\n```\nReliability = (Authority + Recency + Specificity) / 3\n  5.0 - 4.0  =  HIGH    -->  Extract fully, high confidence\n  3.9 - 2.5  =  MEDIUM  -->  Extract selectively, note caveats\n  2.4 - 1.0  =  LOW     -->  Extract key claims only, flag uncertainty\n```\n\n### Triage Priority Matrix\n\n| Reliability | Relevance High | Relevance Medium | Relevance Low |\n|-------------|---------------|-----------------|--------------|\n| **HIGH** | Extract first, full depth | Extract second, full depth | Extract key points only |\n| **MEDIUM** | Extract second, selective | Extract third, selective | Skip or skim |\n| **LOW** | Extract with caveats | Skim for unique claims | Skip |\n\n### Red Flags During Triage\n\nWatch for sources that require extra scrutiny:\n\n- **Undated material** -- Cannot assess recency; flag and note\n- **Promotional content** -- May overstate capabilities; cross-reference claims\n- **Single-source claims** -- Important facts backed by only one source; note as unverified\n- **Contradicting sources** -- Two sources disagree on facts; capture both, flag for resolution\n- **Stale technical content** -- Technology references that may be outdated; verify currency\n\n## Step 3: Content Extraction\n\nApply source-type-specific extraction patterns to pull structured content.\n\n### Extraction by Source Type\n\n| Source Type | Extraction Method | Key Elements to Capture |\n|-------------|-------------------|------------------------|\n| **Documents (PDF, DOCX)** | Section-by-section extraction | Headings, key paragraphs, data tables, figures, conclusions |\n| **URLs / Web pages** | Main content extraction, ignore navigation | Article body, author, date, key data points |\n| **Conversations** | Decision and action extraction | Decisions made, action items, open questions, participants |\n| **Meeting notes** | Structured summary | Attendees, agenda items, decisions, follow-ups |\n| **Freeform notes** | Topic clustering | Group by theme, preserve original phrasing for key ideas |\n| **Spreadsheets** | Data characterization | Column meanings, row counts, key metrics, date ranges |\n| **Code / Repos** | Structure and pattern extraction | Architecture, tech stack, dependencies, conventions |\n| **Email threads** | Chronological decision tracking | Thread of decisions, final positions, open items |\n\n### Extraction Template\n\nFor each source, produce a block following this format:\n\n```markdown\n### [Source Name]\n\n**Source ID:** SRC-001\n**Type:** Document | URL | Conversation | Notes | Code | Data\n**Reliability:** HIGH | MEDIUM | LOW (score: X.X)\n**Extracted:** [date]\n\n#### Key Content\n\n[Extracted content organized by topic. Use direct quotes for important\nstatements. Paraphrase for general context. Always attribute.]\n\n#### Notable Claims\n\n- [Specific factual claim from the source]\n- [Another claim worth tracking]\n\n#### Open Questions\n\n- [Questions raised by this source]\n- [Ambiguities that need clarification]\n```\n\n### Extraction Quality Rules\n\n| Rule | Rationale |\n|------|-----------|\n| **Preserve original language for key claims** | Paraphrasing can shift meaning; quote when precision matters |\n| **Note page/section numbers** | Enables verification without re-reading entire source |\n| **Separate fact from opinion** | Mark subjective assessments as such (e.g., \"Author claims...\") |\n| **Flag quantitative data** | Numbers, dates, and metrics are high-value; always capture precisely |\n| **Capture what is NOT said** | Notable omissions (e.g., no mention of budget) are information too |\n\n## Step 4: Metadata Tagging\n\nEvery source entry gets a complete metadata block. Consistent metadata enables filtering, sorting, and tracing.\n\n### Standard Metadata Schema\n\n```markdown\n---\nsource_id: SRC-001\ntitle: \"Client Requirements Brief\"\ntype: document\nformat: pdf\nauthor: \"Jane Smith, VP Product\"\norganization: \"Acme Corp\"\ndate_created: 2025-11-15\ndate_accessed: 2026-01-25\nreliability: 4.2\nauthority: 5\nrecency: 4\nspecificity: 4\nword_count: 3200\ntopics: [requirements, timeline, budget, integrations]\nrelated_sources: [SRC-003, SRC-007]\ncontradicts: []\nstatus: extracted\nnotes: \"Primary input document. Contains both requirements and constraints.\"\n---\n```\n\n### Required vs Optional Metadata\n\n| Field | Required | Default if Missing |\n|-------|----------|--------------------|\n| `source_id` | Yes | Auto-generated (SRC-NNN) |\n| `title` | Yes | Filename or URL |\n| `type` | Yes | -- |\n| `reliability` | Yes | -- |\n| `date_accessed` | Yes | Current date |\n| `topics` | Yes | -- |\n| `author` | No | \"Unknown\" |\n| `date_created` | No | \"Unknown\" |\n| `organization` | No | \"Unknown\" |\n| `related_sources` | No | [] |\n| `contradicts` | No | [] |\n\n## Step 5: Deduplication and Cross-Referencing\n\nAfter extraction, identify overlaps and build connections between sources.\n\n### Deduplication Rules\n\n| Scenario | Action |\n|----------|--------|\n| **Exact duplicate** | Keep the more authoritative or more recent version; note the duplicate |\n| **Overlapping content** | Merge into a single entry, cite both sources |\n| **Same topic, different angles** | Keep both; link via `related_sources` |\n| **Contradicting claims** | Keep both; flag in `contradicts` field; note in INGESTION-LOG.md |\n\n### Cross-Reference Matrix\n\nBuild a topic-to-source matrix to visualize coverage:\n\n```markdown\n### Cross-Reference Matrix\n\n| Topic | SRC-001 | SRC-002 | SRC-003 | SRC-004 | Coverage |\n|-------|---------|---------|---------|---------|----------|\n| Budget | X | | X | | 2 sources |\n| Timeline | X | X | | | 2 sources |\n| Tech stack | | | X | X | 2 sources |\n| User needs | X | X | X | | 3 sources |\n| Competitors | | | | X | 1 source |\n| Compliance | | | | | 0 -- GAP |\n```\n\nTopics with 0-1 sources should be flagged as potential gaps.\n\n## Step 6: Registry Assembly\n\nCompile CONTEXT-SOURCES.md as the authoritative source registry.\n\n### CONTEXT-SOURCES.md Template\n\n```markdown\n# Context Sources Registry\n\n**Project:** [Project name]\n**Ingestion Date:** [Date]\n**Total Sources:** [Count]\n**Source Types:** [Breakdown by type]\n\n## Summary Statistics\n\n| Metric | Value |\n|--------|-------|\n| Total sources | [N] |\n| High reliability | [N] |\n| Medium reliability | [N] |\n| Low reliability | [N] |\n| Unique topics covered | [N] |\n| Identified gaps | [N] |\n\n## Source Registry\n\n### SRC-001: [Title]\n\n- **Type:** [type]\n- **Author:** [author]\n- **Date:** [date]\n- **Reliability:** [score] ([HIGH/MEDIUM/LOW])\n- **Topics:** [topic1, topic2, topic3]\n- **Related:** [SRC-NNN, SRC-NNN]\n- **Summary:** [One-line description of what this source contributes]\n\n### SRC-002: [Title]\n\n[Same format...]\n\n## Coverage Map\n\n[Cross-reference matrix from Step 5]\n\n## Identified Gaps\n\n| Gap | Impact | Suggested Action |\n|-----|--------|-----------------|\n| [Missing topic] | [How this affects downstream work] | [How to fill it] |\n```\n\n## Step 7: Corpus Compilation\n\nBuild RAW-CONTEXT.md as the organized content corpus, structured for consumption by context-cultivation.\n\n### RAW-CONTEXT.md Template\n\n```markdown\n# Raw Context Corpus\n\n**Project:** [Project name]\n**Compiled:** [Date]\n**Sources:** [Count] (see CONTEXT-SOURCES.md for full registry)\n\n## How to Read This Document\n\nContent is organized by topic. Each section draws from one or more\nsources, identified by source ID (e.g., SRC-001). Direct quotes are\nin blockquotes. Paraphrased content is in plain text.\n\n---\n\n## Topic: [Topic Name]\n\n**Sources:** SRC-001, SRC-003, SRC-007\n\n### From SRC-001 (Client Brief, reliability: HIGH)\n\n[Extracted content relevant to this topic]\n\n> \"Direct quote from the source when precision matters\" (p.12)\n\n### From SRC-003 (Meeting Notes, reliability: MEDIUM)\n\n[Extracted content relevant to this topic]\n\n**Note:** This partially contradicts SRC-001 regarding [specific point].\nSee INGESTION-LOG.md for details.\n\n---\n\n## Topic: [Next Topic]\n\n[Same structure...]\n\n---\n\n## Unclassified Content\n\n[Content that does not fit neatly into a topic but may be relevant.\nTag with source ID for traceability.]\n```\n\n### Organization Principles\n\n| Principle | Application |\n|-----------|-------------|\n| **Topic-first, not source-first** | Group by what the content is about, not where it came from |\n| **Highest reliability first** | Within each topic, lead with the most authoritative source |\n| **Contradictions are visible** | When sources disagree, show both and flag the conflict |\n| **Gaps are explicit** | Empty topics or thin coverage are noted, not hidden |\n| **Original language preserved** | Use blockquotes for critical statements; paraphrase for general context |\n\n## Step 8: Completeness Check\n\nBefore marking ingestion as complete, verify coverage and quality.\n\n### Completeness Checklist\n\n```markdown\n## Ingestion Completeness Review\n\n### Source Coverage\n- [ ] All provided sources have been processed\n- [ ] Each source has a complete metadata block in CONTEXT-SOURCES.md\n- [ ] Each source has extracted content in RAW-CONTEXT.md\n- [ ] Source IDs are consistent across all documents\n\n### Quality Checks\n- [ ] Reliability scores assigned to every source\n- [ ] No placeholder or stub entries remain\n- [ ] Contradictions between sources are flagged\n- [ ] Direct quotes are accurately attributed\n- [ ] Quantitative data (dates, numbers, metrics) verified against source\n\n### Coverage Analysis\n- [ ] Cross-reference matrix is complete\n- [ ] Gaps are identified and documented\n- [ ] Gap impact and suggested actions provided\n- [ ] Topics with single-source coverage flagged for attention\n\n### Downstream Readiness\n- [ ] RAW-CONTEXT.md is organized by topic (not by source)\n- [ ] Content is sufficient for context-cultivation to begin\n- [ ] INGESTION-LOG.md documents any decisions or anomalies (if applicable)\n```\n\n### Coverage Threshold\n\n| Metric | Target | Minimum |\n|--------|--------|---------|\n| Sources processed | 100% of provided | 90% of provided |\n| Metadata completeness | All required fields | source_id + title + type + reliability |\n| Topic coverage | No gaps in core topics | Gaps documented with impact |\n| Cross-references | All relationships mapped | Major relationships identified |\n\n## Output Formats\n\n### Quick Format (3 or fewer sources)\n\n```markdown\n# Context Sources\n\n**Project:** [Name]\n**Date:** [Date]\n\n## Sources\n\n### SRC-001: [Title]\n- **Type:** [type] | **Reliability:** [score]\n- **Key content:** [2-3 sentence summary]\n\n### SRC-002: [Title]\n- **Type:** [type] | **Reliability:** [score]\n- **Key content:** [2-3 sentence summary]\n\n## Compiled Context\n\n### [Topic 1]\n[Combined content from sources, attributed by SRC-ID]\n\n### [Topic 2]\n[Combined content from sources, attributed by SRC-ID]\n\n## Gaps\n- [Any missing information noted]\n```\n\n### Full Format (4+ sources)\n\nUse the complete CONTEXT-SOURCES.md and RAW-CONTEXT.md templates from Steps 6 and 7, with:\n- Full metadata blocks for every source\n- Cross-reference matrix\n- INGESTION-LOG.md for processing decisions\n- Completeness checklist executed and documented\n\n## Common Patterns\n\n### The Deep Dive\n\nProcess a small number of highly detailed sources with maximum extraction depth. Every section, every data point, every claim is captured and tagged.\n\n**Use when:** Working with 1-3 dense, authoritative documents (e.g., an RFP, a technical specification, a regulatory filing).\n\n### The Wide Sweep\n\nProcess many sources at moderate depth, prioritizing breadth of coverage over extraction completeness. Focus on key claims and unique contributions from each source.\n\n**Use when:** Onboarding to a new domain with 10+ heterogeneous sources (e.g., market research, competitor sites, internal docs, meeting notes).\n\n### The Conversation Harvest\n\nExtract structured information from unstructured dialogue -- meetings, chat logs, interviews. Focus on decisions, action items, stated preferences, and unanswered questions.\n\n**Use when:** Primary inputs are conversations, calls, or informal exchanges rather than polished documents.\n\n### The Incremental Build\n\nStart with an initial source set, process it, then add new sources as they arrive. Each addition triggers a targeted update to the registry and corpus rather than a full reprocessing.\n\n**Use when:** Sources arrive over time rather than all at once (e.g., ongoing client engagement, rolling research).\n\n## Relationship to Other Skills\n\n| Skill | Relationship |\n|-------|--------------|\n| `context-cultivation` | Receives CONTEXT-SOURCES.md and RAW-CONTEXT.md as primary inputs; transforms raw context into synthesized insights |\n| `priority-matrix` | Uses cultivated context (which depends on ingested context) to establish priorities |\n| `proposal-builder` | Final consumer in the proposal-loop chain; traces claims back to source IDs |\n| `metadata-extraction` | Can be invoked as a sub-process during Step 4 for automated metadata tagging |\n| `content-analysis` | Complementary skill for deeper analysis of individual complex documents |\n| `architect` | In engineering contexts, architecture decisions benefit from ingested technical context |\n\n## Key Principles\n\n**Provenance is non-negotiable.** Every piece of extracted content must trace back to a specific source via its SRC-ID. Untraceable claims are unreliable claims.\n\n**Evaluate before you extract.** Triage saves time. A low-relevance, low-reliability source processed at full depth is wasted effort. Assess first, then calibrate extraction depth.\n\n**Structure enables downstream work.** The value of ingestion is not in the reading -- it is in organizing content so that context-cultivation, priority-matrix, and proposal-builder can work efficiently.\n\n**Gaps are findings, not failures.** Discovering what is missing is as valuable as capturing what is present. Always document gaps with their impact and a suggested path to fill them.\n\n**Preserve fidelity, defer interpretation.** Capture what sources actually say, in their own words when it matters. Interpretation, synthesis, and judgment belong to downstream skills.\n\n**Contradictions are signals.** When sources disagree, do not resolve the conflict -- surface it. Contradictions often point to the most important areas for further investigation.\n\n## References\n\n- `references/source-evaluation.md`: Detailed criteria for assessing source authority, recency, and specificity\n- `references/extraction-patterns.md`: Source-type-specific extraction templates and techniques\n- `references/metadata-schema.md`: Extended metadata fields for domain-specific ingestion\n- `references/deduplication-rules.md`: Rules for handling overlapping and duplicate content\n- `references/url-fetch-guidelines.md`: Best practices for web content extraction at scale",
  "references": [
    {
      "name": "deduplication-rules.md",
      "path": "references/deduplication-rules.md",
      "content": "# Deduplication Rules\n\nStrategies for identifying duplicate content, merging overlapping sources, resolving contradictions, and maintaining a coherent cross-reference matrix.\n\n## Duplicate Detection\n\n### Exact Duplicates\n\nTwo sources are exact duplicates when their extracted content is identical.\n\n| Detection Method | When to Use | Implementation |\n|-----------------|-------------|----------------|\n| Content hash match | Always (first pass) | Compare `content_hash` fields; SHA-256 match = exact duplicate |\n| URL match | Web sources | Normalize URLs then compare; same canonical URL = likely duplicate |\n| Title + author match | Documents | Identical title and author with same publication year = likely duplicate |\n\n**Action on exact duplicate:** Keep the source with the higher `relevance_score`. If equal, keep the more recently ingested version. Mark the discarded source as `superseded_by: <kept_source_id>`.\n\n### Near Duplicates\n\nContent that is substantially similar but not identical (e.g., different versions, reformatted, or slightly edited).\n\n| Signal | Threshold | Example |\n|--------|-----------|---------|\n| Title similarity | > 80% Jaccard on words | \"PostgreSQL 16 Guide\" vs \"Guide to PostgreSQL 16\" |\n| Content overlap | > 70% shared sentences | Same article reposted on a different site |\n| Same URL, different fetch dates | Any | Content updated since last ingestion |\n| Same author + topic + similar date | Within 30 days | Author's blog post and their conference talk on the same topic |\n\n**Action on near duplicate:** Flag for review. Possible outcomes:\n1. **Merge** if one is a superset of the other\n2. **Keep both** if each contains unique information, and link them via `related_sources`\n3. **Keep newer** if the older version is fully superseded\n\n## Overlap Analysis\n\n### Identifying Partial Overlaps\n\nSources that share some content but each has unique material:\n\n```\nSource A: [--- shared content ---][--- unique to A ---]\nSource B: [--- unique to B ---][--- shared content ---]\n```\n\n**Classification of overlaps:**\n\n| Overlap Type | Description | Strategy |\n|-------------|-------------|----------|\n| Subset | Source B is entirely contained within Source A | Keep A only; mark B as redundant |\n| Superset | Source A is entirely contained within Source B | Keep B only; mark A as redundant |\n| Partial | Both share some content but each has unique parts | Keep both; link and note overlap |\n| Complementary | No content overlap but same topic from different angles | Keep both; link as complementary |\n| Contradictory | Overlapping claims but with different conclusions | Keep both; flag contradiction |\n\n### Merge Strategies for Partial Overlaps\n\nWhen merging is appropriate (one source is a clear superset with minor additions from another):\n\n1. **Select the primary source** - the one with higher authority and more complete coverage\n2. **Identify unique content** from the secondary source that is absent from the primary\n3. **Append unique content** to the primary extraction with attribution: \"Additionally, [secondary source] notes that...\"\n4. **Update metadata:**\n   - `related_sources` includes the secondary source ID\n   - `word_count` reflects merged content\n   - `content_hash` is recalculated\n   - `caveats` notes the merge: \"Merged with src_XXXXXXXX_XXXX\"\n5. **Mark secondary** as `merged_into: <primary_source_id>`\n\n## Handling Contradictory Sources\n\nWhen two credible sources disagree on facts:\n\n### Contradiction Severity Levels\n\n| Level | Description | Example | Action |\n|-------|-------------|---------|--------|\n| Minor | Differences in non-critical details | Version numbers, minor stats | Keep higher-authority source; note discrepancy |\n| Moderate | Different recommendations for the same problem | \"Use approach A\" vs \"Use approach B\" | Keep both; present as alternatives with tradeoffs |\n| Major | Fundamental factual disagreement | \"Feature X exists\" vs \"Feature X was removed\" | Escalate for verification; check official source |\n\n### Contradiction Resolution Process\n\n1. **Detect:** Flag when two sources with overlapping topic tags make incompatible claims\n2. **Classify:** Assign severity level (minor, moderate, major)\n3. **Compare authority:** Higher authority source is presumed correct for minor contradictions\n4. **Check recency:** For moderate contradictions, the more recent source often reflects current state\n5. **Verify independently:** For major contradictions, seek a third source or official documentation\n6. **Document:** Record the contradiction, resolution, and rationale in both sources' `caveats` fields\n\n```\ncaveats: \"Contradicts src_20250110_b2c1 on API rate limits.\nThis source (official docs, 2025-01) is preferred over that\nsource (blog post, 2024-06). Rate limit is 1000/min, not 500/min.\"\n```\n\n## Cross-Reference Matrix\n\nMaintain a matrix of relationships between ingested sources.\n\n### Relationship Types\n\n| Relationship | Meaning | Directionality |\n|-------------|---------|----------------|\n| `supplements` | Adds detail to another source | A supplements B |\n| `contradicts` | Disagrees with another source | Bidirectional |\n| `supersedes` | Replaces an older version | A supersedes B |\n| `derived_from` | Extracted or summarized from another | A derived_from B |\n| `same_topic` | Covers the same subject independently | Bidirectional |\n| `prerequisite` | Understanding A is needed to understand B | A is prerequisite for B |\n\n### Matrix Construction\n\nFor each newly ingested source, check against all existing sources:\n\n1. **Tag overlap check:** Sources sharing 2+ tags are candidates for relationship\n2. **Topic similarity:** Compare summaries for semantic overlap\n3. **Temporal relationship:** Newer source on same topic may supersede older\n4. **Author relationship:** Same author may have updated or expanded their work\n\n### Matrix Entry Format\n\n```yaml\ncross_references:\n  - source_a: \"src_20250115_a3f8\"\n    source_b: \"src_20250110_b2c1\"\n    relationship: \"supersedes\"\n    confidence: \"high\"\n    notes: \"Same API docs, newer version\"\n```\n\n## Deduplication Checklist\n\nRun this checklist for every new source during ingestion:\n\n- [ ] Compute `content_hash` and check for exact match in existing sources\n- [ ] Normalize URL and check for URL match in existing sources\n- [ ] Check title + author similarity against existing sources with shared tags\n- [ ] If near-duplicate found, classify overlap type (subset/superset/partial/complementary/contradictory)\n- [ ] Apply appropriate merge strategy or keep-both decision\n- [ ] If contradiction found, classify severity and resolve\n- [ ] Update cross-reference matrix with any new relationships\n- [ ] Verify no orphaned references after any merges or removals\n"
    },
    {
      "name": "extraction-patterns.md",
      "path": "references/extraction-patterns.md",
      "content": "# Extraction Patterns\n\nTechniques for extracting actionable context from different content types while preserving meaning and attribution.\n\n## Extraction by Content Type\n\n### Documents (PDF, Markdown, Word, Plain Text)\n\n| Element | Extract | Skip |\n|---------|---------|------|\n| Section headings and hierarchy | Yes - preserves structure | - |\n| Body text with key assertions | Yes - core content | - |\n| Tables and structured data | Yes - often highest density | - |\n| Code blocks and examples | Yes - with language annotation | - |\n| Footnotes and endnotes | Yes - often contain caveats | - |\n| Table of contents | No | Redundant with headings |\n| Page numbers and headers | No | Layout artifact |\n| Boilerplate (copyright, disclaimers) | No | Unless legally relevant |\n| Decorative images | No | No informational value |\n\n### Web Pages\n\n| Element | Extract | Skip |\n|---------|---------|------|\n| Main content area (`<article>`, `<main>`) | Yes - primary target | - |\n| Page title and meta description | Yes - useful for metadata | - |\n| Author, date, canonical URL | Yes - for attribution | - |\n| Code snippets and pre-formatted blocks | Yes - with syntax context | - |\n| Navigation menus | No | Site structure, not content |\n| Sidebar ads and promotions | No | Noise |\n| Cookie banners and modals | No | UI chrome |\n| Comments section | Selective | Only if highly upvoted or author-replied |\n\n### Conversations (Chat, Email, Threads)\n\n| Element | Extract | Skip |\n|---------|---------|------|\n| Decisions and conclusions | Yes - highest priority | - |\n| Action items and assignments | Yes - with attribution | - |\n| Requirements stated by stakeholders | Yes - with exact wording | - |\n| Links shared with context | Yes - note who shared and why | - |\n| Greetings and pleasantries | No | Social filler |\n| Repeated back-and-forth negotiation | Summarize | Extract final position only |\n| Emoji reactions | No | Unless they signal consensus (e.g., many thumbs-up) |\n\n### Video/Audio Transcripts\n\n| Element | Extract | Skip |\n|---------|---------|------|\n| Key statements with timestamps | Yes - enables verification | - |\n| Demonstrated workflows (step-by-step) | Yes - as numbered procedures | - |\n| Screen-shared content descriptions | Yes - describe what was shown | - |\n| Speaker introductions and banter | No | Social context only |\n| Filler words and false starts | No | Transcript noise |\n| Q&A segments | Selective | Extract if questions match your topic |\n\n## Core Extraction Techniques\n\n### 1. Assertion Extraction\nPull out factual claims as standalone statements:\n\n```\nSource: \"After migrating to PostgreSQL 16, we observed a 40% improvement\nin query performance for our JSONB workloads, though this required\nrewriting several stored procedures.\"\n\nExtracted:\n- PostgreSQL 16 improves JSONB query performance by ~40%\n- Migration from earlier versions may require stored procedure rewrites\n```\n\n### 2. Definition Extraction\nCapture explicit definitions of terms, concepts, or patterns:\n\n```\nSource: \"Circuit breaking is a stability pattern where calls to an external\nservice are monitored, and if failures exceed a threshold, subsequent calls\nare short-circuited for a cooldown period.\"\n\nExtracted: Circuit breaking = stability pattern; monitor external service\ncalls; short-circuit on failure threshold; cooldown period.\n```\n\n### 3. Procedure Extraction\nConvert narrative instructions into numbered steps:\n\n```\nSource: \"First you'll want to set up the config file, then make sure to\nrun migrations before starting the server...\"\n\nExtracted:\n1. Create/configure the config file\n2. Run database migrations\n3. Start the server\n```\n\n### 4. Constraint Extraction\nIdentify limitations, requirements, and boundaries:\n\n```\nSource: \"The API supports up to 1000 requests per minute per API key.\nBatch endpoints accept a maximum of 100 items. File uploads limited to 25MB.\"\n\nExtracted constraints:\n- Rate limit: 1000 req/min per API key\n- Batch size: max 100 items\n- Upload limit: 25MB per file\n```\n\n## Preserving Context and Attribution\n\nEvery extracted piece of content must carry:\n\n| Field | Purpose | Example |\n|-------|---------|---------|\n| `source_id` | Link back to original source | `src_2024_pg16_blog` |\n| `extracted_from` | Specific location within source | \"Section 3, paragraph 2\" |\n| `extraction_date` | When extraction occurred | `2025-01-15` |\n| `confidence` | Extractor's confidence in accuracy | `high`, `medium`, `low` |\n| `context_note` | Any caveats about the extraction | \"Author later corrected this in comments\" |\n\n## Handling Multi-Format Sources\n\nWhen a single source contains multiple formats (e.g., a web page with embedded video, code, and diagrams):\n\n1. **Extract each format independently** using the appropriate pattern above\n2. **Link extractions** with a shared `source_id` and sequential `part` numbers\n3. **Note format transitions** where meaning depends on seeing both formats together\n4. **Prioritize text over visual** when both convey the same information\n5. **Describe visuals in text** when they contain unique information not stated elsewhere\n\n## What to Never Extract\n\nRegardless of content type, always skip:\n\n- **Personally identifiable information** (emails, phone numbers, addresses) unless explicitly needed\n- **Authentication credentials** (API keys, passwords, tokens) even if visible in examples\n- **Copyrighted content in full** - extract facts and ideas, not verbatim reproductions\n- **Unqualified speculation** - preserve uncertainty markers (\"maybe\", \"I think\") if extracted\n- **Outdated version-specific details** when newer info is available from the same source\n\n## Extraction Quality Check\n\nAfter extraction, verify:\n\n- [ ] Each extraction can stand alone without the original source\n- [ ] Attribution is complete (source, location, date)\n- [ ] No meaning was lost or distorted in summarization\n- [ ] Uncertainty and caveats from the original are preserved\n- [ ] No sensitive data was inadvertently captured\n"
    },
    {
      "name": "metadata-schema.md",
      "path": "references/metadata-schema.md",
      "content": "# Metadata Schema\n\nStandard metadata fields, normalization rules, and templates for cataloging ingested context sources.\n\n## Field Definitions\n\n### Required Fields\n\nEvery ingested source must have these fields populated:\n\n| Field | Type | Description | Example |\n|-------|------|-------------|---------|\n| `source_id` | string | Unique identifier, auto-generated | `src_20250115_a3f8` |\n| `title` | string | Descriptive title of the source | \"PostgreSQL 16 JSONB Performance Guide\" |\n| `source_type` | enum | Category of the source | `document`, `web`, `conversation`, `video`, `code` |\n| `url` | string | Canonical URL or file path | `https://example.com/pg16-guide` |\n| `ingested_at` | ISO 8601 | Timestamp of ingestion | `2025-01-15T14:30:00Z` |\n| `relevance_score` | float | Weighted evaluation score (1.0-5.0) | `4.2` |\n| `summary` | string | 1-3 sentence description of content | \"Covers JSONB indexing improvements in PG16...\" |\n\n### Optional Fields\n\nInclude when available to improve retrieval and quality assessment:\n\n| Field | Type | Description | Example |\n|-------|------|-------------|---------|\n| `author` | string | Content creator | \"Jane Smith\" |\n| `published_at` | ISO 8601 | Original publication date | `2024-11-20T00:00:00Z` |\n| `updated_at` | ISO 8601 | Last modification date | `2025-01-10T09:00:00Z` |\n| `language` | ISO 639-1 | Content language | `en` |\n| `tags` | string[] | Topic tags for classification | `[\"postgresql\", \"performance\", \"jsonb\"]` |\n| `authority_score` | int | Authority dimension (1-5) | `4` |\n| `recency_score` | int | Recency dimension (1-5) | `5` |\n| `specificity_score` | int | Specificity dimension (1-5) | `4` |\n| `parent_source_id` | string | ID of a source this was extracted from | `src_20250110_b2c1` |\n| `related_sources` | string[] | IDs of related ingested sources | `[\"src_20250112_d4e5\"]` |\n| `caveats` | string | Known limitations or biases | \"Written by vendor; may favor their product\" |\n| `content_hash` | string | SHA-256 of extracted content | `a3f8c2...` |\n| `word_count` | int | Approximate word count of extracted content | `2400` |\n| `extraction_method` | enum | How content was obtained | `manual`, `fetch`, `upload`, `transcript` |\n\n## Source Type Enum\n\n| Value | Use When |\n|-------|----------|\n| `document` | PDF, Word, Markdown, plain text files |\n| `web` | Web pages fetched by URL |\n| `conversation` | Chat logs, email threads, meeting notes |\n| `video` | Video or audio transcripts |\n| `code` | Source code files or repositories |\n| `api` | API responses or documentation |\n| `image` | Diagrams, screenshots, visual references |\n| `composite` | Source containing multiple formats |\n\n## Normalization Rules\n\n### Dates\n\n- **Always store as ISO 8601 with timezone:** `YYYY-MM-DDTHH:MM:SSZ`\n- **If only a date is known:** Use midnight UTC: `2025-01-15T00:00:00Z`\n- **If only month/year is known:** Use first of month: `2025-01-01T00:00:00Z`\n- **If no date is available:** Omit the field entirely; do not guess\n- **Relative dates** (\"last week\", \"recently\"): Convert to absolute date based on ingestion time and note conversion in `caveats`\n\n### Authors\n\n- **Full name format:** \"First Last\" (no titles, no suffixes unless disambiguation is needed)\n- **Organizations as authors:** Use the organization name: \"Google Cloud Team\"\n- **Multiple authors:** Comma-separated: \"Jane Smith, John Doe\"\n- **Unknown author:** Omit the field; do not use \"Anonymous\" or \"Unknown\"\n- **Usernames:** Prefix with platform: \"github:octocat\", \"twitter:@handle\"\n\n### URLs\n\n- **Always use canonical form:** Strip tracking parameters (`utm_*`, `ref`, `source`)\n- **Prefer HTTPS:** Upgrade HTTP to HTTPS if the site supports it\n- **Remove fragments** unless they point to a specific section relevant to the extraction\n- **Archive URLs:** If original may disappear, also store `archive_url` pointing to web archive\n- **Local files:** Use absolute paths: `file:///Users/home/docs/spec.pdf`\n- **Normalize trailing slashes:** Remove trailing slash unless it changes the resource\n\n### Tags\n\n- **Lowercase only:** `postgresql` not `PostgreSQL`\n- **Hyphenate multi-word:** `machine-learning` not `machine_learning` or `machine learning`\n- **Prefer specific over general:** `react-hooks` over `react` when content is specifically about hooks\n- **Maximum 10 tags per source** to maintain signal value\n- **Use established vocabularies** when available (e.g., language names, framework names)\n\n## Source Metadata Template\n\n```yaml\nsource_id: \"src_YYYYMMDD_XXXX\"\ntitle: \"\"\nsource_type: \"\"           # document | web | conversation | video | code | api | image | composite\nurl: \"\"\nauthor: \"\"                # Optional\npublished_at: \"\"          # Optional, ISO 8601\nupdated_at: \"\"            # Optional, ISO 8601\ningested_at: \"\"           # ISO 8601, auto-populated\nlanguage: \"en\"            # ISO 639-1\ntags: []\nrelevance_score: 0.0      # 1.0 - 5.0\nauthority_score: 0        # 1 - 5, optional\nrecency_score: 0          # 1 - 5, optional\nspecificity_score: 0      # 1 - 5, optional\nsummary: \"\"\ncaveats: \"\"               # Optional\ncontent_hash: \"\"          # SHA-256 of extracted content\nword_count: 0\nextraction_method: \"\"     # manual | fetch | upload | transcript\nparent_source_id: \"\"      # Optional\nrelated_sources: []       # Optional\n```\n\n## ID Generation\n\nSource IDs follow the pattern `src_YYYYMMDD_XXXX` where:\n\n- `YYYYMMDD` is the ingestion date\n- `XXXX` is a 4-character hex suffix from the first 4 chars of the content hash\n\nThis ensures:\n- IDs are chronologically sortable\n- IDs are unique (hash collision in 4 hex chars is acceptable given date prefix)\n- IDs are human-readable and debuggable\n\n## Validation Rules\n\nBefore persisting metadata, validate:\n\n- [ ] `source_id` matches pattern `src_\\d{8}_[0-9a-f]{4}`\n- [ ] `title` is non-empty and under 200 characters\n- [ ] `source_type` is a valid enum value\n- [ ] `url` is a valid URL or absolute file path\n- [ ] `ingested_at` is a valid ISO 8601 timestamp\n- [ ] `relevance_score` is between 1.0 and 5.0\n- [ ] `tags` contains no duplicates and each tag is lowercase hyphenated\n- [ ] `content_hash` is a valid 64-character hex string (if provided)\n- [ ] No required field is null or empty string\n"
    },
    {
      "name": "source-evaluation.md",
      "path": "references/source-evaluation.md",
      "content": "# Source Evaluation Criteria\n\nStandards for assessing reliability, relevance, and value of context sources before ingestion.\n\n## Evaluation Dimensions\n\nEvery source is scored across four dimensions before inclusion:\n\n| Dimension | Weight | Question Answered |\n|-----------|--------|-------------------|\n| Authority | 30% | Is the source credible and authoritative? |\n| Recency | 25% | Is the information current enough for the task? |\n| Specificity | 25% | Does it directly address the topic at hand? |\n| Accessibility | 20% | Can the content be reliably extracted and referenced? |\n\n## Authority Scoring\n\n| Score | Criteria | Examples |\n|-------|----------|----------|\n| 5 - Definitive | Primary source, official documentation | RFC specs, official API docs, court rulings |\n| 4 - Strong | Recognized expert or institution | Peer-reviewed papers, vendor engineering blogs |\n| 3 - Credible | Known practitioner, reputable outlet | Conference talks, established tech publications |\n| 2 - Informal | Community-generated, unverified | Stack Overflow answers, forum posts, tutorials |\n| 1 - Weak | Anonymous, unattributed, or AI-generated | Uncited blog posts, scraped aggregator content |\n\n## Recency Scoring\n\n| Score | Age of Content | Acceptable When |\n|-------|---------------|-----------------|\n| 5 | < 3 months | Always preferred |\n| 4 | 3-12 months | Topic is not rapidly evolving |\n| 3 | 1-2 years | Foundational concepts, stable APIs |\n| 2 | 2-5 years | Historical context, mature standards |\n| 1 | > 5 years | Only if no newer source exists and content is canonical |\n\n**Override rule:** Recency is irrelevant for mathematical proofs, RFCs, and language specifications. Score these as 5 regardless of age.\n\n## Specificity Scoring\n\n| Score | Criteria |\n|-------|----------|\n| 5 | Directly answers the exact question or covers the precise topic |\n| 4 | Covers the topic with minor tangential content |\n| 3 | Covers a broader area that includes the topic |\n| 2 | Adjacent topic that requires inference to apply |\n| 1 | Only loosely related; requires significant interpretation |\n\n## Quality Red Flags\n\nDowngrade or exclude sources exhibiting these indicators:\n\n| Red Flag | Action | Reason |\n|----------|--------|--------|\n| No author attribution | Downgrade authority by 2 | Accountability matters |\n| No publication date | Downgrade recency to 2 max | Cannot verify currency |\n| Broken external links | Downgrade authority by 1 | Poor maintenance signal |\n| Contradicts 3+ other sources | Flag for manual review | Likely outdated or incorrect |\n| Marketing language dominates | Downgrade specificity by 2 | Promotional bias |\n| Content behind login wall | Downgrade accessibility to 1 | Cannot reliably re-fetch |\n| AI-generated without review | Downgrade authority to 1 | Hallucination risk |\n\n## Quality Positive Indicators\n\nUpgrade sources exhibiting these signals:\n\n| Indicator | Action |\n|-----------|--------|\n| Includes working code examples | Upgrade specificity by 1 |\n| Cites its own sources | Upgrade authority by 1 |\n| Has a changelog or revision history | Upgrade recency confidence |\n| Peer-reviewed or editor-reviewed | Upgrade authority by 1 |\n| Maintained in version control | Upgrade recency confidence |\n\n## Inclusion/Exclusion Decision Matrix\n\nCalculate the weighted score: `(Authority * 0.3) + (Recency * 0.25) + (Specificity * 0.25) + (Accessibility * 0.2)`\n\n| Weighted Score | Decision | Action |\n|----------------|----------|--------|\n| 4.0 - 5.0 | Include | Ingest as primary source |\n| 3.0 - 3.9 | Include with caveat | Ingest, note limitations in metadata |\n| 2.0 - 2.9 | Conditional | Include only if no better source exists for the topic |\n| 1.0 - 1.9 | Exclude | Do not ingest; document why if it was considered |\n\n## Conflict Resolution\n\nWhen two sources disagree on facts:\n\n1. **Higher authority wins** - Official docs override blog posts\n2. **More recent wins** (if authority is equal) - Newer content reflects current state\n3. **More specific wins** (if authority and recency are equal) - Targeted content is more reliable\n4. **Flag for human review** if all dimensions are comparable - Do not silently pick one\n\n## Source Evaluation Checklist\n\nBefore ingesting any source, confirm:\n\n- [ ] Authority score assigned (1-5)\n- [ ] Recency score assigned (1-5) with publication date recorded\n- [ ] Specificity score assigned (1-5) relative to the current task\n- [ ] Accessibility confirmed (content can be extracted and re-accessed)\n- [ ] Red flags checked and documented if present\n- [ ] Weighted score meets inclusion threshold (>= 2.0)\n- [ ] Conflicts with existing sources identified and resolved\n"
    },
    {
      "name": "url-fetch-guidelines.md",
      "path": "references/url-fetch-guidelines.md",
      "content": "# URL Fetch Guidelines\n\nStandards for validating, fetching, extracting, and caching web content during context ingestion.\n\n## URL Validation and Normalization\n\n### Validation Checks\n\nBefore attempting any fetch, validate the URL:\n\n| Check | Rule | Action on Failure |\n|-------|------|-------------------|\n| Scheme present | Must start with `http://` or `https://` | Prepend `https://` |\n| Valid domain | Must resolve to a routable address | Reject; log as unreachable |\n| No credentials in URL | Must not contain `user:pass@` | Strip credentials; warn |\n| Reasonable length | Under 2048 characters | Reject; likely malformed |\n| No local/private IPs | Must not point to `127.0.0.1`, `10.*`, `192.168.*` | Reject; security boundary |\n| Encoding correct | Special characters properly percent-encoded | Re-encode with standard library |\n\n### Normalization Steps\n\nApply in order before fetching or comparing URLs:\n\n1. **Lowercase the scheme and host:** `HTTPS://Example.COM` becomes `https://example.com`\n2. **Remove default ports:** Strip `:443` for HTTPS, `:80` for HTTP\n3. **Remove tracking parameters:** Strip `utm_*`, `ref`, `source`, `fbclid`, `gclid`\n4. **Remove trailing slash** on paths (unless root `/`)\n5. **Sort remaining query parameters** alphabetically for consistent comparison\n6. **Resolve path segments:** Convert `/a/../b/` to `/b/`\n7. **Upgrade HTTP to HTTPS** when the domain is known to support it\n\n**Canonical URL example:**\n```\nInput:  HTTP://Example.COM:80/docs/../api/v2/?ref=twitter&utm_source=blog&format=json\nOutput: https://example.com/api/v2?format=json\n```\n\n## Content Extraction from Web Pages\n\n### Standard Extraction Pipeline\n\n1. **Fetch raw HTML** with appropriate headers (see request configuration below)\n2. **Parse HTML** and locate the main content area\n3. **Convert to Markdown** preserving headings, lists, tables, code blocks, and links\n4. **Strip non-content elements:** navigation, sidebars, ads, footers, cookie banners\n5. **Extract metadata** from `<head>`: title, description, author, canonical URL, published date\n6. **Record extraction metadata:** fetch timestamp, HTTP status, content length\n\n### Request Configuration\n\n| Header | Value | Purpose |\n|--------|-------|---------|\n| `User-Agent` | `ContextIngestion/1.0 (compatible)` | Identify the bot; respect robots.txt |\n| `Accept` | `text/html,application/xhtml+xml` | Request HTML content |\n| `Accept-Language` | `en-US,en;q=0.9` | Prefer English content |\n| `Accept-Encoding` | `gzip, deflate` | Reduce transfer size |\n\n### Handling Special Page Types\n\n| Page Type | Detection | Strategy |\n|-----------|-----------|----------|\n| JS-rendered SPA | Empty or minimal `<body>` after initial fetch | Use headless browser fetch (Playwright/Puppeteer) as fallback |\n| Paywalled content | HTTP 402, login redirect, or paywall modal detected | Record URL and title only; mark `extraction_method: blocked`; do not attempt bypass |\n| Rate-limited | HTTP 429 response | Respect `Retry-After` header; exponential backoff starting at 5 seconds |\n| Redirected | HTTP 301/302 | Follow up to 5 redirects; record both original and final URL |\n| PDF served as web | `Content-Type: application/pdf` | Download and process with PDF extraction pipeline |\n| API/JSON response | `Content-Type: application/json` | Store structured data directly; no HTML parsing |\n| Large page (>1MB) | Content-Length or streamed size exceeds 1MB | Truncate to first 1MB; note truncation in caveats |\n| Geo-blocked | HTTP 403 with geographic indicators | Record as inaccessible; note region restriction |\n\n### robots.txt Compliance\n\n- **Always check** `/robots.txt` before first fetch to a new domain\n- **Respect disallow rules** for the configured User-Agent\n- **Cache robots.txt** per domain for 24 hours\n- **If robots.txt blocks access:** Record URL and title only; mark as `extraction_method: robots_blocked`\n\n## Fallback Strategies\n\nWhen the primary fetch fails, attempt fallbacks in order:\n\n| Priority | Strategy | When to Use | Limitations |\n|----------|----------|-------------|-------------|\n| 1 | Retry with backoff | Transient errors (5xx, timeout, network) | Max 3 retries; 5s, 15s, 45s delays |\n| 2 | Headless browser fetch | JS-rendered content, empty body | Slower; higher resource cost |\n| 3 | Web archive lookup | Page is down or removed | Content may be stale |\n| 4 | Google Cache | Recent page unavailable | May not exist; ephemeral |\n| 5 | Alternative URL | Same content at a different URL (mirror, CDN) | Requires known alternatives |\n| 6 | Manual flag | All automated methods fail | Record URL; mark for human retrieval |\n\n### Web Archive Lookup\n\nUse `https://archive.org/wayback/available?url={url}` to check availability, then fetch from `https://web.archive.org/web/{url}`. Record `archive_url`, `archive_date`, and add caveat: \"Content retrieved from web archive; may not reflect current state.\"\n\n## Caching and Freshness Policies\n\n### Cache Storage\n\n| Field | Description |\n|-------|-------------|\n| `cache_key` | Normalized URL (post-normalization) |\n| `cached_at` | ISO 8601 timestamp of last successful fetch |\n| `content_hash` | SHA-256 of fetched content |\n| `http_etag` | ETag header from response (if provided) |\n| `http_last_modified` | Last-Modified header (if provided) |\n| `ttl_hours` | Time-to-live before re-fetch is required |\n\n### Freshness Rules\n\n| Content Type | Default TTL | Rationale |\n|-------------|-------------|-----------|\n| API documentation | 72 hours | Updated infrequently; versioned |\n| Blog posts | 168 hours (7 days) | Rarely edited after publication |\n| News articles | 24 hours | May receive corrections |\n| GitHub README/docs | 48 hours | Active repos update frequently |\n| Stack Overflow answers | 168 hours | Edits are infrequent after initial period |\n| Official specs (RFC, W3C) | 720 hours (30 days) | Extremely stable |\n| Dynamic dashboards/status pages | 1 hour | Data changes constantly |\n\n### Conditional Fetch\n\nWhen re-fetching cached content, use conditional requests to save bandwidth:\n\n1. **If ETag is stored:** Send `If-None-Match: {etag}` header\n2. **If Last-Modified is stored:** Send `If-Modified-Since: {date}` header\n3. **On HTTP 304 (Not Modified):** Update `cached_at`, keep existing content\n4. **On HTTP 200:** Compare content hash; if changed, update cache and re-extract\n\n### Cache Invalidation\n\nForce re-fetch (ignore cache) when:\n\n- User explicitly requests fresh content\n- Source is referenced in a contradiction that needs resolution\n- More than 2x the TTL has elapsed since last fetch\n- The source is known to have been updated (e.g., changelog entry discovered)\n\n## Fetch Checklist\n\nFor every URL fetch during ingestion:\n\n- [ ] URL validated and normalized\n- [ ] robots.txt checked for the domain\n- [ ] Cache checked; conditional fetch headers set if cached\n- [ ] Request sent with proper headers and timeout (30 second default)\n- [ ] Response status handled (2xx proceed, 3xx follow, 4xx/5xx fallback)\n- [ ] Content extracted and converted to Markdown\n- [ ] Metadata extracted from page headers and HTML head\n- [ ] Cache updated with new content, hash, and headers\n- [ ] Extraction metadata recorded (method, timestamp, status code)\n"
    }
  ],
  "tags": [
    "planning",
    "research",
    "intake",
    "sources",
    "information-gathering"
  ],
  "dependsOn": []
}