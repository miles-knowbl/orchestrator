{
  "id": "quality-eval-design",
  "name": "quality-eval-design",
  "version": "1.0.0",
  "description": "Design content and UX quality evaluation frameworks. Creates scoring rubrics, defines evaluation methodology, and produces eval documentation that can be used for ongoing quality assessment.",
  "phase": "REVIEW",
  "category": "core",
  "content": "# Quality Eval Design\n\nDesign content and UX quality evaluation frameworks.\n\n## When to Use\n\n- **After pipeline discovery** — Runs in REVIEW phase once pipelines are known\n- **Creating quality standards** — Define what \"good\" looks like for this system\n- **Enabling ongoing evaluation** — Produce reusable eval frameworks\n- When you say: \"design quality evals\", \"create scoring rubrics\", \"define quality standards\"\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `content-dimensions.md` | Common content quality dimensions |\n| `ux-dimensions.md` | UX quality dimensions |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `eval-methodology.md` | How to run evaluations |\n\n**Verification:** Eval frameworks are specific to what the system produces.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| `CONTENT-QUALITY-EVALS.md` | Project root | If system generates content |\n| `UX-QUALITY-EVALS.md` | Project root | Always (UI exists) |\n\n## Core Concept\n\nQuality Eval Design answers: **\"What does 'good' look like for this system?\"**\n\nEvaluations should be:\n- **System-specific** — Tailored to what this system produces\n- **Measurable** — 5-point scale with clear rubrics\n- **Evidence-based** — Examples of each score level\n- **Reusable** — Can be run repeatedly over time\n\n## Eval Design Process\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│               QUALITY EVAL DESIGN PROCESS                   │\n│                                                             │\n│  1. IDENTIFY OUTPUTS                                        │\n│     └─→ What does this system produce?                     │\n│                                                             │\n│  2. DETERMINE CATEGORIES                                    │\n│     ├─→ Content? (text, images, data)                      │\n│     └─→ UX? (always yes for interactive systems)          │\n│                                                             │\n│  3. SELECT DIMENSIONS                                       │\n│     └─→ What matters for each output type?                 │\n│                                                             │\n│  4. DEFINE WEIGHTS                                          │\n│     └─→ How important is each dimension?                   │\n│                                                             │\n│  5. CREATE RUBRICS                                          │\n│     └─→ What does 1-5 look like for each dimension?        │\n│                                                             │\n│  6. SET FLOORS                                              │\n│     └─→ What's the minimum acceptable score?               │\n│                                                             │\n│  7. DOCUMENT EXAMPLES                                       │\n│     └─→ Concrete examples of each score level              │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Content Quality Dimensions\n\nCommon dimensions for content-generating systems:\n\n| Dimension | Description | Applies To |\n|-----------|-------------|------------|\n| voice_fidelity | Matches intended voice/persona | All generated text |\n| topic_relevance | Addresses intended topic | All generated text |\n| engagement | Interesting, holds attention | Social, marketing |\n| accuracy | Factually correct | Informational |\n| clarity | Easy to understand | Documentation, instructions |\n| coherence | Logical flow and structure | Long-form content |\n| originality | Fresh, not generic | Creative content |\n\n## UX Quality Dimensions\n\nCommon dimensions for interactive systems:\n\n| Dimension | Description | Weight |\n|-----------|-------------|--------|\n| responsiveness | UI responds quickly | 25% |\n| feedback_clarity | Clear loading/success/error states | 25% |\n| error_recovery | User can recover from errors | 20% |\n| state_consistency | UI shows current state | 20% |\n| accessibility | Keyboard, screen reader, contrast | 10% |\n\n## Rubric Template\n\n```markdown\n### dimension_name\n- **Weight:** NN%\n- **Floor:** N.N\n- **Description:** What this dimension measures\n\n#### Scoring Rubric\n| Score | Label | Definition |\n|-------|-------|------------|\n| 5 | Excellent | Exceeds expectations; delightful |\n| 4 | Good | Meets expectations; minor issues |\n| 3 | Acceptable | Functional; has rough edges |\n| 2 | Poor | Below expectations; frustrating |\n| 1 | Failed | Does not meet basic requirements |\n\n#### Evidence Examples\n- Score 5: \"{Concrete example of excellent}\"\n- Score 3: \"{Concrete example of acceptable}\"\n- Score 1: \"{Concrete example of failed}\"\n```\n\n## Output Format\n\n### CONTENT-QUALITY-EVALS.md\n\n```markdown\n# Content Quality Evaluations\n\nMeasures the quality of AI-generated content in [system name].\n\n## Content Types\n\n| Type | Pipeline | Description |\n|------|----------|-------------|\n| Twitter Thread | P2 | Generated tweet threads |\n| Article | P2 | Long-form articles |\n| Caption | P3 | Image captions |\n\n## Dimensions\n\n### voice_fidelity\n- **Weight:** 40%\n- **Floor:** 2.5\n- **Description:** Generated content matches the intended voice and persona\n\n#### Scoring Rubric\n| Score | Label | Definition |\n|-------|-------|------------|\n| 5 | Excellent | Indistinguishable from target voice; natural, authentic |\n| 4 | Good | Clearly in target voice; minor inconsistencies |\n| 3 | Acceptable | Generally correct voice; occasionally generic |\n| 2 | Poor | Voice inconsistent; often wrong |\n| 1 | Failed | No resemblance to target voice |\n\n#### Evidence Examples\n- Score 5: \"Thread captures creator's signature humor and catchphrases naturally\"\n- Score 3: \"Thread is professional but lacks creator's distinctive style\"\n- Score 1: \"Thread sounds like corporate press release despite casual persona\"\n\n[... more dimensions ...]\n\n## Evaluation Methodology\n\n1. Sample 5 outputs from each content type\n2. Score each dimension 1-5\n3. Calculate weighted average\n4. Compare against quality gates\n```\n\n### UX-QUALITY-EVALS.md\n\n```markdown\n# UX Quality Evaluations\n\nMeasures the user experience of [system name].\n\n## Dimensions\n\n### responsiveness\n- **Weight:** 25%\n- **Floor:** 3.0\n- **Description:** UI responds quickly to user actions\n\n#### Scoring Rubric\n| Score | Label | Definition |\n|-------|-------|------------|\n| 5 | Excellent | Instant response (<100ms); feels native |\n| 4 | Good | Quick response (<500ms); smooth |\n| 3 | Acceptable | Noticeable delay (<2s); usable |\n| 2 | Poor | Slow response (2-5s); frustrating |\n| 1 | Failed | Very slow or hangs (>5s) |\n\n[... more dimensions ...]\n\n## Evaluation Methodology\n\n1. Walk through each U-series pipeline\n2. Score each dimension at each step\n3. Note specific issues as evidence\n4. Calculate weighted average\n```\n\n## Quality Gates\n\nDefine ship thresholds:\n\n| Weighted Score | Status | Action |\n|----------------|--------|--------|\n| >= 4.0 | Ship | Ready to launch |\n| 3.0 - 4.0 | Polish then ship | Address gaps first |\n| < 3.0 | Fix before ship | Significant issues |\n\n## Validation Checklist\n\n- [ ] All system outputs identified\n- [ ] Relevant categories selected (content/UX)\n- [ ] Dimensions tailored to this system\n- [ ] Weights sum to 100% per category\n- [ ] Rubrics have clear score definitions\n- [ ] Floors set for each dimension\n- [ ] Examples provided where helpful\n- [ ] Methodology documented",
  "references": [
    {
      "name": "content-dimensions.md",
      "path": "references/content-dimensions.md",
      "content": "# Content Dimensions\n\nQuality dimensions for evaluating generated content.\n\n## Core Dimensions\n\n### voice_fidelity\n**What it measures:** Does the output match the intended voice/persona?\n\n**Applies to:** Any system with a defined voice or persona\n\n**Indicators:**\n- Vocabulary matches target\n- Tone is appropriate\n- Personality markers present\n- Consistency across outputs\n\n### topic_relevance\n**What it measures:** Does the content address the intended topic?\n\n**Applies to:** All generated text\n\n**Indicators:**\n- On-topic throughout\n- Key points covered\n- No off-topic tangents\n- Appropriate depth\n\n### engagement\n**What it measures:** Is the content interesting and attention-holding?\n\n**Applies to:** Social media, marketing, entertainment\n\n**Indicators:**\n- Compelling hooks\n- Varied structure\n- Emotional resonance\n- Call to action effective\n\n### accuracy\n**What it measures:** Is the content factually correct?\n\n**Applies to:** Informational, educational, news\n\n**Indicators:**\n- Facts verifiable\n- No hallucinations\n- Sources credited\n- Claims supported\n\n### clarity\n**What it measures:** Is the content easy to understand?\n\n**Applies to:** Documentation, instructions, explanations\n\n**Indicators:**\n- Simple language\n- Logical structure\n- Terms defined\n- No ambiguity\n\n### coherence\n**What it measures:** Does the content flow logically?\n\n**Applies to:** Long-form content, articles\n\n**Indicators:**\n- Smooth transitions\n- Consistent thread\n- No contradictions\n- Clear progression\n\n### originality\n**What it measures:** Is the content fresh and unique?\n\n**Applies to:** Creative content\n\n**Indicators:**\n- Novel angles\n- Unique phrasing\n- Not templated\n- Avoids clichés\n\n## Domain-Specific Dimensions\n\n### For Social Media\n- **shareability:** Would people share this?\n- **platform_fit:** Optimized for the platform?\n- **timing_relevance:** Timely/current?\n\n### For Marketing\n- **brand_alignment:** Matches brand guidelines?\n- **cta_effectiveness:** Clear call to action?\n- **value_proposition:** Benefit clear?\n\n### For Technical\n- **completeness:** All necessary info included?\n- **actionability:** Reader can act on it?\n- **code_correctness:** Examples work?\n\n## Weight Guidelines\n\n| Use Case | Primary Dimensions |\n|----------|-------------------|\n| Social Media | engagement (40%), voice (30%), relevance (30%) |\n| Documentation | clarity (40%), accuracy (30%), completeness (30%) |\n| Marketing | engagement (35%), brand (35%), cta (30%) |\n| News/Info | accuracy (50%), clarity (30%), relevance (20%) |\n\n## Dimension Selection\n\nChoose dimensions based on:\n1. What the system produces\n2. Who consumes it\n3. What success looks like\n4. What failures would be\n\n**Rule:** 3-5 dimensions per category is ideal.\n"
    },
    {
      "name": "eval-methodology.md",
      "path": "references/eval-methodology.md",
      "content": "# Evaluation Methodology\n\nHow to run quality evaluations.\n\n## Content Evaluation Process\n\n### 1. Sampling\nSelect representative outputs:\n- 5-10 samples per content type\n- Include variety (different topics, lengths)\n- Include edge cases\n- Recent outputs preferred\n\n### 2. Blind Evaluation\nIf possible, evaluate without knowing source:\n- Removes bias toward \"our\" content\n- More objective scoring\n\n### 3. Per-Dimension Scoring\nFor each sample, for each dimension:\n1. Read/review the content\n2. Apply rubric criteria\n3. Assign score 1-5\n4. Note specific evidence\n\n### 4. Evidence Documentation\n```yaml\nsample_id: tweet-001\ndimension: voice_fidelity\nscore: 3\nevidence:\n  - \"Uses correct terminology\"\n  - \"Misses signature humor\"\n  - \"Slightly too formal\"\n```\n\n### 5. Aggregation\n```\nSample Score = Σ(dimension_score × dimension_weight)\nContent Type Score = Average(all sample scores)\nCategory Score = Average(all content type scores)\n```\n\n## UX Evaluation Process\n\n### 1. Flow Selection\nIdentify flows to evaluate:\n- Each U-series pipeline\n- Critical user journeys\n- Recently changed areas\n\n### 2. Walkthrough\nFor each flow:\n1. Start from trigger point\n2. Step through entire flow\n3. At each step, assess each dimension\n4. Note issues encountered\n\n### 3. Dimension Scoring\n```yaml\npipeline: U1-Chat-to-Edit\nstep: 5. SSE stream response\ndimensions:\n  responsiveness: 4  # Stream starts quickly\n  feedback_clarity: 2  # No loading indicator\n  error_recovery: 1  # No retry on disconnect\n  state_consistency: 3  # Usually shows current\n  accessibility: 4  # Keyboard accessible\n```\n\n### 4. Overall Scoring\n```\nStep Score = Lowest dimension score at that step\nPipeline Score = Weighted average of dimensions across steps\nOverall UX = Weighted average of all pipelines\n```\n\n## Scoring Calibration\n\n### Before Evaluating\n- Review rubric definitions\n- Look at example scores\n- Align on edge cases\n\n### During Evaluation\n- Use half-points (3.5) for borderline\n- Note uncertainty\n- Don't inflate/deflate consistently\n\n### After Evaluation\n- Compare scores across evaluators\n- Discuss discrepancies\n- Calibrate future evaluations\n\n## Evaluation Frequency\n\n| Context | Frequency |\n|---------|-----------|\n| Initial audit | Once |\n| After major changes | Each release |\n| Ongoing monitoring | Monthly sample |\n| Quality regression | On issue report |\n\n## Reporting Format\n\n```markdown\n## Quality Evaluation: [Date]\n\n### Summary\n| Category | Score | Target | Status |\n|----------|-------|--------|--------|\n| Content | 3.4 | 3.5 | ⚠️ Gap |\n| UX | 3.8 | 3.5 | ✓ Met |\n| Overall | 3.6 | 3.5 | ✓ Met |\n\n### Gaps Identified\n1. voice_fidelity: 3.2 (floor 2.5) - Acceptable but needs polish\n2. feedback_clarity: 2.8 (floor 3.0) - GAP\n\n### Recommendations\n1. Improve voice prompts for more personality\n2. Add loading indicators to all async operations\n```\n"
    },
    {
      "name": "ux-dimensions.md",
      "path": "references/ux-dimensions.md",
      "content": "# UX Dimensions\n\nQuality dimensions for evaluating user experience.\n\n## Core Dimensions\n\n### responsiveness\n**What it measures:** Does the UI respond quickly to actions?\n\n**Weight:** 25%\n**Floor:** 3.0\n\n**Scoring:**\n- 5: Instant (<100ms)\n- 4: Quick (<500ms)\n- 3: Noticeable (<2s)\n- 2: Slow (2-5s)\n- 1: Hanging (>5s)\n\n**What to check:**\n- Button click response\n- Page load time\n- Form submission\n- Data fetching\n\n### feedback_clarity\n**What it measures:** Are loading, success, and error states clear?\n\n**Weight:** 25%\n**Floor:** 3.0\n\n**Scoring:**\n- 5: Every action has clear, helpful feedback\n- 4: Most actions have feedback\n- 3: Basic feedback present\n- 2: Feedback often missing or confusing\n- 1: No feedback, user guesses state\n\n**What to check:**\n- Loading indicators present?\n- Success confirmation shown?\n- Error messages helpful?\n- Progress visible for long operations?\n\n### error_recovery\n**What it measures:** Can users recover from errors easily?\n\n**Weight:** 20%\n**Floor:** 2.5\n\n**Scoring:**\n- 5: Errors rare, recovery effortless\n- 4: Clear recovery path\n- 3: Can recover with effort\n- 2: Recovery difficult\n- 1: Stuck, must refresh/restart\n\n**What to check:**\n- Retry buttons present?\n- Error messages actionable?\n- Form data preserved on error?\n- Undo available?\n\n### state_consistency\n**What it measures:** Does the UI show current, accurate state?\n\n**Weight:** 20%\n**Floor:** 2.5\n\n**Scoring:**\n- 5: Always accurate, real-time\n- 4: Accurate, minor delays\n- 3: Usually accurate\n- 2: Often stale\n- 1: Frequently wrong\n\n**What to check:**\n- Cache invalidation working?\n- Real-time updates arriving?\n- Optimistic updates correct?\n- Navigation preserves state?\n\n### accessibility\n**What it measures:** Is the UI usable by everyone?\n\n**Weight:** 10%\n**Floor:** 3.0\n\n**Scoring:**\n- 5: WCAG AA compliant, excellent a11y\n- 4: Good keyboard/screen reader support\n- 3: Basic accessibility\n- 2: Some issues\n- 1: Major barriers\n\n**What to check:**\n- Keyboard navigation works?\n- Screen reader announces correctly?\n- Color contrast sufficient?\n- Focus indicators visible?\n\n## Additional Dimensions\n\n### intuitiveness\n**What it measures:** Can users figure it out without help?\n\n**Applies to:** New or complex features\n\n### efficiency\n**What it measures:** Can tasks be completed quickly?\n\n**Applies to:** Productivity apps, frequent actions\n\n### discoverability\n**What it measures:** Can users find features?\n\n**Applies to:** Feature-rich applications\n\n### satisfaction\n**What it measures:** Do users enjoy using it?\n\n**Applies to:** Consumer apps, optional usage\n\n## Weight Adjustment\n\nAdjust weights based on context:\n\n| Context | Emphasize |\n|---------|-----------|\n| Real-time app | responsiveness, state_consistency |\n| Form-heavy | feedback_clarity, error_recovery |\n| Public-facing | accessibility |\n| Power users | efficiency, shortcuts |\n\n## Evaluation Method\n\n1. **Walk through each U-series pipeline**\n2. **At each step, check each dimension:**\n   - Is it responsive here?\n   - Is feedback clear here?\n   - Can user recover from errors here?\n   - Is state accurate here?\n   - Is it accessible here?\n3. **Score overall dimension 1-5**\n4. **Note specific issues as evidence**\n"
    }
  ],
  "tags": [
    "audit",
    "quality",
    "evaluation",
    "content",
    "ux"
  ],
  "dependsOn": [
    "pipeline-discovery",
    "ui-pipeline-discovery"
  ]
}