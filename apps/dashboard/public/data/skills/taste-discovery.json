{
  "id": "taste-discovery",
  "name": "taste-discovery",
  "version": "1.0.0",
  "description": "Find and load project-specific taste evaluations. Discovers eval definitions through manifest, convention, or defaults. Provides the foundation for taste-first auditing by identifying what quality dimensions matter for this specific project.",
  "phase": "TASTE",
  "category": "core",
  "content": "# Taste Discovery\n\nFind and load project-specific taste evaluations.\n\n## When to Use\n\n- **Starting an audit** — First skill in TASTE phase, runs before any evaluation\n- **New project audit** — Discover what quality dimensions the project cares about\n- **Custom eval setup** — When project has defined its own quality evals\n- When you say: \"what quality dimensions matter here?\", \"find the evals\", \"discover taste criteria\"\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `taste-manifest-schema.md` | Understand manifest format if one exists |\n| `eval-file-format.md` | Parse *-QUALITY-EVALS.md files correctly |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `minimal-defaults.md` | When no project evals found |\n\n**Verification:** Ensure discovery source is identified and dimensions are parsed.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| Discovery report | audit-state.json | Always (taste.eval_source field) |\n| Parsed dimensions | audit-state.json | Always (taste.dimensions array) |\n\n## Core Concept\n\nTaste Discovery answers: **\"What does 'good' mean for this project?\"**\n\nDifferent projects have different quality criteria:\n- A tweet generator cares about voice fidelity and engagement\n- A documentation tool cares about clarity and completeness\n- A data pipeline cares about accuracy and freshness\n\nThis skill finds project-specific definitions rather than imposing generic standards.\n\n## Discovery Order\n\nThe skill searches for eval definitions in priority order (first match wins):\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    DISCOVERY ORDER                          │\n│                                                             │\n│  1. .claude/taste-manifest.json                             │\n│     └─→ Explicit manifest with full configuration           │\n│                                                             │\n│  2. TASTE-EVALS.md                                          │\n│     └─→ Single-file eval definition in project root         │\n│                                                             │\n│  3. *-QUALITY-EVALS.md (glob pattern)                       │\n│     └─→ Convention: CONTENT-QUALITY-EVALS.md, etc.          │\n│                                                             │\n│  4. Minimal Defaults                                        │\n│     └─→ 4 UX dimensions if nothing else found               │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Step 1: Check for Manifest\n\nLook for `.claude/taste-manifest.json`:\n\n```json\n{\n  \"version\": \"1.0.0\",\n  \"eval_files\": [\n    \"CONTENT-QUALITY-EVALS.md\",\n    \"UX-QUALITY-EVALS.md\"\n  ],\n  \"category_weights\": {\n    \"content\": 0.6,\n    \"ux\": 0.4\n  },\n  \"quality_gates\": {\n    \"ship\": 4.0,\n    \"polish\": 3.0,\n    \"fix\": 2.5\n  }\n}\n```\n\nIf found, load referenced eval files and apply weights.\n\n## Step 2: Check for TASTE-EVALS.md\n\nLook for `TASTE-EVALS.md` in project root. This is a single-file format containing all dimensions:\n\n```markdown\n# Taste Evaluations\n\n## Dimensions\n\n### voice_fidelity\n- **Category:** content\n- **Weight:** 40%\n- **Floor:** 2.5\n- **Description:** Output matches the intended voice/persona\n\n### engagement\n- **Category:** content\n- **Weight:** 30%\n- **Floor:** 2.5\n- **Description:** Content is engaging and interesting\n```\n\n## Step 3: Check for Convention Files\n\nGlob for `*-QUALITY-EVALS.md` in project root:\n\n| Pattern | Example |\n|---------|---------|\n| `CONTENT-QUALITY-EVALS.md` | Content generation quality |\n| `UX-QUALITY-EVALS.md` | User experience quality |\n| `API-QUALITY-EVALS.md` | API design quality |\n| `DATA-QUALITY-EVALS.md` | Data pipeline quality |\n\nParse each file for dimensions and combine into unified dimension list.\n\n## Step 4: Apply Minimal Defaults\n\nIf no evals found, apply defaults:\n\n| Dimension | Weight | Floor | Description |\n|-----------|--------|-------|-------------|\n| Usability | 35% | 2.5 | Can users accomplish their goals? |\n| Responsiveness | 25% | 2.5 | Does UI respond quickly? |\n| Reliability | 25% | 2.5 | Does it work consistently? |\n| Accessibility | 15% | 2.5 | Keyboard nav, screen reader, contrast |\n\n**Inform user:** When defaults are used, show guidance on adding custom evals.\n\n## Output Format\n\nUpdate `audit-state.json`:\n\n```json\n{\n  \"taste\": {\n    \"eval_source\": \"convention\",\n    \"eval_files\": [\"CONTENT-QUALITY-EVALS.md\", \"UX-QUALITY-EVALS.md\"],\n    \"dimensions\": [\n      {\n        \"name\": \"voice_fidelity\",\n        \"category\": \"content\",\n        \"weight\": 0.40,\n        \"floor\": 2.5,\n        \"description\": \"Output matches the intended voice/persona\"\n      }\n    ],\n    \"category_weights\": {\n      \"content\": 0.6,\n      \"ux\": 0.4\n    },\n    \"quality_gates\": {\n      \"ship\": 3.5,\n      \"polish\": 2.5,\n      \"fix\": 2.5\n    }\n  }\n}\n```\n\n## Discovery Message Templates\n\n**When manifest found:**\n```\nTaste Discovery: Found .claude/taste-manifest.json\n  Loading: 2 eval files, 2 categories\n  Dimensions: 6 total (4 content, 2 ux)\n```\n\n**When convention files found:**\n```\nTaste Discovery: Found 2 convention files\n  - CONTENT-QUALITY-EVALS.md (4 dimensions)\n  - UX-QUALITY-EVALS.md (5 dimensions)\n  Dimensions: 9 total\n```\n\n**When using defaults:**\n```\n═══════════════════════════════════════════════════════════════\n║  TASTE DISCOVERY                                            ║\n║                                                             ║\n║  No project-specific taste evals found.                     ║\n║  Using minimal defaults (4 UX dimensions).                  ║\n║                                                             ║\n║  To add custom evals:                                       ║\n║                                                             ║\n║  Option 1: Convention files                                 ║\n║    Create *-QUALITY-EVALS.md in project root                ║\n║                                                             ║\n║  Option 2: Single eval file                                 ║\n║    Create TASTE-EVALS.md in project root                    ║\n║                                                             ║\n║  Option 3: Explicit manifest                                ║\n║    Create .claude/taste-manifest.json                       ║\n═══════════════════════════════════════════════════════════════\n```\n\n## Validation\n\nBefore completing, verify:\n\n- [ ] Discovery source is identified (manifest | taste-evals | convention | defaults)\n- [ ] All dimension names are unique\n- [ ] All weights sum to 1.0 within each category\n- [ ] All floors are between 1.0 and 5.0\n- [ ] Quality gates are defined (ship, polish, fix thresholds)\n\n## Common Issues\n\n| Issue | Resolution |\n|-------|------------|\n| Weights don't sum to 1.0 | Normalize weights within category |\n| Missing floor | Default to 2.5 |\n| Duplicate dimension names | Prefix with category (content_voice, ux_voice) |\n| Empty eval file | Skip file, warn user |",
  "references": [
    {
      "name": "eval-file-format.md",
      "path": "references/eval-file-format.md",
      "content": "# Eval File Format\n\nQuality eval files (`*-QUALITY-EVALS.md`) define dimensions for taste evaluation.\n\n## File Structure\n\n```markdown\n# [Category] Quality Evaluations\n\nBrief description of what this category measures.\n\n## Dimensions\n\n### dimension_name\n- **Weight:** NN%\n- **Floor:** N.N\n- **Description:** What this dimension measures\n\n#### Scoring Rubric\n| Score | Label | Definition |\n|-------|-------|------------|\n| 5 | Excellent | ... |\n| 4 | Good | ... |\n| 3 | Acceptable | ... |\n| 2 | Poor | ... |\n| 1 | Failed | ... |\n\n#### Evidence Examples\n- Score 5: \"Example of excellent...\"\n- Score 3: \"Example of acceptable...\"\n- Score 1: \"Example of failed...\"\n```\n\n## Required Fields\n\n### dimension_name (heading)\nUnique identifier, lowercase with underscores. Example: `voice_fidelity`\n\n### Weight\nPercentage weight within this category. All weights in a file should sum to 100%.\n\n### Floor\nMinimum acceptable score (1.0-5.0). Below floor = gap.\n\n### Description\nOne sentence explaining what this dimension measures.\n\n## Optional Fields\n\n### Scoring Rubric\n5-point scale with definitions. If omitted, generic rubric applies.\n\n### Evidence Examples\nConcrete examples of what each score level looks like.\n\n## Example: CONTENT-QUALITY-EVALS.md\n\n```markdown\n# Content Quality Evaluations\n\nMeasures the quality of AI-generated content outputs.\n\n## Dimensions\n\n### voice_fidelity\n- **Weight:** 40%\n- **Floor:** 2.5\n- **Description:** Generated content matches the intended voice and persona\n\n#### Scoring Rubric\n| Score | Label | Definition |\n|-------|-------|------------|\n| 5 | Excellent | Indistinguishable from human-written content in target voice |\n| 4 | Good | Clearly in target voice with minor inconsistencies |\n| 3 | Acceptable | Generally correct voice but occasionally generic |\n| 2 | Poor | Voice is inconsistent or often wrong |\n| 1 | Failed | No resemblance to target voice |\n\n### topic_relevance\n- **Weight:** 35%\n- **Floor:** 3.0\n- **Description:** Content addresses the intended topic accurately\n\n### engagement\n- **Weight:** 25%\n- **Floor:** 2.5\n- **Description:** Content is interesting and holds attention\n```\n\n## Example: UX-QUALITY-EVALS.md\n\n```markdown\n# UX Quality Evaluations\n\nMeasures the user experience of the application.\n\n## Dimensions\n\n### responsiveness\n- **Weight:** 25%\n- **Floor:** 3.0\n- **Description:** UI responds quickly to user actions\n\n### feedback_clarity\n- **Weight:** 25%\n- **Floor:** 3.0\n- **Description:** Loading, success, and error states are clear\n\n### error_recovery\n- **Weight:** 20%\n- **Floor:** 2.5\n- **Description:** Users can recover from errors easily\n\n### state_consistency\n- **Weight:** 20%\n- **Floor:** 2.5\n- **Description:** UI always shows current state\n\n### accessibility\n- **Weight:** 10%\n- **Floor:** 3.0\n- **Description:** Keyboard nav, screen reader, contrast\n```\n\n## Parsing Rules\n\n1. Dimension names: Extract from `### name` headings under `## Dimensions`\n2. Weight: Parse percentage from `**Weight:** NN%`\n3. Floor: Parse float from `**Floor:** N.N`\n4. Description: First line after `**Description:**`\n5. Category: Derive from filename (CONTENT-QUALITY → content)\n"
    },
    {
      "name": "minimal-defaults.md",
      "path": "references/minimal-defaults.md",
      "content": "# Minimal Defaults\n\nWhen no project-specific taste evaluations are found, apply these minimal defaults.\n\n## Default Dimensions\n\n| Dimension | Category | Weight | Floor | Description |\n|-----------|----------|--------|-------|-------------|\n| usability | ux | 35% | 2.5 | Can users accomplish their goals? |\n| responsiveness | ux | 25% | 2.5 | Does UI respond quickly to actions? |\n| reliability | ux | 25% | 2.5 | Does the system work consistently? |\n| accessibility | ux | 15% | 2.5 | Keyboard nav, screen reader support, contrast |\n\n## Default Quality Gates\n\n| Gate | Threshold | Meaning |\n|------|-----------|---------|\n| Ship | ≥ 3.5 | Ready to ship |\n| Polish | 2.5 - 3.5 | Polish then ship |\n| Fix | < 2.5 | Fix before ship |\n\n## Scoring Rubric (Generic)\n\n| Score | Label | Definition |\n|-------|-------|------------|\n| 5 | Excellent | Exceeds expectations, delightful |\n| 4 | Good | Meets expectations with minor issues |\n| 3 | Acceptable | Functional but has rough edges |\n| 2 | Poor | Frustrating or confusing |\n| 1 | Failed | Broken or unusable |\n\n## When to Use Defaults\n\nDefaults apply when:\n1. No `.claude/taste-manifest.json` exists\n2. No `TASTE-EVALS.md` exists\n3. No `*-QUALITY-EVALS.md` files exist\n\n## Limitations\n\nDefaults are intentionally minimal:\n- Only cover UX basics\n- Don't include content quality\n- Don't include domain-specific criteria\n\n**Recommendation:** Always create project-specific evals for meaningful audits.\n\n## Upgrade Path\n\nTo upgrade from defaults:\n\n1. **Quick start:** Create `TASTE-EVALS.md` with custom dimensions\n2. **Organized:** Create separate `*-QUALITY-EVALS.md` files per category\n3. **Full control:** Create `.claude/taste-manifest.json` with explicit configuration\n"
    },
    {
      "name": "taste-manifest-schema.md",
      "path": "references/taste-manifest-schema.md",
      "content": "# Taste Manifest Schema\n\nThe `.claude/taste-manifest.json` file provides explicit configuration for taste evaluation.\n\n## Full Schema\n\n```json\n{\n  \"version\": \"1.0.0\",\n\n  \"eval_files\": [\n    \"CONTENT-QUALITY-EVALS.md\",\n    \"UX-QUALITY-EVALS.md\"\n  ],\n\n  \"category_weights\": {\n    \"content\": 0.6,\n    \"ux\": 0.4\n  },\n\n  \"quality_gates\": {\n    \"ship\": 4.0,\n    \"polish\": 3.0,\n    \"fix\": 2.5\n  },\n\n  \"custom_dimensions\": [\n    {\n      \"name\": \"brand_consistency\",\n      \"category\": \"brand\",\n      \"weight\": 0.3,\n      \"floor\": 3.0,\n      \"description\": \"Output matches brand voice guidelines\"\n    }\n  ]\n}\n```\n\n## Field Descriptions\n\n### version (required)\nSchema version. Currently `\"1.0.0\"`.\n\n### eval_files (optional)\nArray of eval file paths relative to project root. If omitted, discovers via convention.\n\n### category_weights (optional)\nHow to weight different categories when computing overall score. Must sum to 1.0.\n\nDefault: Equal weight across all discovered categories.\n\n### quality_gates (optional)\nThresholds for ship decisions:\n\n| Gate | Default | Meaning |\n|------|---------|---------|\n| ship | 3.5 | Score >= this: ready to ship |\n| polish | 2.5 | Score >= this: polish then ship |\n| fix | 2.5 | Score < this: fix before ship |\n\n### custom_dimensions (optional)\nAdditional dimensions not defined in eval files. Useful for project-specific criteria.\n\n## Validation Rules\n\n1. `version` must be present\n2. `category_weights` values must sum to 1.0\n3. `quality_gates.ship` >= `quality_gates.polish` >= `quality_gates.fix`\n4. `custom_dimensions[].weight` must be between 0 and 1\n5. `custom_dimensions[].floor` must be between 1 and 5\n\n## Example: Content-Heavy Project\n\n```json\n{\n  \"version\": \"1.0.0\",\n  \"category_weights\": {\n    \"content\": 0.7,\n    \"ux\": 0.3\n  },\n  \"quality_gates\": {\n    \"ship\": 4.0,\n    \"polish\": 3.5,\n    \"fix\": 3.0\n  }\n}\n```\n\n## Example: UX-Heavy Project\n\n```json\n{\n  \"version\": \"1.0.0\",\n  \"category_weights\": {\n    \"ux\": 0.8,\n    \"content\": 0.2\n  }\n}\n```\n"
    }
  ],
  "tags": [
    "audit",
    "taste",
    "quality",
    "discovery",
    "evals"
  ],
  "dependsOn": []
}