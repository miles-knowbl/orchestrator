{
  "id": "debug-assist",
  "name": "debug-assist",
  "version": "1.0.0",
  "description": "Systematic bug isolation for when you're stuck. Guides structured debugging using hypothesis generation, binary search, and systematic elimination. Tracks debugging state across turns. Specialized modes for memory issues, concurrency bugs, performance problems, and production incidents.",
  "phase": "VERIFY",
  "category": "core",
  "content": "# Debug Assist\n\nSystematic debugging guidance for isolating and fixing bugs.\n\n## When to Use\n\n- **Stuck on a bug** — \"I can't figure out why this is failing\"\n- **Intermittent failures** — \"It works sometimes but not always\"\n- **Production incident** — \"Something is broken in prod\"\n- **Performance issue** — \"This is slow but I don't know why\"\n- When you say: \"help me debug\", \"why isn't this working?\", \"I'm stuck\"\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `hypothesis-generation.md` | Systematic debugging approach |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `concurrency-debugging.md` | For race conditions, deadlocks |\n| `memory-debugging.md` | For memory leaks, corruption |\n| `performance-debugging.md` | For slowness issues |\n| `production-debugging.md` | For prod incidents |\n\n**Verification:** Document hypothesis and verification for each debugging attempt.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| `DEBUG-LOG.md` | Project root or inline | When debugging session occurs |\n| Bug fix code | `src/` | When bug is fixed |\n\n## Core Concept\n\nDebugging is **systematic elimination**, not random guessing.\n\nThe scientific method for bugs:\n1. **Observe** — What exactly is happening?\n2. **Hypothesize** — What could cause this?\n3. **Test** — Design an experiment to confirm or eliminate\n4. **Repeat** — Narrow down until root cause is found\n\n## The Debug Loop\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    DEBUG LOOP                           │\n│                                                         │\n│  1. SPECIFY THE SYMPTOM                                 │\n│     └─→ What exactly is wrong? Be precise.              │\n│                                                         │\n│  2. REPRODUCE                                           │\n│     └─→ Can you make it happen reliably?                │\n│                                                         │\n│  3. GENERATE HYPOTHESES                                 │\n│     └─→ What could cause this symptom?                  │\n│                                                         │\n│  4. PRIORITIZE                                          │\n│     └─→ Most likely? Easiest to test?                   │\n│                                                         │\n│  5. TEST ONE HYPOTHESIS                                 │\n│     └─→ Design experiment, observe result               │\n│                                                         │\n│  6. UPDATE AND REPEAT                                   │\n│     └─→ Eliminate hypothesis, refine, continue          │\n│                                                         │\n│  7. ROOT CAUSE FOUND                                    │\n│     └─→ Fix, verify, document                           │\n└─────────────────────────────────────────────────────────┘\n```\n\n## Step 1: Specify the Symptom\n\n**Be precise.** Vague symptoms lead to vague debugging.\n\n| Bad | Good |\n|-----|------|\n| \"It doesn't work\" | \"Clicking submit returns 500 error\" |\n| \"It's slow\" | \"API response takes 8 seconds instead of 200ms\" |\n| \"It crashes\" | \"Process exits with SIGKILL after 10 minutes\" |\n| \"Data is wrong\" | \"User balance shows $0 after successful payment\" |\n\n**Capture:**\n- Exact error message (copy/paste, don't paraphrase)\n- Stack trace if available\n- When it started (after which change?)\n- Who/what is affected\n- Frequency (always? sometimes? once?)\n\n→ See `references/symptom-specification.md`\n\n## Step 2: Reproduce\n\n**If you can't reproduce it, you can't fix it.**\n\n| Reproducibility | Approach |\n|-----------------|----------|\n| Always happens | Proceed to hypothesize |\n| Sometimes happens | Find the conditions that trigger it |\n| Happened once | Gather logs, make best hypothesis |\n\n**To improve reproducibility:**\n- Simplify: Remove variables until minimal reproduction\n- Isolate: Test component in isolation\n- Control: Fix all variables (data, time, environment)\n- Log: Add logging to capture state at failure\n\n**Minimal reproduction:**\n```\nStart: Full application with 100 features\nGoal: Smallest code that still shows the bug\n\nRemove half → still fails? → remove half again\n                → works? → bug is in removed half\n```\n\n→ See `references/reproduction-techniques.md`\n\n## Step 3: Generate Hypotheses\n\n**List everything that could cause this symptom.**\n\nCategories to consider:\n\n| Category | Examples |\n|----------|----------|\n| **Input** | Bad data, unexpected format, edge case |\n| **State** | Stale cache, race condition, corrupted state |\n| **Environment** | Config, permissions, dependencies |\n| **Code** | Logic error, typo, wrong assumption |\n| **External** | API change, network issue, third-party bug |\n| **Resources** | Memory, disk, connections exhausted |\n\n**Hypothesis format:**\n```\nH1: [What might be wrong]\n    Evidence for: [Why this might be it]\n    Evidence against: [Why this might not be it]\n    Test: [How to confirm or eliminate]\n```\n\n**Example:**\n```\nH1: Database connection pool exhausted\n    Evidence for: Error mentions timeout, high traffic lately\n    Evidence against: Error is immediate, not after 30s timeout\n    Test: Check connection pool metrics, try increasing pool size\n\nH2: Query is missing index, timing out\n    Evidence for: Slow query log shows this table\n    Evidence against: Query worked yesterday\n    Test: EXPLAIN ANALYZE the query\n```\n\n→ See `references/hypothesis-generation.md`\n\n## Step 4: Prioritize Hypotheses\n\n**Test the most likely or easiest to eliminate first.**\n\n| Prioritization Factor | Higher Priority |\n|----------------------|-----------------|\n| Likelihood | More likely causes first |\n| Test speed | Quick tests before slow tests |\n| Risk | Eliminate dangerous causes early |\n| Reversibility | Easy-to-undo tests first |\n\n**Quick wins:**\n- Check recent changes (git log, deploys)\n- Check obvious things (config, permissions, typos)\n- Check external dependencies (status pages, logs)\n\n**Don't skip:** Even if you're \"sure\" it's not X, quick tests to eliminate X are worth it. Assumptions are dangerous.\n\n## Step 5: Test One Hypothesis\n\n**One at a time.** Changing multiple things means you won't know what fixed it.\n\n**Experiment design:**\n```\nHypothesis: Connection pool is exhausted\nTest: Increase pool size from 10 to 50\nExpected if true: Errors stop\nExpected if false: Same errors continue\n```\n\n**Testing techniques:**\n\n| Technique | When to Use |\n|-----------|-------------|\n| **Binary search** | Narrow down in code/commits |\n| **Substitution** | Replace component with known-good |\n| **Isolation** | Test component alone |\n| **Injection** | Force specific conditions |\n| **Logging** | Observe internal state |\n| **Bisect** | Find breaking commit |\n\n→ See `references/testing-techniques.md`\n\n## Step 6: Update and Repeat\n\nAfter each test:\n\n| Result | Action |\n|--------|--------|\n| Hypothesis confirmed | You found it! Fix and verify |\n| Hypothesis eliminated | Remove from list, update others |\n| Inconclusive | Refine test, gather more data |\n| New information | Generate new hypotheses |\n\n**Track your progress:**\n```\n## Debug Log\n\n### Symptom\nAPI returns 500 on /users endpoint\n\n### Hypotheses\n- [x] H1: Database down — ELIMINATED (DB healthy)\n- [x] H2: Auth token expired — ELIMINATED (token valid)\n- [ ] H3: Query timeout — TESTING\n- [ ] H4: Memory pressure — Not yet tested\n\n### Tests Run\n1. Checked DB status → healthy, 50ms ping\n2. Validated auth token → token valid, not expired\n3. Running: EXPLAIN ANALYZE on user query\n```\n\n## Step 7: Root Cause Found\n\n**Don't stop at the fix.** Document and prevent recurrence.\n\n```markdown\n## Root Cause Analysis\n\n**Symptom:** API returns 500 on /users endpoint\n\n**Root Cause:** Missing index on users.organization_id.\nQuery did full table scan on 2M rows, timing out.\n\n**Fix:** Added index, query now 50ms.\n\n**Prevention:**\n- Added query performance monitoring\n- Alert on queries > 1 second\n- Review process for schema changes\n\n**Timeline:**\n- 14:30 - Reports of errors\n- 14:35 - Identified slow query in logs\n- 14:45 - Confirmed missing index\n- 14:50 - Index deployed, verified fix\n```\n\n## Specialized Modes\n\n### Memory Issues\n\n**Symptoms:** OOM kills, growing memory, GC pauses\n\n**Approach:**\n1. Confirm it's memory (monitor RSS over time)\n2. Identify growth pattern (steady leak vs. spike)\n3. Isolate (which operation causes growth?)\n4. Inspect (heap dump, allocation profiling)\n\n→ See `references/memory-debugging.md`\n\n### Concurrency Issues\n\n**Symptoms:** Intermittent failures, race conditions, deadlocks\n\n**Approach:**\n1. Confirm it's concurrency (does single-threaded work?)\n2. Identify shared state\n3. Add logging around critical sections\n4. Try stress testing (increase concurrency)\n5. Look for: check-then-act, shared mutable state, lock ordering\n\n→ See `references/concurrency-debugging.md`\n\n### Performance Issues\n\n**Symptoms:** Slow responses, high latency, timeouts\n\n**Approach:**\n1. Measure (where is time spent?)\n2. Profile (CPU? I/O? Network? Waiting?)\n3. Identify hotspots (what takes the most time?)\n4. Focus on the critical path\n5. Fix highest-impact issues first\n\n→ See `references/performance-debugging.md`\n\n### Production Incidents\n\n**Symptoms:** Alerts firing, users complaining\n\n**Approach:**\n1. **Mitigate first** — Restore service, then debug\n2. Gather data (logs, metrics, traces)\n3. Identify blast radius (what's affected?)\n4. Correlate with changes (what changed recently?)\n5. Fix or rollback\n6. Post-mortem later\n\n→ See `references/production-debugging.md`\n\n## Debug State Tracking\n\nWhen debugging across multiple turns, maintain state:\n\n```markdown\n## Debug Session: [Brief Description]\n\n### Symptom\n[Precise description of the bug]\n\n### Environment\n[Relevant context: versions, config, etc.]\n\n### Hypotheses\n| # | Hypothesis | Status | Evidence |\n|---|------------|--------|----------|\n| 1 | [Theory] | Eliminated/Testing/Confirmed | [Notes] |\n| 2 | [Theory] | Not tested | |\n\n### Tests Run\n| # | Test | Result | Conclusion |\n|---|------|--------|------------|\n| 1 | [What you did] | [What happened] | [What this means] |\n\n### Current Focus\n[What we're investigating now]\n\n### Next Steps\n1. [Next action]\n```\n\n## Anti-Patterns\n\n### Shotgun Debugging\n\n**Symptom:** Changing random things hoping something works.\n\n**Problem:** You won't know what fixed it, and you might introduce new bugs.\n\n**Fix:** One change at a time, test between each.\n\n### Assumption Blindness\n\n**Symptom:** \"It can't be X\" without actually testing X.\n\n**Problem:** X is frequently the problem.\n\n**Fix:** Test your assumptions, even \"obvious\" ones.\n\n### Tunnel Vision\n\n**Symptom:** Convinced it's a certain cause, ignoring evidence.\n\n**Problem:** Wastes time on wrong path.\n\n**Fix:** Let evidence guide you. Write down hypotheses, update based on tests.\n\n### Insufficient Logging\n\n**Symptom:** \"I don't know what's happening inside.\"\n\n**Problem:** Can't debug what you can't observe.\n\n**Fix:** Add logging, then reproduce.\n\n### Debugging in Production\n\n**Symptom:** Making changes to prod to debug.\n\n**Problem:** High risk, pressure, incomplete data.\n\n**Fix:** Reproduce locally first. If impossible, add observability, don't experiment.\n\n## Relationship to Other Skills\n\n| Skill | Relationship |\n|-------|--------------|\n| `code-verification` | Verification catches bugs before they need debugging |\n| `code-validation` | Validation may surface issues that need debugging |\n| `code-review` | Review may identify potential bug sources |\n| `implement` | Return to implement to fix identified bugs |\n| `test-generation` | Create regression tests after fixing |\n\n## Quick Reference\n\n**When stuck:**\n1. Be precise about the symptom\n2. Reproduce reliably (or understand why you can't)\n3. List all possible causes\n4. Test one at a time, most likely first\n5. Track what you've tried\n6. Ask for help after 30 minutes of no progress\n\n**Golden rules:**\n- Change one thing at a time\n- Don't trust assumptions—verify them\n- If you can't reproduce, add logging\n- Recent changes are prime suspects\n- Take breaks—fresh eyes find bugs\n\n## Mode-Specific Behavior\n\nDebugging approach and constraints differ by orchestrator mode:\n\n### Greenfield Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Full system - any code in the new system |\n| **Approach** | Comprehensive hypothesis exploration |\n| **Patterns** | Free choice of debugging techniques |\n| **Deliverables** | Full debug log + fix + regression tests |\n| **Validation** | Standard verification after fix |\n| **Constraints** | Minimal - refactor freely if needed |\n\n### Brownfield-Polish Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Gap-related code primarily |\n| **Approach** | Extend debugging with existing system awareness |\n| **Patterns** | Should match existing logging/testing patterns |\n| **Deliverables** | Delta debug log + fix matching existing style |\n| **Validation** | Existing tests + new regression tests |\n| **Constraints** | Don't break existing functionality |\n\n**Polish considerations:**\n- Bug may be in existing code, not new code\n- Existing workarounds may mask root cause\n- Fix must not break existing functionality\n- Regression testing against baseline required\n\n### Brownfield-Enterprise Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Changed code path only |\n| **Approach** | Surgical fix - minimal modification |\n| **Patterns** | Must conform exactly to existing patterns |\n| **Deliverables** | Change record with audit trail |\n| **Validation** | Full regression + change-specific testing |\n| **Constraints** | Requires approval - escalate if bug in existing code |\n\n**Enterprise debugging constraints:**\n- Do not modify code outside the change scope\n- If bug is in existing code, escalate to team\n- Rollback is preferred over complex fix\n- All debugging steps must be documented for audit\n\n---\n\n## References\n\n- `references/symptom-specification.md`: How to describe bugs precisely\n- `references/reproduction-techniques.md`: Getting bugs to happen reliably\n- `references/hypothesis-generation.md`: Thinking of all possible causes\n- `references/testing-techniques.md`: Binary search, isolation, bisect\n- `references/memory-debugging.md`: Leaks, OOM, GC issues\n- `references/concurrency-debugging.md`: Races, deadlocks, ordering\n- `references/performance-debugging.md`: Profiling and optimization\n- `references/production-debugging.md`: Incident response",
  "references": [
    {
      "name": "concurrency-debugging.md",
      "path": "references/concurrency-debugging.md",
      "content": "# Concurrency Debugging\n\nDebugging race conditions, deadlocks, and ordering issues.\n\n## Why Concurrency Bugs Are Hard\n\n1. **Non-deterministic:** May not happen every time\n2. **Timing-dependent:** Change timing, change behavior\n3. **Heisenbug effect:** Adding logging can make bug disappear\n4. **Hard to reproduce:** Work on your machine, fail in production\n\n## Symptoms\n\n| Symptom | Likely Cause |\n|---------|--------------|\n| Intermittent failures | Race condition |\n| Works in debugger, fails normally | Timing-sensitive race |\n| Sometimes wrong data | Read-write race |\n| Hangs forever | Deadlock |\n| Slow under load | Lock contention |\n| Different results each run | Order-dependent race |\n\n## Quick Diagnosis\n\n### Step 1: Confirm It's Concurrency\n\n**Test: Does it work single-threaded?**\n\n```javascript\n// Force sequential execution\nfor (const item of items) {\n  await processItem(item); // One at a time\n}\n// vs\nawait Promise.all(items.map(processItem)); // Concurrent\n```\n\nIf single-threaded works but concurrent fails → concurrency bug.\n\n### Step 2: Identify Shared State\n\nList everything that could be shared:\n- Global variables\n- Module-level variables\n- Database records\n- Cache entries\n- Files\n- External services\n\n### Step 3: Find the Race Window\n\nWhere could two operations interleave?\n\n```javascript\n// Race window between read and write\nconst balance = await getBalance(userId);    // T1 reads 100\n// Another request could read here            // T2 reads 100\nif (balance >= amount) {\n  await setBalance(userId, balance - amount); // T1 writes 50\n}                                              // T2 writes 50 (should be 0!)\n```\n\n## Race Condition Patterns\n\n### Pattern 1: Check-Then-Act\n\n```javascript\n// RACE: Check and act are not atomic\nif (await userExists(email)) {\n  throw new Error('Email taken');\n}\nawait createUser(email); // Another request could create between check and create\n```\n\n**Fix:** Use atomic operations or locks.\n\n```javascript\n// FIX: Atomic insert with unique constraint\ntry {\n  await createUser(email); // DB enforces uniqueness\n} catch (e) {\n  if (e.code === 'UNIQUE_VIOLATION') {\n    throw new Error('Email taken');\n  }\n  throw e;\n}\n```\n\n### Pattern 2: Read-Modify-Write\n\n```javascript\n// RACE: Three separate operations\nconst count = await getCount();   // Read\nconst newCount = count + 1;        // Modify (in memory)\nawait setCount(newCount);          // Write\n```\n\n**Fix:** Atomic increment.\n\n```javascript\n// FIX: Single atomic operation\nawait db.query('UPDATE counters SET count = count + 1 WHERE id = $1', [id]);\n```\n\n### Pattern 3: Lost Update\n\n```javascript\n// RACE: Both read same version, one update lost\n// Request A: read order, modify, save\n// Request B: read order, modify, save\n// B's save overwrites A's changes\n```\n\n**Fix:** Optimistic locking.\n\n```javascript\n// FIX: Include version in update condition\nconst result = await db.query(\n  'UPDATE orders SET data = $1, version = $2 WHERE id = $3 AND version = $4',\n  [newData, version + 1, orderId, version]\n);\nif (result.rowCount === 0) {\n  throw new ConcurrencyError('Order was modified by another request');\n}\n```\n\n### Pattern 4: Initialization Race\n\n```javascript\n// RACE: Two calls might both initialize\nasync function getClient() {\n  if (!this.client) {\n    this.client = await createClient(); // Both might enter this block\n  }\n  return this.client;\n}\n```\n\n**Fix:** Store the promise.\n\n```javascript\n// FIX: Store promise, not result\nfunction getClient() {\n  if (!this.clientPromise) {\n    this.clientPromise = createClient();\n  }\n  return this.clientPromise; // All callers await same promise\n}\n```\n\n## Debugging Techniques\n\n### 1. Add Strategic Logging\n\nLog entry/exit of critical sections with timestamps:\n\n```javascript\nconsole.log(`[${Date.now()}] [${requestId}] Entering critical section`);\n// ... critical section ...\nconsole.log(`[${Date.now()}] [${requestId}] Exiting critical section`);\n```\n\nLook for interleaving in logs.\n\n### 2. Add Artificial Delays\n\nMake race windows more likely to trigger:\n\n```javascript\nconst balance = await getBalance(userId);\nawait sleep(100); // Widen the race window\nif (balance >= amount) {\n  await setBalance(userId, balance - amount);\n}\n```\n\nIf it fails more often → confirms race condition.\n\n### 3. Stress Testing\n\nIncrease concurrency to trigger races:\n\n```javascript\n// Run many concurrent operations\nawait Promise.all(\n  Array(100).fill().map(() => transferMoney(from, to, 1))\n);\n// Check: is the final balance correct?\n```\n\n### 4. Deterministic Testing\n\nControl timing in tests:\n\n```javascript\n// Force specific interleaving\nconst step1 = processA.start();\nawait processA.waitForCheckpoint('after-read');\nconst step2 = processB.start();\nawait processB.waitForCheckpoint('after-write');\nawait processA.continue();\n// Assert the race condition occurred\n```\n\n## Deadlock Debugging\n\n### Symptoms\n\n- Process hangs indefinitely\n- No CPU usage (waiting, not spinning)\n- Multiple processes all blocked\n\n### Finding Deadlocks\n\n**Look for circular wait:**\n\n```\nProcess A holds Lock 1, waiting for Lock 2\nProcess B holds Lock 2, waiting for Lock 1\n→ Neither can proceed\n```\n\n**In databases:**\n\n```sql\n-- PostgreSQL: Find blocking queries\nSELECT\n  blocked.pid AS blocked_pid,\n  blocking.pid AS blocking_pid,\n  blocked.query AS blocked_query,\n  blocking.query AS blocking_query\nFROM pg_stat_activity blocked\nJOIN pg_locks blocked_locks ON blocked.pid = blocked_locks.pid\nJOIN pg_locks blocking_locks ON blocked_locks.locktype = blocking_locks.locktype\n  AND blocked_locks.relation = blocking_locks.relation\nJOIN pg_stat_activity blocking ON blocking_locks.pid = blocking.pid\nWHERE blocked_locks.granted = false AND blocking_locks.granted = true;\n```\n\n### Preventing Deadlocks\n\n1. **Lock ordering:** Always acquire locks in the same order\n2. **Lock timeouts:** Don't wait forever\n3. **Fewer locks:** Reduce lock scope\n4. **Avoid holding locks during I/O**\n\n```javascript\n// BAD: Lock order depends on input\nasync function transfer(from, to, amount) {\n  await lock(from);\n  await lock(to); // Could deadlock if another transfer goes to→from\n}\n\n// GOOD: Consistent lock order\nasync function transfer(from, to, amount) {\n  const [first, second] = from < to ? [from, to] : [to, from];\n  await lock(first);\n  await lock(second);\n}\n```\n\n## Concurrency Debugging Checklist\n\n```markdown\n### Concurrency Investigation\n\n**Symptom:** [Intermittent failure / hang / wrong data]\n\n**Reproduction:**\n- [ ] Confirmed single-threaded works\n- [ ] Increased concurrency triggers more failures\n- [ ] Added delays widen race window\n\n**Shared State Identified:**\n- [ ] [State 1]\n- [ ] [State 2]\n\n**Race Window Located:**\n- Location: [File:line]\n- Operations: [What interleaves]\n\n**Pattern:**\n- [ ] Check-then-act\n- [ ] Read-modify-write\n- [ ] Lost update\n- [ ] Initialization race\n- [ ] Deadlock\n\n**Fix Strategy:**\n- [ ] Atomic operation\n- [ ] Locking\n- [ ] Optimistic concurrency\n- [ ] Lock ordering\n- [ ] Remove shared state\n```\n\n## Tools\n\n| Tool | Use For |\n|------|---------|\n| Thread sanitizer (TSan) | Detect races in C/C++ |\n| Go race detector | `go run -race` |\n| Database lock monitoring | Find blocking queries |\n| Load testing tools | Stress test concurrency |\n| Logging with request IDs | Trace interleaved operations |\n"
    },
    {
      "name": "hypothesis-generation.md",
      "path": "references/hypothesis-generation.md",
      "content": "# Hypothesis Generation\n\nHow to think of all possible causes.\n\n## Why This Matters\n\nThe root cause will be one of your hypotheses. If you don't think of it, you won't test it. Systematic hypothesis generation ensures you don't miss obvious (or non-obvious) possibilities.\n\n## Hypothesis Categories\n\nWork through each category to generate hypotheses:\n\n### 1. Recent Changes\n\n**\"What changed?\"** is the most powerful debugging question.\n\n| Change Type | Questions to Ask |\n|-------------|------------------|\n| Code | What commits since it last worked? |\n| Deploy | What was deployed recently? |\n| Config | Any environment variable changes? |\n| Data | New records, migrations, imports? |\n| Dependencies | Any updates, new packages? |\n| Infrastructure | Scaling, DNS, certificates? |\n| External | Third-party API changes, deprecations? |\n\n**Technique:** `git log --since=\"2 days ago\"` or check deploy history.\n\n### 2. Input Problems\n\n**\"Is the input what we expect?\"**\n\n| Hypothesis Type | Examples |\n|-----------------|----------|\n| Missing data | Required field is null/undefined |\n| Wrong type | String where number expected |\n| Wrong format | Date in wrong timezone, encoding issue |\n| Edge case | Empty array, very long string, special characters |\n| Invalid state | Enum value that shouldn't exist |\n| Stale data | Cached data doesn't match current state |\n\n### 3. State Problems\n\n**\"Is the system in the state we expect?\"**\n\n| Hypothesis Type | Examples |\n|-----------------|----------|\n| Stale cache | Cached value doesn't reflect reality |\n| Race condition | Two processes interleaved unexpectedly |\n| Corrupted state | Invalid data in database/memory |\n| Missing state | Expected record doesn't exist |\n| Leaked state | State from previous operation persists |\n| Order dependency | Operations ran in wrong order |\n\n### 4. Environment Problems\n\n**\"Is the environment configured correctly?\"**\n\n| Hypothesis Type | Examples |\n|-----------------|----------|\n| Missing config | Environment variable not set |\n| Wrong config | Pointing to wrong database/service |\n| Permissions | File/network permissions denied |\n| Resources | Disk full, memory exhausted, ports in use |\n| Network | DNS failure, firewall, timeout |\n| Version mismatch | Incompatible dependency versions |\n\n### 5. Code Problems\n\n**\"Is the code logic correct?\"**\n\n| Hypothesis Type | Examples |\n|-----------------|----------|\n| Logic error | Wrong operator, inverted condition |\n| Off-by-one | Loop bounds, array indices |\n| Null handling | Accessing property of null/undefined |\n| Error handling | Exception not caught, swallowed |\n| Type coercion | Implicit conversion causes bug |\n| Concurrency | Race condition, deadlock |\n\n### 6. External Problems\n\n**\"Is something outside our control broken?\"**\n\n| Hypothesis Type | Examples |\n|-----------------|----------|\n| API change | Third-party changed their API |\n| Service down | External dependency is unavailable |\n| Rate limited | Exceeded API limits |\n| Network issues | Latency, packet loss, DNS |\n| Certificate | Expired SSL cert |\n| Time-related | Timezone, DST, expiration |\n\n## Hypothesis Generation Techniques\n\n### The Five Whys\n\nKeep asking \"why\" to get to root cause:\n\n```\nBug: User sees error page\nWhy? → Server returned 500\nWhy? → Database query threw exception\nWhy? → Query timed out\nWhy? → Query did full table scan\nWhy? → Missing index on frequently-queried column\n```\n\n### Fault Tree Analysis\n\nWork backward from symptom:\n\n```\n                    [Error on checkout]\n                           │\n         ┌─────────────────┼─────────────────┐\n         │                 │                 │\n    [API error]      [JS error]       [Network error]\n         │                 │                 │\n    ┌────┴────┐       ┌────┴────┐      [timeout?]\n    │         │       │         │      [DNS?]\n[auth fail] [DB fail] [null ref] [type error]\n```\n\n### Similar Past Bugs\n\nQuestions:\n- Have we seen this symptom before?\n- What caused it last time?\n- Are there related bug reports in history?\n- Did we fix something similar that might have regressed?\n\n### Component Isolation\n\nList all components involved and hypothesize failures for each:\n\n```\nRequest flow: Browser → CDN → Load Balancer → App Server → Cache → Database\n\nHypotheses:\n- Browser: JavaScript error, cookie issue\n- CDN: Cached stale response, routing error\n- Load Balancer: Health check failing, wrong backend\n- App Server: Code bug, memory exhaustion\n- Cache: Stale data, connection failure\n- Database: Query error, connection pool exhausted\n```\n\n## Hypothesis Format\n\nFor each hypothesis, document:\n\n```markdown\n### H[N]: [Brief description]\n\n**What it would explain:** [How this hypothesis explains the symptom]\n\n**Evidence for:**\n- [Observation that supports this]\n- [Another supporting observation]\n\n**Evidence against:**\n- [Observation that doesn't fit]\n\n**Test:**\n[Specific action to confirm or eliminate]\n\n**Status:** Not tested / Testing / Eliminated / Confirmed\n```\n\n### Example Hypotheses\n\n```markdown\n### H1: Database connection pool exhausted\n\n**What it would explain:** Requests timeout waiting for connection\n\n**Evidence for:**\n- Error mentions \"connection timeout\"\n- High traffic period\n- Other endpoints also affected\n\n**Evidence against:**\n- Error is immediate (50ms), not after 30s pool timeout\n- Connection pool metrics show available connections\n\n**Test:**\n1. Check connection pool metrics (pg_stat_activity)\n2. Temporarily increase pool size\n3. Add logging around connection acquisition\n\n**Status:** Testing\n\n---\n\n### H2: Query missing index, timing out\n\n**What it would explain:** Query takes too long, times out\n\n**Evidence for:**\n- Slow query log shows this table\n- Table has grown recently\n\n**Evidence against:**\n- Same query worked yesterday\n- Index exists for common query pattern\n\n**Test:**\n1. Run EXPLAIN ANALYZE on the query\n2. Check if query plan changed\n3. Check table statistics freshness\n\n**Status:** Not tested\n```\n\n## Prioritizing Hypotheses\n\nAfter generating hypotheses, prioritize:\n\n| Factor | Higher Priority |\n|--------|-----------------|\n| **Likelihood** | Matches evidence, fits symptom |\n| **Test speed** | Quick to confirm or eliminate |\n| **Recent change** | Something changed around failure time |\n| **Past occurrence** | This type of bug has happened before |\n| **Risk** | Could cause data loss or security issue |\n\n### Quick Wins First\n\nAlways check these first (quick to eliminate):\n- [ ] Recent deployments/changes\n- [ ] Service status pages (external dependencies)\n- [ ] Resource metrics (CPU, memory, disk, connections)\n- [ ] Log for obvious errors\n- [ ] Config/environment variables\n\n## When You're Stuck\n\nIf no hypotheses seem right:\n\n1. **Widen the scope** — Are you looking in the right place?\n2. **Question assumptions** — What are you \"sure\" isn't the problem?\n3. **Add more logging** — You might be missing visibility\n4. **Take a break** — Fresh eyes often see what tired eyes miss\n5. **Explain to someone** — Rubber duck debugging\n6. **Bisect** — Find the exact commit that introduced the bug\n\n## Output Format\n\nWhen presenting hypotheses:\n\n```markdown\n## Hypotheses for: [Bug summary]\n\n### Most Likely\n\n| # | Hypothesis | Supporting Evidence | Test |\n|---|------------|---------------------|------|\n| 1 | Connection pool exhausted | Timeout error, high traffic | Check pool metrics |\n| 2 | Missing database index | Slow query log, recent data growth | EXPLAIN ANALYZE |\n\n### Possible\n\n| # | Hypothesis | Supporting Evidence | Test |\n|---|------------|---------------------|------|\n| 3 | Memory pressure causing GC pauses | Response times variable | Check memory metrics |\n| 4 | External API rate limited | Uses third-party API | Check API response headers |\n\n### Already Eliminated\n\n| # | Hypothesis | How Eliminated |\n|---|------------|----------------|\n| 5 | Database down | Confirmed DB healthy, other queries work |\n| 6 | Auth token expired | Validated token, not expired |\n\n### Next Steps\n1. Check connection pool metrics\n2. If pool healthy, run EXPLAIN ANALYZE on suspect query\n```\n"
    },
    {
      "name": "memory-debugging.md",
      "path": "references/memory-debugging.md",
      "content": "# Memory Debugging\n\nDebugging memory leaks, OOM, and GC issues.\n\n## Symptoms\n\n| Symptom | Possible Cause |\n|---------|----------------|\n| OOM kill (code 137) | Memory leak or unbounded growth |\n| Gradual memory growth | Slow leak |\n| Sudden memory spike | Large allocation |\n| High GC pause times | Too many objects, memory pressure |\n| Degrading performance over time | Memory pressure causing GC thrashing |\n\n## Quick Diagnosis\n\n### Step 1: Confirm It's Memory\n\n```bash\n# Watch memory over time\nwatch -n 1 'ps -o pid,rss,command -p $(pgrep node)'\n\n# Check for OOM kills\ndmesg | grep -i \"out of memory\"\ngrep -i \"oom\" /var/log/syslog\n```\n\n### Step 2: Identify Growth Pattern\n\n| Pattern | Likely Cause |\n|---------|--------------|\n| Steady climb | Classic leak (objects not released) |\n| Sawtooth | Normal—GC working but baseline increasing |\n| Sudden spike | Large allocation, bulk operation |\n| Plateau then spike | Slow leak until threshold |\n\n### Step 3: Correlate with Operations\n\n- What operations are running when memory grows?\n- Does memory drop after certain operations complete?\n- Is growth proportional to traffic/data?\n\n## Node.js Memory Debugging\n\n### Heap Snapshot\n\n```javascript\n// In code: trigger heap dump\nconst v8 = require('v8');\nconst fs = require('fs');\n\nfunction takeHeapSnapshot() {\n  const snapshotFile = `/tmp/heap-${Date.now()}.heapsnapshot`;\n  const stream = fs.createWriteStream(snapshotFile);\n  v8.writeHeapSnapshot(snapshotFile);\n  console.log(`Heap snapshot written to ${snapshotFile}`);\n}\n\n// Trigger via signal\nprocess.on('SIGUSR2', takeHeapSnapshot);\n```\n\n```bash\n# Trigger snapshot\nkill -USR2 $(pgrep node)\n```\n\n**Analyze in Chrome DevTools:**\n1. Open DevTools → Memory tab\n2. Load snapshot file\n3. Look for: Retained size, object counts, detached DOM\n\n### Comparing Snapshots\n\n1. Take snapshot when memory is normal\n2. Run suspected leaky operation\n3. Take another snapshot\n4. Compare: what objects increased?\n\n### Memory Timeline\n\n```javascript\n// Log memory usage periodically\nsetInterval(() => {\n  const usage = process.memoryUsage();\n  console.log({\n    heapUsed: Math.round(usage.heapUsed / 1024 / 1024) + 'MB',\n    heapTotal: Math.round(usage.heapTotal / 1024 / 1024) + 'MB',\n    rss: Math.round(usage.rss / 1024 / 1024) + 'MB',\n    external: Math.round(usage.external / 1024 / 1024) + 'MB'\n  });\n}, 5000);\n```\n\n### Garbage Collection Logging\n\n```bash\n# Run Node with GC logging\nnode --trace-gc app.js\n\n# More detailed\nnode --trace-gc --trace-gc-verbose app.js\n```\n\n## Common Memory Leak Patterns\n\n### 1. Event Listeners Not Removed\n\n```javascript\n// LEAK: Listener never removed\nclass Component {\n  start() {\n    window.addEventListener('resize', this.handleResize);\n  }\n  // No cleanup method!\n}\n\n// FIX: Remove listener\nclass Component {\n  start() {\n    window.addEventListener('resize', this.handleResize);\n  }\n  stop() {\n    window.removeEventListener('resize', this.handleResize);\n  }\n}\n```\n\n### 2. Closures Holding References\n\n```javascript\n// LEAK: Closure captures large object\nfunction createHandler(hugeData) {\n  return () => {\n    console.log(hugeData.items[0]); // Holds reference to all of hugeData\n  };\n}\n\n// FIX: Capture only what's needed\nfunction createHandler(hugeData) {\n  const firstItem = hugeData.items[0];\n  return () => {\n    console.log(firstItem); // Only holds reference to one item\n  };\n}\n```\n\n### 3. Unbounded Caches/Collections\n\n```javascript\n// LEAK: Cache grows forever\nconst cache = {};\nfunction getData(key) {\n  if (!cache[key]) {\n    cache[key] = expensiveComputation(key);\n  }\n  return cache[key];\n}\n\n// FIX: Bounded cache with LRU eviction\nconst LRU = require('lru-cache');\nconst cache = new LRU({ max: 500 });\n```\n\n### 4. Timers Not Cleared\n\n```javascript\n// LEAK: Interval runs forever\nfunction startPolling() {\n  setInterval(poll, 1000);\n}\n\n// FIX: Store and clear interval\nlet pollInterval;\nfunction startPolling() {\n  pollInterval = setInterval(poll, 1000);\n}\nfunction stopPolling() {\n  clearInterval(pollInterval);\n}\n```\n\n### 5. Subscriptions Not Unsubscribed\n\n```javascript\n// LEAK: Subscription never cancelled\nuseEffect(() => {\n  const sub = eventBus.subscribe('event', handler);\n  // No cleanup!\n}, []);\n\n// FIX: Unsubscribe on cleanup\nuseEffect(() => {\n  const sub = eventBus.subscribe('event', handler);\n  return () => sub.unsubscribe();\n}, []);\n```\n\n## Memory Debugging Checklist\n\n```markdown\n### Memory Investigation\n\n**Symptom:** [What you're seeing]\n\n**Measurements:**\n- Starting memory: [X MB]\n- After 1 hour: [Y MB]\n- Growth rate: [Z MB/hour]\n\n**Correlation:**\n- [ ] Checked correlation with traffic\n- [ ] Checked correlation with specific operations\n- [ ] Checked for scheduled tasks\n\n**Analysis:**\n- [ ] Took heap snapshots\n- [ ] Compared snapshots\n- [ ] Identified growing object types\n\n**Common Causes Checked:**\n- [ ] Event listeners\n- [ ] Closures\n- [ ] Caches without limits\n- [ ] Timers not cleared\n- [ ] Subscriptions not cancelled\n- [ ] Global variables accumulating\n\n**Root Cause:** [What's leaking]\n\n**Fix:** [How to fix it]\n```\n\n## Tools\n\n| Tool | Use For |\n|------|---------|\n| Chrome DevTools | Heap snapshots, memory timeline |\n| `node --inspect` | Attach Chrome DevTools to Node |\n| `heapdump` package | Programmatic heap dumps |\n| `clinic` | Memory profiling suite |\n| `memwatch-next` | Leak detection for Node |\n| `valgrind` | Native memory analysis |\n"
    },
    {
      "name": "performance-debugging.md",
      "path": "references/performance-debugging.md",
      "content": "# Performance Debugging\n\nFinding and fixing slow code.\n\n## Symptoms\n\n| Symptom | Likely Area |\n|---------|-------------|\n| High latency | Slow operation in critical path |\n| Timeout errors | Operation exceeds time limit |\n| High CPU | CPU-bound computation or busy loop |\n| High memory | Memory allocation or leak |\n| High I/O wait | Database, disk, or network bound |\n| Works fast then slows | Resource exhaustion, GC pressure |\n\n## The Performance Debugging Process\n\n### Step 1: Measure, Don't Guess\n\n**Never optimize without data.** Intuition about performance is often wrong.\n\n```javascript\n// Add timing\nconst start = Date.now();\nawait expensiveOperation();\nconsole.log(`Operation took ${Date.now() - start}ms`);\n```\n\n### Step 2: Find the Bottleneck\n\nWhere is time actually spent?\n\n| Resource | How to Check |\n|----------|--------------|\n| CPU | `top`, `htop`, CPU profiler |\n| Memory | Memory profiler, heap snapshots |\n| Database | Slow query log, EXPLAIN ANALYZE |\n| Network | Network tab, latency metrics |\n| Disk | `iostat`, disk I/O metrics |\n\n### Step 3: Focus on the Critical Path\n\nThe critical path is the sequence of operations that determines total time.\n\n```\nRequest time = auth(50ms) + getData(500ms) + transform(10ms) + respond(5ms)\n                                  ↑\n                           Optimize THIS\n```\n\n### Step 4: Fix Highest Impact First\n\nAmdahl's Law: Optimizing something that takes 1% of time won't help much.\n\n| Time Spent | Max Improvement if Fixed |\n|------------|--------------------------|\n| 90% | 10x faster |\n| 50% | 2x faster |\n| 10% | 1.1x faster |\n| 1% | 1.01x faster |\n\n## Database Performance\n\n### Finding Slow Queries\n\n```sql\n-- PostgreSQL: Enable slow query logging\nALTER SYSTEM SET log_min_duration_statement = '1000'; -- Log queries > 1 second\nSELECT pg_reload_conf();\n\n-- Find currently running slow queries\nSELECT pid, now() - query_start AS duration, query\nFROM pg_stat_activity\nWHERE state = 'active' AND now() - query_start > interval '5 seconds';\n```\n\n### EXPLAIN ANALYZE\n\n```sql\nEXPLAIN ANALYZE SELECT * FROM orders WHERE user_id = 123;\n```\n\n**Red flags:**\n- `Seq Scan` on large table (needs index)\n- High `actual rows` vs `planned rows` (bad statistics)\n- `Nested Loop` with high row counts (O(n²))\n\n### Common Database Fixes\n\n| Problem | Fix |\n|---------|-----|\n| Seq Scan | Add index |\n| N+1 queries | Use JOIN or eager loading |\n| Large result set | Add pagination |\n| Lock contention | Smaller transactions |\n| Connection exhaustion | Connection pooling |\n\n## API/Network Performance\n\n### Finding Network Issues\n\n```javascript\n// Log external call timing\nconst start = Date.now();\nconst response = await fetch(url);\nconsole.log(`External API: ${Date.now() - start}ms`);\n```\n\n### Common Network Fixes\n\n| Problem | Fix |\n|---------|-----|\n| Sequential calls | Promise.all for parallel |\n| No timeout | Add timeout |\n| No caching | Cache responses |\n| Large payloads | Compression, pagination |\n| SSL handshake | Connection reuse |\n\n## Code Performance\n\n### Profiling\n\n**Node.js:**\n```bash\n# CPU profiling\nnode --prof app.js\nnode --prof-process isolate-*.log > profile.txt\n\n# Or use Chrome DevTools\nnode --inspect app.js\n```\n\n### Common Code Fixes\n\n| Problem | Fix |\n|---------|-----|\n| O(n²) loops | Use Map for O(1) lookup |\n| Repeated computation | Caching, memoization |\n| Blocking I/O | Async I/O |\n| Large JSON parsing | Streaming parser |\n| Regex backtracking | Simpler regex, anchoring |\n\n## Quick Wins Checklist\n\n```markdown\n### Performance Quick Wins\n\n**Database:**\n- [ ] Check for missing indexes (EXPLAIN ANALYZE)\n- [ ] Check for N+1 queries\n- [ ] Check connection pool size\n- [ ] Add pagination to large queries\n\n**Network:**\n- [ ] Parallelize independent calls (Promise.all)\n- [ ] Add timeouts to external calls\n- [ ] Enable compression\n- [ ] Add caching headers\n\n**Code:**\n- [ ] Check for O(n²) patterns\n- [ ] Cache expensive computations\n- [ ] Use async for I/O operations\n- [ ] Check for memory leaks (GC pressure)\n\n**Infrastructure:**\n- [ ] Check resource limits (CPU, memory)\n- [ ] Check for resource contention\n- [ ] Review scaling configuration\n```\n\n## Performance Investigation Template\n\n```markdown\n## Performance Issue: [Description]\n\n**Symptom:** [What's slow, how slow]\n\n**Baseline:** [What's expected]\n\n**Measurements:**\n| Operation | Time | % of Total |\n|-----------|------|------------|\n| [Op 1] | [X ms] | [Y%] |\n| [Op 2] | [X ms] | [Y%] |\n\n**Bottleneck:** [What's taking the most time]\n\n**Root Cause:** [Why it's slow]\n\n**Fix:** [What to change]\n\n**Expected Improvement:** [How much faster]\n\n**Actual Result:** [After fix]\n```\n\n## Tools\n\n| Tool | Use For |\n|------|---------|\n| Chrome DevTools | JS profiling, network analysis |\n| `ab`, `wrk`, `k6` | Load testing |\n| `EXPLAIN ANALYZE` | Database query analysis |\n| `perf` (Linux) | System-level profiling |\n| APM tools | End-to-end tracing |\n"
    },
    {
      "name": "production-debugging.md",
      "path": "references/production-debugging.md",
      "content": "# Production Debugging\n\nDebugging live systems safely and effectively.\n\n## Why This Is Different\n\nProduction debugging has constraints that local debugging doesn't:\n- **Real users affected** — Every minute matters\n- **Limited access** — Can't attach debugger, add arbitrary logging\n- **Risk of making it worse** — Bad changes compound the problem\n- **Incomplete information** — Limited to what's already logged\n- **Pressure** — People are watching, waiting\n\n## The Production Debugging Mindset\n\n### Rule 1: Mitigate First, Debug Later\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    INCIDENT FLOW                        │\n│                                                         │\n│   Alert fires                                           │\n│       ↓                                                 │\n│   MITIGATE (restore service)                            │\n│       ↓                                                 │\n│   Gather data (while it's fresh)                        │\n│       ↓                                                 │\n│   Diagnose (find root cause)                            │\n│       ↓                                                 │\n│   Fix properly                                          │\n│       ↓                                                 │\n│   Post-mortem (prevent recurrence)                      │\n└─────────────────────────────────────────────────────────┘\n```\n\n**Mitigation options (fastest first):**\n- Rollback deployment\n- Restart service\n- Scale up resources\n- Enable feature flag fallback\n- Redirect traffic\n- Fail to cached/static content\n\n**Don't diagnose while users are down.** Restore service first, understand why later.\n\n### Rule 2: Don't Make It Worse\n\nBefore any action in production:\n- What could go wrong?\n- Is this reversible?\n- What's the blast radius?\n- Do I have approval?\n\n**Safe actions:**\n- Reading logs\n- Reading metrics\n- Non-mutating queries\n\n**Risky actions (require approval):**\n- Config changes\n- Restarts\n- Code changes\n- Data modifications\n\n### Rule 3: Document Everything\n\nDuring an incident, capture:\n- Timeline of events\n- Actions taken\n- Who did what\n- What worked, what didn't\n- Decisions and rationale\n\nYou won't remember later, and others need to follow along.\n\n## Production Information Sources\n\n### Logs\n\n**What to look for:**\n- Error messages around incident time\n- Stack traces\n- Request IDs (to trace specific requests)\n- Patterns (same error repeated, specific users)\n\n**Log analysis:**\n```bash\n# Find errors in time range\ngrep \"ERROR\" app.log | grep \"2024-01-15 14:\"\n\n# Count error types\ngrep \"ERROR\" app.log | cut -d':' -f4 | sort | uniq -c | sort -rn\n\n# Follow specific request\ngrep \"req-abc123\" app.log\n```\n\n### Metrics\n\n**Key metrics during incident:**\n| Metric | What It Tells You |\n|--------|-------------------|\n| Error rate | How bad is it? |\n| Latency p99 | Is it slow or broken? |\n| Request rate | Traffic spike? |\n| CPU/Memory | Resource exhaustion? |\n| Connection count | Pool exhaustion? |\n| Queue depth | Backlog building? |\n\n**Compare to baseline:** Is this metric abnormal compared to same time yesterday/last week?\n\n### Traces\n\n**Distributed traces show:**\n- Which service is slow/failing\n- Where time is spent\n- Request flow through system\n\n**Look for:**\n- Long spans (bottleneck)\n- Error spans (failure point)\n- Missing spans (service didn't respond)\n\n### External Status\n\n**Check before assuming it's you:**\n- Cloud provider status page\n- Third-party API status\n- DNS status\n- CDN status\n\n## Production-Safe Investigation\n\n### Safe Queries\n\n```sql\n-- Check active queries (non-mutating)\nSELECT pid, now() - pg_stat_activity.query_start AS duration, query\nFROM pg_stat_activity\nWHERE state = 'active';\n\n-- Check table sizes\nSELECT relname, pg_size_pretty(pg_total_relation_size(relid))\nFROM pg_catalog.pg_statio_user_tables\nORDER BY pg_total_relation_size(relid) DESC\nLIMIT 10;\n\n-- Check for locks (don't kill without approval)\nSELECT * FROM pg_locks WHERE NOT granted;\n```\n\n### Safe Commands\n\n```bash\n# Check process status\nps aux | grep myapp\n\n# Check open connections\nnetstat -an | grep ESTABLISHED | wc -l\n\n# Check disk space\ndf -h\n\n# Check memory\nfree -m\n\n# Check recent deployments (read-only)\ngit log --oneline -20\n```\n\n### Risky Commands (Require Approval)\n\n```bash\n# Restarting services\nsystemctl restart myapp\n\n# Killing processes\nkill -9 <pid>\n\n# Changing config\nvi /etc/myapp/config.yaml\n\n# Running migrations\n./manage.py migrate\n\n# Data modifications\nUPDATE users SET ...\n```\n\n## Common Production Scenarios\n\n### Scenario: Error Rate Spike\n\n**Immediate actions:**\n1. Check if recent deploy (rollback candidate)\n2. Check error messages for pattern\n3. Check if specific endpoint or all\n4. Check external dependency status\n\n**Common causes:**\n- Bad deploy\n- External API failure\n- Database issue\n- Resource exhaustion\n- Traffic spike\n\n### Scenario: Latency Spike\n\n**Immediate actions:**\n1. Check if specific endpoint or system-wide\n2. Check database query times\n3. Check external API response times\n4. Check resource metrics (CPU, memory)\n\n**Common causes:**\n- Slow query (missing index, data growth)\n- Resource contention\n- External dependency slow\n- GC pressure\n- Connection pool exhaustion\n\n### Scenario: Service Unresponsive\n\n**Immediate actions:**\n1. Check if process is running\n2. Check if it's accepting connections\n3. Check resource usage (memory, CPU, disk)\n4. Check logs for fatal errors\n\n**Common causes:**\n- OOM killed\n- Deadlock\n- Infinite loop\n- Disk full\n- Network partition\n\n### Scenario: Data Inconsistency\n\n**Immediate actions:**\n1. Stop writes if possible (prevent more damage)\n2. Identify scope (how much data affected)\n3. Find source of bad data\n4. Check if backups are clean\n\n**Common causes:**\n- Race condition\n- Migration bug\n- Application bug\n- Manual mistake\n- Partial failure\n\n## Incident Communication\n\n### Status Updates\n\nCommunicate regularly (every 15-30 min) even if no progress:\n\n```markdown\n## [Time] Incident Update\n\n**Status:** Investigating / Identified / Mitigating / Resolved\n\n**Impact:** [Who/what is affected]\n\n**Current understanding:** [What we know]\n\n**Actions:** [What we're doing]\n\n**Next update:** [Time]\n```\n\n### Escalation\n\n**Escalate when:**\n- Impact is high and you're stuck\n- You need access/approval you don't have\n- Multiple domains involved\n- Customer-facing with executive visibility\n- You've been stuck for > 30 minutes\n\n**How to escalate:**\n- Brief summary of issue\n- What you've tried\n- What you need\n- How urgent\n\n## Post-Incident\n\n### Preserve Evidence\n\nBefore things get cleaned up:\n- Save relevant logs\n- Screenshot dashboards\n- Export metrics\n- Note timeline\n\n### Post-Mortem Template\n\n```markdown\n## Incident Post-Mortem: [Title]\n\n**Date:** [Date]\n**Duration:** [Start] to [End] ([X] minutes)\n**Severity:** [P1/P2/P3]\n\n### Summary\n[One paragraph description]\n\n### Impact\n- [Number] users affected\n- [Service] unavailable for [duration]\n- [Any data impact]\n\n### Timeline\n- HH:MM - [Event]\n- HH:MM - [Event]\n- ...\n\n### Root Cause\n[What actually caused the incident]\n\n### Contributing Factors\n- [What made it worse or delayed detection]\n\n### What Went Well\n- [What worked]\n\n### What Could Be Improved\n- [What was difficult]\n\n### Action Items\n| Action | Owner | Due Date |\n|--------|-------|----------|\n| [Preventive measure] | [Name] | [Date] |\n\n### Lessons Learned\n- [Key takeaways]\n```\n\n## Quick Reference\n\n**During incident:**\n1. Mitigate (restore service)\n2. Communicate (status update)\n3. Gather data (logs, metrics, traces)\n4. Diagnose (find root cause)\n5. Fix (with approval)\n6. Verify (confirm fix worked)\n7. Document (capture for post-mortem)\n\n**Don't:**\n- Debug while service is down\n- Make changes without understanding impact\n- Forget to communicate\n- Skip documentation\n- Blame individuals\n\n**Do:**\n- Restore service first\n- Get help when stuck\n- Document everything\n- Preserve evidence\n- Conduct blameless post-mortem\n"
    },
    {
      "name": "reproduction-techniques.md",
      "path": "references/reproduction-techniques.md",
      "content": "# Reproduction Techniques\n\nHow to make bugs happen reliably.\n\n## Why This Matters\n\nYou can't fix what you can't reproduce. Intermittent bugs that \"just happen sometimes\" are impossible to debug systematically. Getting reliable reproduction is often the hardest and most important part of debugging.\n\n## Reproduction Levels\n\n| Level | Description | Debuggability |\n|-------|-------------|---------------|\n| **On-demand** | Bug happens every time you try | Best |\n| **High-frequency** | Bug happens most attempts | Good |\n| **Low-frequency** | Bug happens occasionally | Hard |\n| **Environment-specific** | Only in certain environments | Medium |\n| **One-time** | Happened once, can't recreate | Hardest |\n\n## From Unreliable to Reliable\n\n### Step 1: Gather Information\n\nFor an unreliable bug, collect:\n- When did it happen? (timestamps)\n- What was the user doing?\n- What was the system state?\n- What were the inputs?\n- What were concurrent operations?\n- What was different from times it worked?\n\n### Step 2: Identify Variables\n\nList everything that varies:\n- Input data\n- User/account\n- Time of day\n- System load\n- Concurrent operations\n- Environment (browser, OS, config)\n- Network conditions\n- Database state\n\n### Step 3: Control Variables\n\nFix variables one by one:\n```markdown\nTest 1: Original (unreliable)\n- Any user, any data, any time\n- Result: Sometimes fails\n\nTest 2: Fix the user\n- User: testuser@example.com\n- Result: Still sometimes fails\n\nTest 3: Fix the data\n- User: testuser@example.com\n- Data: Order #12345\n- Result: Always fails! ← Found it\n```\n\n### Step 4: Simplify\n\nOnce reproducible, remove unnecessary elements:\n- Remove unrelated code\n- Hardcode values that don't affect bug\n- Remove network calls with mocks\n- Remove database with fixtures\n\n**Goal:** Minimal reproduction—the smallest code/input that still fails.\n\n## Techniques by Bug Type\n\n### Timing-Dependent Bugs\n\n**Symptoms:** Works sometimes, fails other times.\n\n**Techniques:**\n- Add delays to slow down operations\n- Use debugger to pause at critical points\n- Log timestamps to see ordering\n- Increase concurrency to trigger races\n\n```javascript\n// Force timing issue to manifest\nawait sleep(100); // Add delay before critical section\n```\n\n### Data-Dependent Bugs\n\n**Symptoms:** Works for some inputs, fails for others.\n\n**Techniques:**\n- Compare working vs failing inputs\n- Use property-based testing to find edge cases\n- Test boundary values\n- Test with production data samples\n\n```javascript\n// Test with exact failing input\nconst failingInput = { items: [{ qty: 0 }] }; // Captured from production\nprocessOrder(failingInput);\n```\n\n### Environment-Dependent Bugs\n\n**Symptoms:** Works locally, fails in production (or vice versa).\n\n**Techniques:**\n- Compare environment configurations\n- Use same versions (Node, database, etc.)\n- Check environment variables\n- Use production data locally\n- Test in staging that mirrors production\n\n```bash\n# Get production environment\nheroku config --app myapp\n\n# Compare to local\ndiff <(heroku config) <(cat .env)\n```\n\n### Load-Dependent Bugs\n\n**Symptoms:** Works at low load, fails at high load.\n\n**Techniques:**\n- Use load testing tools (k6, artillery, ab)\n- Gradually increase load until failure\n- Monitor resources during load test\n- Check connection pool behavior\n\n```bash\n# Load test with increasing concurrency\nab -n 10000 -c 100 http://localhost:3000/api/endpoint\n```\n\n### Memory-Dependent Bugs\n\n**Symptoms:** Works initially, fails after running for a while.\n\n**Techniques:**\n- Monitor memory over time\n- Reduce available memory\n- Run operations in a loop\n- Take heap snapshots at intervals\n\n```javascript\n// Accelerate memory issues\nfor (let i = 0; i < 10000; i++) {\n  await processRequest(); // Run many times\n}\n```\n\n## Creating Minimal Reproductions\n\n### The Reduction Process\n\n```\nFull Application (10,000 lines)\n        ↓ Remove unrelated features\nRelevant Module (1,000 lines)\n        ↓ Remove unrelated functions\nSingle File (200 lines)\n        ↓ Remove unrelated logic\nMinimal Case (20 lines)\n```\n\n### Minimal Reproduction Template\n\n```javascript\n// Minimal reproduction for: [Bug description]\n// Environment: Node 18, PostgreSQL 14\n\nconst setup = async () => {\n  // Minimal setup required\n};\n\nconst reproduce = async () => {\n  // Exact steps that trigger the bug\n};\n\nconst expected = \"This should happen\";\nconst actual = await reproduce();\n\nconsole.log('Expected:', expected);\nconsole.log('Actual:', actual);\nconsole.log('Bug reproduced:', actual !== expected);\n```\n\n### Sharing Reproductions\n\nGood reproduction reports include:\n\n```markdown\n## Reproduction Steps\n\n### Environment\n- Node: 18.17.0\n- OS: macOS 14.0\n- Database: PostgreSQL 14.1\n\n### Setup\n1. Clone repository\n2. `npm install`\n3. Set `DATABASE_URL=...`\n4. Run `npm run seed`\n\n### Steps\n1. Start server: `npm start`\n2. Run: `curl -X POST localhost:3000/api/orders -d '{\"userId\": 1, \"items\": []}'`\n3. Observe: 500 error with \"Cannot read property 'length' of undefined\"\n\n### Expected\n400 error with \"items cannot be empty\"\n\n### Frequency\n100% reproducible with these steps\n```\n\n## When You Can't Reproduce\n\n### Strategies for One-Time Bugs\n\n1. **Log analysis:** Look for clues in logs around the failure\n2. **State reconstruction:** Try to recreate the exact state\n3. **Hypothesis testing:** Test likely causes even without reproduction\n4. **Add logging:** Prepare to catch it next time\n5. **Monitoring:** Set up alerts for the symptom\n\n### Improving Logging for Future Reproduction\n\nAdd logging that captures:\n```javascript\n// Before risky operation\nlogger.info('Starting order processing', {\n  orderId: order.id,\n  userId: user.id,\n  items: order.items.map(i => i.id),\n  timestamp: Date.now(),\n  requestId: req.id\n});\n\n// After failure\nlogger.error('Order processing failed', {\n  orderId: order.id,\n  error: error.message,\n  stack: error.stack,\n  state: JSON.stringify(currentState),\n  requestId: req.id\n});\n```\n\n### When to Give Up (Temporarily)\n\nIf you can't reproduce after significant effort:\n1. Document everything learned\n2. Add monitoring/logging to catch it next time\n3. Set a time limit (\"revisit if it happens again\")\n4. Move on, but don't forget\n\n## Output Format\n\nWhen documenting reproduction:\n\n```markdown\n## Reproduction Status\n\n**Bug:** [Description]\n\n**Reproduction Level:** On-demand / High-frequency / Low-frequency / Cannot reproduce\n\n### Reproduction Steps\n1. [Step]\n2. [Step]\n3. [Bug occurs]\n\n### Minimal Code\n```javascript\n// [Minimal reproduction code]\n```\n\n### Key Variables\n| Variable | Required Value | Notes |\n|----------|---------------|-------|\n| User | ID 12345 | Bug only occurs for this user |\n| Data | Empty array | Bug is related to empty input handling |\n\n### What Doesn't Affect Reproduction\n- Time of day (tested at multiple times)\n- Browser (happens in all browsers)\n- Server load (happens even under no load)\n\n### Confidence\n[High/Medium/Low] - [Why]\n```\n"
    },
    {
      "name": "symptom-specification.md",
      "path": "references/symptom-specification.md",
      "content": "# Symptom Specification\n\nHow to describe bugs precisely.\n\n## Why This Matters\n\nVague bug reports lead to vague debugging. \"It doesn't work\" could be a thousand different problems. Precise symptoms narrow the search space and make hypotheses easier to generate and test.\n\n## The Bug Specification Template\n\n```markdown\n## Bug: [One-line summary]\n\n### What happens\n[Precise description of the incorrect behavior]\n\n### What should happen\n[Expected behavior]\n\n### Error details\n[Exact error message, stack trace, logs - copy/paste, don't paraphrase]\n\n### Steps to reproduce\n1. [First step]\n2. [Second step]\n3. [Bug occurs]\n\n### Environment\n- Version/commit: [specific version]\n- OS: [if relevant]\n- Browser: [if relevant]\n- Configuration: [relevant settings]\n\n### Frequency\n[Always / Sometimes / Once]\n[If sometimes: conditions that seem to affect it]\n\n### When it started\n[Date/time or after which change]\n\n### Related context\n[Recent changes, similar past bugs, relevant business context]\n```\n\n## Precision Techniques\n\n### Describe Behavior, Not Interpretation\n\n| Imprecise (Interpretation) | Precise (Behavior) |\n|---------------------------|-------------------|\n| \"Login is broken\" | \"Login form returns 401 with valid credentials\" |\n| \"It's slow\" | \"Page load takes 12 seconds, was 2 seconds\" |\n| \"Data is wrong\" | \"Order total shows $0.00, should be $47.99\" |\n| \"It crashed\" | \"Node process exits with code 137 (OOM)\" |\n\n### Quantify When Possible\n\n| Vague | Quantified |\n|-------|------------|\n| \"Sometimes fails\" | \"Fails 3 out of 10 times\" |\n| \"It's slow\" | \"Response time is 8,000ms, expected 200ms\" |\n| \"After a while\" | \"After 30 minutes of operation\" |\n| \"Under load\" | \"At 500 concurrent connections\" |\n\n### Capture Exact Error Messages\n\n**Bad:**\n> It says something about connection being refused\n\n**Good:**\n> Error: connect ECONNREFUSED 127.0.0.1:5432\n>     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1141:16)\n\n### Include Stack Traces\n\nStack traces show:\n- Where the error occurred\n- The call chain that led there\n- Which file/line to investigate\n\n**Read stack traces bottom-to-top** for call flow, top-to-bottom for immediate cause.\n\n### Note What Changed\n\n| Change Type | How It Helps |\n|-------------|--------------|\n| Code changes | `git diff` since it last worked |\n| Deploys | Correlation with deploy time |\n| Config changes | Environment variable diffs |\n| Data changes | New/modified records |\n| External changes | Dependencies, APIs, infrastructure |\n\n## Symptom Categories\n\n### Error Symptoms\n\nQuestions to answer:\n- What is the exact error message?\n- What is the error code/type?\n- Is there a stack trace?\n- Which operation triggered it?\n- Is it consistent or intermittent?\n\n### Behavioral Symptoms\n\nQuestions to answer:\n- What does the system do?\n- What should it do instead?\n- Are there side effects? (data changed, emails sent, etc.)\n- Is the behavior consistent?\n\n### Performance Symptoms\n\nQuestions to answer:\n- What metric is affected? (latency, throughput, CPU, memory)\n- What are the current values?\n- What are the expected values?\n- When did it change?\n- Is it constant or variable?\n\n### Data Symptoms\n\nQuestions to answer:\n- What data is wrong?\n- What should it be?\n- When did it become wrong?\n- Which records are affected? (all, some, pattern)\n- Can you see the correct data somewhere else?\n\n## Environment Specification\n\n### What to Capture\n\n| Component | Details to Include |\n|-----------|-------------------|\n| Application | Version, commit SHA, build date |\n| Runtime | Node version, Python version, etc. |\n| Database | Version, schema version, relevant data |\n| OS | Version, architecture |\n| Browser | Name, version, extensions |\n| Network | VPN, proxy, firewall |\n| Config | Relevant environment variables |\n\n### Environment Comparison\n\nIf it works in one environment and not another:\n\n```markdown\n## Working Environment\n- Node 18.17.0\n- PostgreSQL 14.1\n- Redis 6.2\n- Env: development\n\n## Failing Environment\n- Node 18.19.0  ← Version difference\n- PostgreSQL 14.1\n- Redis 7.0     ← Version difference\n- Env: staging\n```\n\nDifferences are prime suspects.\n\n## Reproduction Quality\n\n| Level | Description | Debuggability |\n|-------|-------------|---------------|\n| **Deterministic** | Happens every time with specific steps | Easy |\n| **Statistical** | Happens X% of the time | Medium |\n| **Environmental** | Happens only in certain environments | Medium |\n| **Temporal** | Happens after N minutes/hours | Hard |\n| **Heisenbug** | Disappears when you observe it | Very hard |\n| **One-time** | Happened once, can't reproduce | Hardest |\n\nFor non-deterministic bugs, capture as much context as possible from the occurrence.\n\n## Output Format\n\nWhen specifying a symptom:\n\n```markdown\n## Bug Specification\n\n### Summary\nUser authentication fails silently on mobile Safari\n\n### What Happens\nUser enters valid credentials, clicks login, form resets to empty state.\nNo error message displayed, no network error in console.\n\n### What Should Happen\nUser should be redirected to dashboard, or see error message if auth fails.\n\n### Error Details\nNo JavaScript errors in console.\nNetwork tab shows POST /api/auth returns 200 OK.\nResponse body: {\"success\": true, \"token\": \"eyJ...\"}\n\n### Steps to Reproduce\n1. Open app in Safari on iOS 16\n2. Enter valid email/password\n3. Tap \"Login\"\n4. Observe: form clears, stays on login page\n\n### Environment\n- iOS 16.4, Safari\n- App version: 2.1.0\n- API version: production (v3.2.1)\n\n### Frequency\nAlways on iOS Safari\nNever on Chrome, Firefox, or Safari macOS\n\n### When It Started\nFirst reported: Jan 15\nPossibly after deploy of v2.1.0 on Jan 14\n\n### Related Context\nv2.1.0 included refactor of auth flow to use new cookie settings.\nSafari has stricter third-party cookie handling.\n```\n"
    },
    {
      "name": "testing-techniques.md",
      "path": "references/testing-techniques.md",
      "content": "# Testing Techniques\n\nSystematic methods for confirming or eliminating hypotheses.\n\n## Why This Matters\n\nRandom testing wastes time. Systematic testing efficiently narrows down the root cause. Each test should either confirm a hypothesis, eliminate it, or provide information for better hypotheses.\n\n## Core Techniques\n\n### 1. Binary Search\n\n**When to use:** Bug is in a large codebase or long time range.\n\n**How it works:**\n```\n[---------------------?---------------------]\n                      ↓\n[---------?---------] [-------------------]\n          ↓\n[---?---] [---------]\n    ↓\n[?] [---]\n ↓\nFound!\n```\n\n**In code:**\n1. Comment out half the code\n2. Still fails? Bug is in remaining half\n3. Works? Bug is in commented half\n4. Repeat until isolated\n\n**In time (git bisect):**\n```bash\ngit bisect start\ngit bisect bad HEAD           # Current version is bad\ngit bisect good v1.2.0        # This version was good\n# Git checks out middle commit\n# You test: good or bad?\ngit bisect good  # or git bisect bad\n# Repeat until found\n```\n\n**In data:**\n1. Take half the input data\n2. Still fails? Bug is triggered by this half\n3. Works? Bug is triggered by other half\n4. Repeat until minimal failing input found\n\n### 2. Substitution\n\n**When to use:** Suspect a specific component.\n\n**How it works:** Replace the suspected component with a known-good version.\n\n| Suspected | Substitution |\n|-----------|--------------|\n| Database | Hardcode the expected response |\n| API call | Return mock data |\n| Config | Use known-working config |\n| Library | Downgrade to previous version |\n| Service | Point to different instance |\n\n**Example:**\n```javascript\n// Original\nconst users = await userService.getUsers();\n\n// Substitution test\nconst users = [{ id: 1, name: 'Test User' }]; // Hardcoded\n\n// If bug disappears: problem is in userService\n// If bug persists: problem is elsewhere\n```\n\n### 3. Isolation\n\n**When to use:** Bug happens in a complex system.\n\n**How it works:** Test the suspected component completely alone.\n\n**Techniques:**\n- Write a minimal script that only calls the failing function\n- Create a test case that exercises just the buggy path\n- Remove all other code and dependencies\n- Use a REPL to test interactively\n\n**Example:**\n```javascript\n// Isolation test\nconst { calculateTotal } = require('./order-utils');\n\n// Test in isolation with controlled input\nconst result = calculateTotal([\n  { price: 10, quantity: 2 },\n  { price: 5, quantity: 3 }\n]);\n\nconsole.log(result); // Expected: 35\n```\n\n### 4. Injection\n\n**When to use:** Bug depends on specific conditions that are hard to reproduce.\n\n**How it works:** Force the suspected condition to occur.\n\n| Condition | Injection Method |\n|-----------|------------------|\n| Error response | Mock to return error |\n| Slow response | Add artificial delay |\n| Empty data | Pass empty array/object |\n| Specific time | Mock Date.now() |\n| Race condition | Add sleeps to control timing |\n| Resource exhaustion | Artificially limit resources |\n\n**Example:**\n```javascript\n// Inject slow response to test timeout handling\njest.mock('./api', () => ({\n  fetchUsers: () => new Promise(resolve =>\n    setTimeout(() => resolve([]), 10000) // 10 second delay\n  )\n}));\n```\n\n### 5. Logging\n\n**When to use:** Can't see what's happening inside.\n\n**How it works:** Add output to observe internal state.\n\n**Strategic logging:**\n```javascript\n// Log inputs\nconsole.log('calculateTotal input:', JSON.stringify(items));\n\n// Log intermediate state\nconsole.log('subtotal before discount:', subtotal);\n\n// Log decisions\nconsole.log('discount applied:', discount, 'because:', reason);\n\n// Log outputs\nconsole.log('calculateTotal output:', total);\n```\n\n**What to log:**\n- Function inputs and outputs\n- Branch decisions (which if path taken)\n- Loop iterations (first, last, count)\n- External call request/response\n- Error details and stack traces\n- Timestamps for performance\n\n**Structured logging for production:**\n```javascript\nlogger.info('Processing order', {\n  orderId: order.id,\n  itemCount: order.items.length,\n  userId: order.userId,\n  timestamp: Date.now()\n});\n```\n\n### 6. Comparison\n\n**When to use:** It works in one case but not another.\n\n**How it works:** Find the difference between working and failing cases.\n\n**Compare:**\n- Working vs. failing input\n- Working vs. failing environment\n- Working vs. failing time\n- Previous version vs. current version\n\n**Example:**\n```markdown\n## Working Case\n- Input: { userId: 1, items: [{ id: 'A', qty: 1 }] }\n- User: Regular user\n- Time: Monday 10 AM\n- Result: Success\n\n## Failing Case\n- Input: { userId: 2, items: [{ id: 'B', qty: 0 }] }  ← qty: 0\n- User: Admin user\n- Time: Monday 10 AM\n- Result: Error\n\nHypothesis: qty: 0 causes division by zero or validation failure\n```\n\n### 7. Simplification\n\n**When to use:** Complex scenario, want minimal reproduction.\n\n**How it works:** Remove elements until bug disappears, then add back the essential one.\n\n**Process:**\n1. Start with full failing case\n2. Remove one element\n3. Still fails? Element wasn't required\n4. Works? Element is required for bug\n5. Repeat until minimal case\n\n**Result:** Minimal reproduction with only essential elements.\n\n### 8. Reversal\n\n**When to use:** Something that used to work now fails.\n\n**How it works:** Go back to when it worked, then forward to when it broke.\n\n**Techniques:**\n- `git checkout` previous commits\n- Roll back deployments\n- Restore database from backup\n- Revert config changes\n\n**Once you find the breaking change:** Examine what that change did to understand why it broke things.\n\n## Test Design\n\n### The Scientific Method\n\nFor each hypothesis:\n\n```markdown\n**Hypothesis:** [What you think is wrong]\n\n**Prediction:** If this hypothesis is true, then [observable outcome]\n\n**Experiment:** [Specific action to test]\n\n**Expected results:**\n- If hypothesis TRUE: [what you'll observe]\n- If hypothesis FALSE: [what you'll observe]\n\n**Actual result:** [what happened]\n\n**Conclusion:** [confirmed / eliminated / inconclusive]\n```\n\n### Good Tests\n\n| Quality | Description |\n|---------|-------------|\n| **Specific** | Tests exactly one hypothesis |\n| **Observable** | Clear pass/fail outcome |\n| **Reversible** | Can undo changes after test |\n| **Quick** | Fast feedback loop |\n| **Safe** | Won't cause more damage |\n\n### Test Checklist\n\nBefore running a test:\n- [ ] Do I know what I'm testing?\n- [ ] Do I know what result means?\n- [ ] Can I undo this change?\n- [ ] Will this affect users/data?\n- [ ] Am I changing only one thing?\n\n## Debugging Tools\n\n### General\n\n| Tool | Use For |\n|------|---------|\n| Debugger | Step through code, inspect variables |\n| REPL | Test code snippets interactively |\n| Logging | Observe internal state |\n| Profiler | Find performance bottlenecks |\n| Network inspector | See HTTP requests/responses |\n\n### Language-Specific\n\n| Language | Tools |\n|----------|-------|\n| JavaScript | Chrome DevTools, node --inspect, console.log |\n| Python | pdb, ipdb, print(), logging |\n| Java | IDE debugger, jstack, jmap |\n| Go | delve, fmt.Printf, runtime/pprof |\n\n### Database\n\n| Task | Tool/Command |\n|------|--------------|\n| Query analysis | EXPLAIN ANALYZE |\n| Lock inspection | pg_stat_activity, SHOW PROCESSLIST |\n| Slow queries | Slow query log |\n| Connection status | Pool metrics, max_connections |\n\n### Production\n\n| Task | Tool |\n|------|------|\n| Distributed tracing | Jaeger, Zipkin, Datadog APM |\n| Log aggregation | ELK, Splunk, CloudWatch |\n| Metrics | Prometheus, Grafana, Datadog |\n| Error tracking | Sentry, Bugsnag, Rollbar |\n\n## Output Format\n\nWhen documenting tests:\n\n```markdown\n## Test Log\n\n### Test 1: Check database connection pool\n\n**Hypothesis:** Connection pool exhausted\n\n**Action:** Query pg_stat_activity for connection count\n\n**Command:**\n```sql\nSELECT count(*) FROM pg_stat_activity WHERE datname = 'myapp';\n```\n\n**Result:** 12 connections (pool max is 20)\n\n**Conclusion:** ELIMINATED - Pool is not exhausted\n\n---\n\n### Test 2: Check query performance\n\n**Hypothesis:** Query missing index\n\n**Action:** Run EXPLAIN ANALYZE on suspect query\n\n**Result:**\n```\nSeq Scan on users  (cost=0.00..1000.00 rows=50000)\n```\n\n**Conclusion:** CONFIRMED - Full table scan, missing index\n```\n"
    }
  ],
  "tags": [
    "debugging",
    "diagnosis",
    "troubleshooting",
    "error-analysis",
    "core-workflow"
  ],
  "dependsOn": []
}