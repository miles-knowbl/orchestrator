{
  "id": "perf-analysis",
  "name": "perf-analysis",
  "version": "1.0.0",
  "description": "Identify and resolve performance bottlenecks in code and systems. Profiles CPU, memory, and I/O usage. Analyzes database queries, API latency, and frontend rendering. Conducts load testing and provides optimization recommendations with measurable impact.",
  "phase": "VALIDATE",
  "category": "core",
  "content": "# Performance Analysis\n\nIdentify and fix performance bottlenecks.\n\n## When to Use\n\n- **Slow responses** — API or page load times degraded\n- **High resource usage** — CPU, memory, or I/O spikes\n- **Scaling issues** — System struggles under load\n- **Before launch** — Validate performance requirements\n- **After changes** — Verify no performance regression\n- **Cost optimization** — Reduce infrastructure costs\n- When you say: \"why is this slow\", \"profile this\", \"load test\", \"optimize\"\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `profiling-tools.md` | Tools for performance measurement |\n| `optimization-patterns.md` | Common optimization approaches |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `database-optimization.md` | For database performance |\n| `frontend-performance.md` | For UI performance |\n| `load-testing.md` | For capacity testing |\n\n**Verification:** Ensure performance metrics are measured before and after changes.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| `PERF-ANALYSIS.md` | Project root | Always |\n\n## Core Concept\n\nPerformance analysis answers: **\"Why is this slow and how do we fix it?\"**\n\nPerformance is about:\n- **Latency** — How long does it take? (response time)\n- **Throughput** — How much can it handle? (requests/second)\n- **Resource efficiency** — How much does it cost? (CPU, memory, I/O)\n\nThe optimization process:\n1. **Measure** — Get baseline numbers\n2. **Identify** — Find the bottleneck\n3. **Optimize** — Fix the bottleneck\n4. **Verify** — Confirm improvement\n5. **Repeat** — Next bottleneck\n\n## The Performance Analysis Process\n\n```\n┌─────────────────────────────────────────────────────────┐\n│            PERFORMANCE ANALYSIS PROCESS                 │\n│                                                         │\n│  1. DEFINE REQUIREMENTS                                 │\n│     └─→ What are acceptable performance targets?        │\n│                                                         │\n│  2. MEASURE BASELINE                                    │\n│     └─→ Current latency, throughput, resource usage     │\n│                                                         │\n│  3. IDENTIFY BOTTLENECKS                                │\n│     └─→ Profile, trace, analyze metrics                 │\n│                                                         │\n│  4. ANALYZE ROOT CAUSE                                  │\n│     └─→ Why is this the bottleneck?                     │\n│                                                         │\n│  5. OPTIMIZE                                            │\n│     └─→ Apply targeted fix                              │\n│                                                         │\n│  6. VERIFY IMPROVEMENT                                  │\n│     └─→ Measure again, compare to baseline              │\n│                                                         │\n│  7. DOCUMENT & MONITOR                                  │\n│     └─→ Record findings, set up alerts                  │\n└─────────────────────────────────────────────────────────┘\n```\n\n## Step 1: Define Performance Requirements\n\n### Setting Targets\n\n```markdown\n## Performance Requirements\n\n### Response Time (Latency)\n| Endpoint | p50 | p95 | p99 |\n|----------|-----|-----|-----|\n| GET /api/users | <50ms | <100ms | <200ms |\n| POST /api/orders | <200ms | <500ms | <1s |\n| Page load (FCP) | <1s | <2s | <3s |\n\n### Throughput\n| Scenario | Target |\n|----------|--------|\n| Normal load | 1,000 req/s |\n| Peak load | 5,000 req/s |\n| Sustained | 500 req/s for 1 hour |\n\n### Resource Limits\n| Resource | Limit |\n|----------|-------|\n| CPU | <70% average |\n| Memory | <80% of available |\n| Database connections | <80% of pool |\n```\n\n### Performance Budgets\n\n```markdown\n## Frontend Performance Budget\n\n| Metric | Budget |\n|--------|--------|\n| First Contentful Paint (FCP) | <1.8s |\n| Largest Contentful Paint (LCP) | <2.5s |\n| First Input Delay (FID) | <100ms |\n| Cumulative Layout Shift (CLS) | <0.1 |\n| Time to Interactive (TTI) | <3.8s |\n| Total Bundle Size | <200KB gzipped |\n| JavaScript | <100KB gzipped |\n| CSS | <50KB gzipped |\n| Images | <500KB total |\n```\n\n## Step 2: Measure Baseline\n\n### What to Measure\n\n| Layer | Metrics |\n|-------|---------|\n| **Frontend** | FCP, LCP, TTI, bundle size, render time |\n| **API** | Response time (p50/p95/p99), throughput, error rate |\n| **Database** | Query time, connection pool, locks |\n| **Infrastructure** | CPU, memory, disk I/O, network |\n\n### Measurement Tools\n\n```bash\n# API response time\ncurl -w \"@curl-format.txt\" -o /dev/null -s \"http://api.example.com/users\"\n\n# curl-format.txt:\n#     time_namelookup:  %{time_namelookup}s\\n\n#        time_connect:  %{time_connect}s\\n\n#     time_appconnect:  %{time_appconnect}s\\n\n#    time_pretransfer:  %{time_pretransfer}s\\n\n#       time_redirect:  %{time_redirect}s\\n\n#  time_starttransfer:  %{time_starttransfer}s\\n\n#                     ----------\\n\n#          time_total:  %{time_total}s\\n\n```\n\n```typescript\n// Application-level timing\nconst start = performance.now();\nconst result = await heavyOperation();\nconst duration = performance.now() - start;\nconsole.log(`Operation took ${duration}ms`);\n\n// With context\nconsole.time('database-query');\nconst users = await db.users.findMany();\nconsole.timeEnd('database-query');\n```\n\n### Baseline Report Template\n\n```markdown\n## Performance Baseline Report\n**Date:** 2024-01-15\n**Environment:** Production\n**Load:** Normal (500 req/min)\n\n### API Endpoints\n| Endpoint | p50 | p95 | p99 | Throughput |\n|----------|-----|-----|-----|------------|\n| GET /api/users | 45ms | 120ms | 350ms | 100 req/s |\n| GET /api/orders | 80ms | 250ms | 800ms | 50 req/s |\n| POST /api/orders | 150ms | 400ms | 1.2s | 20 req/s |\n\n### Database\n| Query | Avg Time | Calls/min |\n|-------|----------|-----------|\n| SELECT users | 5ms | 1000 |\n| SELECT orders JOIN | 45ms | 500 |\n| INSERT orders | 15ms | 200 |\n\n### Resources\n| Resource | Average | Peak |\n|----------|---------|------|\n| CPU | 35% | 65% |\n| Memory | 2.1GB / 4GB | 2.8GB |\n| DB Connections | 15 / 50 | 35 |\n```\n\n## Step 3: Identify Bottlenecks\n\n### The Bottleneck Hierarchy\n\n```\n┌─────────────────────────────────────────────────────────┐\n│              WHERE IS THE BOTTLENECK?                   │\n│                                                         │\n│  Network?                                               │\n│  ├─→ DNS resolution                                     │\n│  ├─→ TLS handshake                                      │\n│  ├─→ Bandwidth                                          │\n│  └─→ Latency (geography)                                │\n│                                                         │\n│  Application?                                           │\n│  ├─→ CPU-bound (computation)                            │\n│  ├─→ Memory-bound (allocations, GC)                     │\n│  ├─→ I/O-bound (waiting for external)                   │\n│  └─→ Concurrency (locks, thread pool)                   │\n│                                                         │\n│  Database?                                              │\n│  ├─→ Slow queries                                       │\n│  ├─→ Missing indexes                                    │\n│  ├─→ Lock contention                                    │\n│  └─→ Connection pool exhaustion                         │\n│                                                         │\n│  External Services?                                     │\n│  ├─→ Third-party API latency                            │\n│  ├─→ Cache misses                                       │\n│  └─→ Queue backlog                                      │\n│                                                         │\n└─────────────────────────────────────────────────────────┘\n```\n\n### Profiling Tools\n\n| Type | Node.js | Browser | Python |\n|------|---------|---------|--------|\n| CPU | `--prof`, clinic.js | DevTools Performance | cProfile, py-spy |\n| Memory | `--inspect`, heapdump | DevTools Memory | memory_profiler |\n| Async | clinic.js bubbleprof | DevTools | asyncio debug |\n| Tracing | OpenTelemetry | Lighthouse | OpenTelemetry |\n\n### Quick Diagnostics\n\n```typescript\n// Find slow operations with simple timing\nasync function profiledHandler(req: Request, res: Response) {\n  const timings: Record<string, number> = {};\n  \n  const time = async <T>(name: string, fn: () => Promise<T>): Promise<T> => {\n    const start = performance.now();\n    const result = await fn();\n    timings[name] = performance.now() - start;\n    return result;\n  };\n  \n  const user = await time('getUser', () => userService.getById(req.params.id));\n  const orders = await time('getOrders', () => orderService.getByUser(user.id));\n  const enriched = await time('enrich', () => enrichOrders(orders));\n  \n  console.log('Timings:', timings);\n  // Timings: { getUser: 5, getOrders: 450, enrich: 12 }\n  // ^ getOrders is the bottleneck!\n  \n  res.json(enriched);\n}\n```\n\n→ See `references/profiling-tools.md`\n\n## Step 4: Analyze Root Cause\n\n### Common Bottleneck Patterns\n\n| Symptom | Likely Cause | Investigation |\n|---------|--------------|---------------|\n| High CPU, fast response | Efficient but heavy computation | Profile CPU |\n| High CPU, slow response | Inefficient algorithm | Profile hot paths |\n| Low CPU, slow response | I/O bound (DB, network, disk) | Trace external calls |\n| Memory growing | Leak or unbounded cache | Heap snapshot |\n| Periodic slowdowns | GC pauses or cron jobs | Correlate with logs |\n| Slow under load only | Contention or pool exhaustion | Load test + profile |\n\n### Database Analysis\n\n```sql\n-- PostgreSQL: Find slow queries\nSELECT \n  query,\n  calls,\n  mean_exec_time,\n  total_exec_time\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n-- Find missing indexes\nSELECT\n  schemaname,\n  tablename,\n  seq_scan,\n  idx_scan,\n  seq_tup_read\nFROM pg_stat_user_tables\nWHERE seq_scan > idx_scan\nORDER BY seq_tup_read DESC;\n\n-- Analyze specific query\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT * FROM orders \nWHERE user_id = 123 \nORDER BY created_at DESC \nLIMIT 10;\n```\n\n### N+1 Query Detection\n\n```typescript\n// BAD: N+1 queries\nconst users = await User.findAll();\nfor (const user of users) {\n  user.orders = await Order.findAll({ where: { userId: user.id } });\n  // This runs N additional queries!\n}\n\n// GOOD: Eager loading\nconst users = await User.findAll({\n  include: [{ model: Order }]\n});\n// Single query with JOIN\n```\n\n→ See `references/database-optimization.md`\n\n## Step 5: Optimize\n\n### Optimization Strategies\n\n| Strategy | When to Use | Example |\n|----------|-------------|---------|\n| **Caching** | Repeated expensive operations | Redis, CDN |\n| **Indexing** | Slow database queries | Add missing indexes |\n| **Batching** | Many small operations | Bulk inserts |\n| **Async/Parallel** | Independent operations | Promise.all |\n| **Lazy loading** | Data not always needed | Load on demand |\n| **Pagination** | Large result sets | Limit + offset |\n| **Denormalization** | Complex joins | Store computed values |\n| **Algorithm** | Inefficient code | Better Big-O |\n\n### Code Optimizations\n\n```typescript\n// SLOW: Sequential async\nfor (const id of ids) {\n  const result = await fetchData(id);\n  results.push(result);\n}\n\n// FAST: Parallel async\nconst results = await Promise.all(ids.map(id => fetchData(id)));\n\n// FAST with concurrency limit\nimport pLimit from 'p-limit';\nconst limit = pLimit(10);\nconst results = await Promise.all(\n  ids.map(id => limit(() => fetchData(id)))\n);\n```\n\n```typescript\n// SLOW: Repeated expensive computation\nfunction renderUsers(users: User[]) {\n  return users.map(user => ({\n    ...user,\n    displayName: computeExpensiveDisplayName(user), // Called every render\n  }));\n}\n\n// FAST: Memoization\nconst memoizedDisplayName = memoize(computeExpensiveDisplayName);\nfunction renderUsers(users: User[]) {\n  return users.map(user => ({\n    ...user,\n    displayName: memoizedDisplayName(user),\n  }));\n}\n```\n\n### Caching Patterns\n\n```typescript\n// Cache-aside pattern\nasync function getUser(id: string): Promise<User> {\n  // Check cache\n  const cached = await cache.get(`user:${id}`);\n  if (cached) return JSON.parse(cached);\n  \n  // Fetch from DB\n  const user = await db.users.findById(id);\n  \n  // Store in cache\n  await cache.set(`user:${id}`, JSON.stringify(user), 'EX', 3600);\n  \n  return user;\n}\n\n// Write-through pattern\nasync function updateUser(id: string, data: Partial<User>): Promise<User> {\n  // Update DB\n  const user = await db.users.update(id, data);\n  \n  // Update cache\n  await cache.set(`user:${id}`, JSON.stringify(user), 'EX', 3600);\n  \n  return user;\n}\n```\n\n→ See `references/optimization-patterns.md`\n\n## Step 6: Verify Improvement\n\n### Before/After Comparison\n\n```markdown\n## Optimization Results\n\n### Change\nAdded index on `orders.user_id` and implemented eager loading.\n\n### Results\n| Metric | Before | After | Improvement |\n|--------|--------|-------|-------------|\n| GET /api/users/:id/orders p50 | 450ms | 45ms | **90%** |\n| GET /api/users/:id/orders p99 | 1.2s | 120ms | **90%** |\n| Database CPU | 65% | 25% | **62%** |\n| Queries per request | 51 | 2 | **96%** |\n\n### Verification\n- [x] Load test passed (1000 req/s sustained)\n- [x] No errors in 1 hour test\n- [x] Memory stable (no leaks)\n```\n\n### Statistical Validity\n\n```typescript\n// Don't trust single measurements\n// Run multiple times and use statistics\n\nimport { mean, std, quantile } from 'simple-statistics';\n\nasync function benchmark(fn: () => Promise<void>, iterations = 100) {\n  const times: number[] = [];\n  \n  // Warmup\n  for (let i = 0; i < 10; i++) await fn();\n  \n  // Measure\n  for (let i = 0; i < iterations; i++) {\n    const start = performance.now();\n    await fn();\n    times.push(performance.now() - start);\n  }\n  \n  return {\n    mean: mean(times),\n    std: std(times),\n    p50: quantile(times, 0.5),\n    p95: quantile(times, 0.95),\n    p99: quantile(times, 0.99),\n    min: Math.min(...times),\n    max: Math.max(...times),\n  };\n}\n```\n\n## Step 7: Document & Monitor\n\n### Performance Documentation\n\n```markdown\n## Performance Optimization Record\n\n**Date:** 2024-01-15\n**Author:** @engineer\n**Component:** Order Service\n\n### Problem\nGET /api/users/:id/orders endpoint taking 450ms average,\ncausing timeout errors under load.\n\n### Analysis\n- Profiling showed 95% time in database queries\n- N+1 query pattern: 1 query for user + N queries for orders\n- Missing index on orders.user_id\n\n### Solution\n1. Added composite index: `CREATE INDEX idx_orders_user_date ON orders(user_id, created_at DESC)`\n2. Implemented eager loading with single JOIN query\n3. Added Redis cache with 5-minute TTL\n\n### Results\n- Latency reduced from 450ms to 45ms (90% improvement)\n- Database load reduced by 60%\n- Can now handle 10x more concurrent requests\n\n### Monitoring\n- Alert if p99 > 200ms\n- Dashboard: grafana.example.com/d/orders\n```\n\n### Monitoring Setup\n\n```typescript\n// Custom metrics for performance monitoring\nimport { Histogram, Counter } from 'prom-client';\n\nconst httpDuration = new Histogram({\n  name: 'http_request_duration_seconds',\n  help: 'Duration of HTTP requests in seconds',\n  labelNames: ['method', 'route', 'status'],\n  buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5],\n});\n\nconst dbQueryDuration = new Histogram({\n  name: 'db_query_duration_seconds',\n  help: 'Duration of database queries',\n  labelNames: ['query_type', 'table'],\n  buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5],\n});\n\n// Middleware to track HTTP duration\napp.use((req, res, next) => {\n  const end = httpDuration.startTimer();\n  res.on('finish', () => {\n    end({ method: req.method, route: req.route?.path, status: res.statusCode });\n  });\n  next();\n});\n```\n\n→ See `references/load-testing.md`\n\n## Load Testing\n\n### Load Test Scenarios\n\n| Scenario | Purpose | Pattern |\n|----------|---------|---------|\n| **Smoke** | Basic sanity check | 1-2 users, few seconds |\n| **Load** | Normal production load | Expected users, 10-30 min |\n| **Stress** | Find breaking point | Ramp up until failure |\n| **Spike** | Handle sudden traffic | Sudden burst, then normal |\n| **Soak** | Find memory leaks | Moderate load, hours |\n\n### k6 Load Test Example\n\n```javascript\n// load-test.js\nimport http from 'k6/http';\nimport { check, sleep } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 100 },  // Ramp up\n    { duration: '5m', target: 100 },  // Stay at 100\n    { duration: '2m', target: 200 },  // Ramp to 200\n    { duration: '5m', target: 200 },  // Stay at 200\n    { duration: '2m', target: 0 },    // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(95)<500'],  // 95% under 500ms\n    http_req_failed: ['rate<0.01'],    // <1% errors\n  },\n};\n\nexport default function () {\n  const res = http.get('http://api.example.com/users');\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'response time < 500ms': (r) => r.timings.duration < 500,\n  });\n  \n  sleep(1);\n}\n```\n\n```bash\n# Run load test\nk6 run load-test.js\n\n# With output to cloud\nk6 run --out cloud load-test.js\n```\n\n→ See `references/load-testing.md`\n\n## Frontend Performance\n\n### Core Web Vitals\n\n| Metric | Good | Needs Work | Poor |\n|--------|------|------------|------|\n| **LCP** (Largest Contentful Paint) | ≤2.5s | ≤4s | >4s |\n| **FID** (First Input Delay) | ≤100ms | ≤300ms | >300ms |\n| **CLS** (Cumulative Layout Shift) | ≤0.1 | ≤0.25 | >0.25 |\n\n### Frontend Optimization Checklist\n\n```markdown\n## Frontend Performance Checklist\n\n### Loading\n- [ ] Bundle size minimized (code splitting)\n- [ ] Images optimized (WebP, lazy loading)\n- [ ] Critical CSS inlined\n- [ ] Fonts optimized (subset, preload)\n- [ ] Third-party scripts deferred\n\n### Rendering\n- [ ] No layout thrashing\n- [ ] Virtualized long lists\n- [ ] Debounced scroll/resize handlers\n- [ ] No forced synchronous layouts\n\n### Caching\n- [ ] Static assets have cache headers\n- [ ] Service worker for offline\n- [ ] API responses cached appropriately\n```\n\n→ See `references/frontend-performance.md`\n\n## Relationship to Other Skills\n\n| Skill | Relationship |\n|-------|--------------|\n| `architect` | Performance requirements in architecture |\n| `implement` | Write performant code from the start |\n| `code-review` | Review for performance issues |\n| `code-validation` | Performance tests validate behavior |\n| `debug-assist` | Performance issues are a type of bug |\n| `security-audit` | DoS prevention is security + performance |\n\n## Key Principles\n\n**Measure first.** Don't optimize without data.\n\n**Find the bottleneck.** Optimizing non-bottlenecks is waste.\n\n**One change at a time.** Know what helped.\n\n**Verify improvement.** Measure after, compare to before.\n\n**Good enough is enough.** Stop when targets are met.\n\n**Document findings.** Future you will thank you.\n\n## Mode-Specific Behavior\n\nPerformance analysis differs by orchestrator mode:\n\n### Greenfield Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Full system - establish baselines and performance budgets |\n| **Approach** | Comprehensive profiling and load testing |\n| **Patterns** | Free choice of monitoring and optimization patterns |\n| **Deliverables** | Full PERF-ANALYSIS.md with baselines, budgets, and test results |\n| **Validation** | Standard load testing (smoke, load, stress, soak) |\n| **Constraints** | Minimal - define targets based on requirements |\n\n**Greenfield performance analysis:**\n- Define performance requirements upfront\n- Establish baseline measurements\n- Set up performance monitoring\n- Proactive optimization during development\n- Full load testing before launch\n\n**Greenfield deliverables:**\n```markdown\n- PERF-ANALYSIS.md with baseline\n- Performance budgets defined\n- Monitoring dashboards created\n- Load test results documented\n```\n\n### Brownfield-Polish Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Gap-specific - identify and address performance gaps |\n| **Approach** | Extend existing monitoring and optimization |\n| **Patterns** | Should match existing performance patterns |\n| **Deliverables** | Delta updates to PERF-ANALYSIS.md |\n| **Validation** | Existing baselines plus new gap coverage |\n| **Constraints** | Don't regress existing performance |\n\n**Polish considerations:**\n- Baseline existing performance\n- Identify performance gaps\n- Ensure changes don't regress\n- Fill performance monitoring gaps\n- Targeted optimization of gaps\n\n**Polish focus areas:**\n```markdown\n- What's slow that shouldn't be?\n- What's missing monitoring?\n- Where are performance gaps?\n- How do we avoid regressions?\n```\n\n### Brownfield-Enterprise Mode\n\n| Aspect | Behavior |\n|--------|----------|\n| **Scope** | Change-specific - measure impact of specific changes only |\n| **Approach** | Surgical before/after comparison |\n| **Patterns** | Must conform exactly to existing performance standards |\n| **Deliverables** | Change record with performance impact documentation |\n| **Validation** | Full regression testing against baselines |\n| **Constraints** | Requires approval for any optimization; no speculative changes |\n\n**Enterprise performance analysis:**\n- Measure baseline before change\n- Measure after change\n- Verify no performance regression\n- Document any performance impact\n- Escalate if regression detected\n\n**Enterprise constraints:**\n- No speculative optimization\n- Changes must not degrade performance\n- Performance regression blocks deployment\n- Document performance impact of change\n\n### Performance Requirements by Mode\n\n| Mode | Response Time | Throughput | Resources |\n|------|---------------|------------|-----------|\n| **Greenfield** | Define from scratch | Define from scratch | Define budgets |\n| **Polish** | Match existing or improve | Match existing | Match existing |\n| **Enterprise** | No regression | No regression | No increase |\n\n### Load Testing by Mode\n\n| Mode | Test Types | Duration | Pass Criteria |\n|------|------------|----------|---------------|\n| **Greenfield** | All (smoke, load, stress, soak) | Comprehensive | Meet targets |\n| **Polish** | Load, regression | Focused | No regression + gaps |\n| **Enterprise** | Impact, regression | Minimal | No regression |\n\n## References\n\n- `references/profiling-tools.md`: CPU, memory, and async profiling\n- `references/database-optimization.md`: Query analysis and indexing\n- `references/optimization-patterns.md`: Caching, batching, algorithms\n- `references/load-testing.md`: k6, Artillery, stress testing\n- `references/frontend-performance.md`: Core Web Vitals, bundle optimization\n\n- `references/profiling-tools.md`: CPU, memory, and async profiling\n- `references/database-optimization.md`: Query analysis and indexing\n- `references/optimization-patterns.md`: Caching, batching, algorithms\n- `references/load-testing.md`: k6, Artillery, stress testing\n- `references/frontend-performance.md`: Core Web Vitals, bundle optimization",
  "references": [
    {
      "name": "database-optimization.md",
      "path": "references/database-optimization.md",
      "content": "# Database Optimization\n\nTechniques for optimizing database performance.\n\n## Query Analysis\n\n### Identifying Slow Queries\n\n```sql\n-- PostgreSQL: Enable slow query logging\nALTER SYSTEM SET log_min_duration_statement = 100;  -- ms\nSELECT pg_reload_conf();\n\n-- MySQL: Enable slow query log\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 0.1;  -- seconds\nSET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';\n\n-- View slow query log\n-- PostgreSQL: /var/log/postgresql/\n-- MySQL: SHOW VARIABLES LIKE 'slow_query_log_file';\n```\n\n### Execution Plan Analysis\n\n```sql\n-- PostgreSQL EXPLAIN\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT o.*, u.name as user_name\nFROM orders o\nJOIN users u ON o.user_id = u.id\nWHERE o.status = 'pending'\nORDER BY o.created_at DESC\nLIMIT 10;\n\n/* Output analysis:\nLimit  (cost=1000.00..1000.50 rows=10 width=200) (actual time=50.123..50.145 rows=10 loops=1)\n  ->  Sort  (cost=1000.00..1025.00 rows=10000 width=200) (actual time=50.120..50.130 rows=10 loops=1)\n        Sort Key: o.created_at DESC\n        Sort Method: top-N heapsort  Memory: 25kB\n        ->  Hash Join  (cost=100.00..800.00 rows=10000 width=200) (actual time=5.000..45.000 rows=10000 loops=1)\n              Hash Cond: (o.user_id = u.id)\n              ->  Seq Scan on orders o  (cost=0.00..500.00 rows=10000 width=150) (actual time=0.010..30.000 rows=10000 loops=1)\n                    Filter: (status = 'pending')\n                    Rows Removed by Filter: 90000    <- Reading 100k rows to get 10k!\n              ->  Hash  (cost=50.00..50.00 rows=1000 width=50) (actual time=4.500..4.500 rows=1000 loops=1)\n                    ->  Seq Scan on users u ...\nPlanning Time: 0.500 ms\nExecution Time: 50.500 ms\n\nProblems identified:\n1. Seq Scan on orders with filter removing 90% of rows -> Need index on status\n2. Sorting 10000 rows to get 10 -> Need composite index for ORDER BY\n*/\n```\n\n### Plan Node Types\n\n| Node Type | What It Means | Optimization |\n|-----------|---------------|--------------|\n| Seq Scan | Full table scan | Add index |\n| Index Scan | Using index | Good |\n| Index Only Scan | Using covering index | Best |\n| Bitmap Scan | Combines multiple indexes | Acceptable for complex queries |\n| Nested Loop | O(n*m) joins | Ensure inner side is indexed |\n| Hash Join | Builds hash table | Good for large joins |\n| Merge Join | Both sides sorted | Good for sorted data |\n| Sort | Sorting in memory/disk | Add index for ORDER BY |\n\n## Indexing Strategies\n\n### Index Types\n\n```sql\n-- B-tree (default): Equality and range queries\nCREATE INDEX idx_orders_created ON orders(created_at);\n-- Good for: =, <, >, <=, >=, BETWEEN, IN, IS NULL\n\n-- Hash: Equality only (PostgreSQL)\nCREATE INDEX idx_users_email ON users USING HASH (email);\n-- Good for: = only (faster than B-tree for equality)\n\n-- GIN: Full-text search, arrays, JSONB\nCREATE INDEX idx_products_tags ON products USING GIN (tags);\n-- Good for: @>, ?, ?&, ?| on arrays/jsonb\n\n-- GiST: Geometric, full-text\nCREATE INDEX idx_locations_point ON locations USING GIST (point);\n-- Good for: Spatial queries, range types\n\n-- Partial: Index subset of rows\nCREATE INDEX idx_orders_pending ON orders(created_at) \nWHERE status = 'pending';\n-- Only indexes pending orders\n```\n\n### Composite Indexes\n\n```sql\n-- Order matters! (leftmost prefix rule)\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n\n-- This index helps:\nWHERE user_id = 123                          -- ✓ Uses full index\nWHERE user_id = 123 AND status = 'pending'   -- ✓ Uses full index\nWHERE user_id = 123 ORDER BY status          -- ✓ Uses full index\n\n-- This index does NOT help:\nWHERE status = 'pending'                     -- ✗ Can't use (no user_id)\nORDER BY status                              -- ✗ Can't use (no user_id)\n```\n\n### Covering Indexes (Index-Only Scans)\n\n```sql\n-- Include non-key columns\nCREATE INDEX idx_orders_user_covering ON orders(user_id) \nINCLUDE (total, status);\n\n-- Query can be satisfied from index alone\nSELECT total, status FROM orders WHERE user_id = 123;\n-- Index-only scan: no table access needed!\n```\n\n### Index Maintenance\n\n```sql\n-- Find unused indexes (PostgreSQL)\nSELECT\n  schemaname,\n  tablename,\n  indexname,\n  idx_scan as times_used\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nAND indexname NOT LIKE '%pkey%';\n\n-- Find duplicate indexes\nSELECT\n  indrelid::regclass as table,\n  array_agg(indexrelid::regclass) as indexes\nFROM pg_index\nGROUP BY indrelid, indkey\nHAVING COUNT(*) > 1;\n\n-- Rebuild bloated indexes\nREINDEX INDEX idx_orders_user_id;\n-- Or concurrently (no locks)\nREINDEX INDEX CONCURRENTLY idx_orders_user_id;\n```\n\n## Query Optimization\n\n### N+1 Query Problem\n\n```typescript\n// BAD: N+1 queries\nconst orders = await Order.findAll({ where: { status: 'pending' } });\nfor (const order of orders) {\n  order.user = await User.findByPk(order.userId);  // N additional queries!\n  order.items = await OrderItem.findAll({ where: { orderId: order.id } });  // N more!\n}\n\n// GOOD: Eager loading (2 queries with JOIN)\nconst orders = await Order.findAll({\n  where: { status: 'pending' },\n  include: [\n    { model: User },\n    { model: OrderItem },\n  ],\n});\n\n// GOOD: Batch loading (3 queries total)\nconst orders = await Order.findAll({ where: { status: 'pending' } });\nconst userIds = [...new Set(orders.map(o => o.userId))];\nconst users = await User.findAll({ where: { id: userIds } });\nconst userMap = new Map(users.map(u => [u.id, u]));\norders.forEach(o => o.user = userMap.get(o.userId));\n```\n\n### Pagination\n\n```sql\n-- OFFSET pagination (slow for deep pages)\nSELECT * FROM orders ORDER BY id LIMIT 10 OFFSET 10000;\n-- Must scan 10010 rows!\n\n-- Keyset/Cursor pagination (fast)\nSELECT * FROM orders \nWHERE id > :last_id \nORDER BY id \nLIMIT 10;\n-- Always scans just 10 rows\n\n-- For complex sorting\nSELECT * FROM orders \nWHERE (created_at, id) > (:last_created_at, :last_id)\nORDER BY created_at, id\nLIMIT 10;\n```\n\n### Batch Operations\n\n```sql\n-- BAD: Individual inserts\nINSERT INTO items (name) VALUES ('a');\nINSERT INTO items (name) VALUES ('b');\nINSERT INTO items (name) VALUES ('c');\n\n-- GOOD: Batch insert\nINSERT INTO items (name) VALUES ('a'), ('b'), ('c');\n\n-- BAD: Individual updates\nUPDATE users SET last_seen = NOW() WHERE id = 1;\nUPDATE users SET last_seen = NOW() WHERE id = 2;\n\n-- GOOD: Batch update\nUPDATE users SET last_seen = NOW() WHERE id IN (1, 2, 3);\n\n-- GOOD: UPSERT batch\nINSERT INTO stats (user_id, count) \nVALUES (1, 1), (2, 1), (3, 1)\nON CONFLICT (user_id) \nDO UPDATE SET count = stats.count + 1;\n```\n\n### Query Rewrites\n\n```sql\n-- BAD: OR on different columns (can't use index efficiently)\nSELECT * FROM orders WHERE user_id = 123 OR status = 'pending';\n\n-- GOOD: UNION (uses indexes)\nSELECT * FROM orders WHERE user_id = 123\nUNION\nSELECT * FROM orders WHERE status = 'pending';\n\n-- BAD: Function on indexed column\nSELECT * FROM users WHERE LOWER(email) = 'test@example.com';\n\n-- GOOD: Expression index\nCREATE INDEX idx_users_email_lower ON users(LOWER(email));\n-- Or store normalized\nSELECT * FROM users WHERE email_normalized = 'test@example.com';\n\n-- BAD: LIKE with leading wildcard\nSELECT * FROM products WHERE name LIKE '%widget%';\n\n-- GOOD: Full-text search\nCREATE INDEX idx_products_name_fts ON products USING GIN(to_tsvector('english', name));\nSELECT * FROM products WHERE to_tsvector('english', name) @@ to_tsquery('widget');\n```\n\n## Connection Management\n\n### Connection Pooling\n\n```typescript\n// Node.js with pg-pool\nimport { Pool } from 'pg';\n\nconst pool = new Pool({\n  host: 'localhost',\n  database: 'myapp',\n  max: 20,                    // Max connections\n  idleTimeoutMillis: 30000,   // Close idle connections after 30s\n  connectionTimeoutMillis: 2000, // Fail if can't connect in 2s\n});\n\n// Use pool for queries\nconst result = await pool.query('SELECT * FROM users WHERE id = $1', [userId]);\n\n// Monitor pool\npool.on('error', (err) => console.error('Pool error', err));\npool.on('connect', () => console.log('New connection'));\npool.on('remove', () => console.log('Connection removed'));\n```\n\n### Connection Pool Sizing\n\n```markdown\n## Pool Sizing Guidelines\n\n### Formula\nconnections = ((core_count * 2) + effective_spindle_count)\n\nFor SSD: effective_spindle_count ≈ 1\nFor 4-core server with SSD: (4 * 2) + 1 = 9 connections\n\n### Per-Application\nIf you have 3 app servers, each with pool size 10:\n- Total connections: 30\n- Database max_connections should be > 30 + some buffer\n\n### Signs of Wrong Pool Size\nToo small:\n- Queries waiting for connections\n- Connection timeout errors\n\nToo large:\n- Memory pressure on database\n- Context switching overhead\n- Lock contention\n```\n\n## Caching Strategies\n\n### Query Result Caching\n\n```typescript\nimport Redis from 'ioredis';\n\nconst redis = new Redis();\n\nasync function getUser(userId: string): Promise<User> {\n  const cacheKey = `user:${userId}`;\n  \n  // Check cache\n  const cached = await redis.get(cacheKey);\n  if (cached) {\n    return JSON.parse(cached);\n  }\n  \n  // Query database\n  const user = await db.query('SELECT * FROM users WHERE id = $1', [userId]);\n  \n  // Cache result (expire in 5 minutes)\n  await redis.setex(cacheKey, 300, JSON.stringify(user));\n  \n  return user;\n}\n\n// Invalidate on update\nasync function updateUser(userId: string, data: Partial<User>): Promise<User> {\n  const user = await db.query(\n    'UPDATE users SET ... WHERE id = $1 RETURNING *',\n    [userId, ...]\n  );\n  \n  // Invalidate cache\n  await redis.del(`user:${userId}`);\n  \n  return user;\n}\n```\n\n### Materialized Views\n\n```sql\n-- Create materialized view for expensive aggregation\nCREATE MATERIALIZED VIEW order_stats AS\nSELECT\n  user_id,\n  COUNT(*) as order_count,\n  SUM(total) as total_spent,\n  MAX(created_at) as last_order_at\nFROM orders\nGROUP BY user_id;\n\n-- Create index on materialized view\nCREATE INDEX idx_order_stats_user ON order_stats(user_id);\n\n-- Refresh periodically\nREFRESH MATERIALIZED VIEW order_stats;\n\n-- Refresh concurrently (no locks, requires unique index)\nREFRESH MATERIALIZED VIEW CONCURRENTLY order_stats;\n\n-- Use in queries\nSELECT u.*, os.order_count, os.total_spent\nFROM users u\nLEFT JOIN order_stats os ON u.id = os.user_id\nWHERE u.id = 123;\n```\n\n## Database-Specific Optimizations\n\n### PostgreSQL\n\n```sql\n-- Analyze tables for query planner\nANALYZE orders;\n\n-- Vacuum to reclaim space\nVACUUM orders;\n-- Or full vacuum (locks table)\nVACUUM FULL orders;\n\n-- Tune work_mem for complex queries\nSET work_mem = '256MB';\n-- Good for: sorting, hash joins\n-- Caution: per-operation, not per-query\n\n-- Parallel query settings\nSET max_parallel_workers_per_gather = 4;\nSET parallel_tuple_cost = 0.01;\n```\n\n### MySQL/MariaDB\n\n```sql\n-- Buffer pool (most important setting)\nSET GLOBAL innodb_buffer_pool_size = 4G;\n-- Should be ~70-80% of available RAM\n\n-- Query cache (deprecated in MySQL 8)\n-- Use application-level caching instead\n\n-- Index hints (use sparingly)\nSELECT * FROM orders FORCE INDEX (idx_status) WHERE status = 'pending';\n```\n\n### MongoDB\n\n```javascript\n// Compound indexes\ndb.orders.createIndex({ userId: 1, createdAt: -1 });\n\n// Text indexes\ndb.products.createIndex({ name: \"text\", description: \"text\" });\n\n// Aggregation pipeline optimization\ndb.orders.aggregate([\n  { $match: { status: \"pending\" } },  // Filter early!\n  { $sort: { createdAt: -1 } },\n  { $limit: 10 },\n  { $lookup: {\n      from: \"users\",\n      localField: \"userId\",\n      foreignField: \"_id\",\n      as: \"user\"\n  }}\n]);\n\n// allowDiskUse for large aggregations\ndb.orders.aggregate([...], { allowDiskUse: true });\n```\n"
    },
    {
      "name": "frontend-performance.md",
      "path": "references/frontend-performance.md",
      "content": "# Frontend Performance\n\nOptimizing client-side performance and Core Web Vitals.\n\n## Core Web Vitals\n\n### Metrics\n\n| Metric | Full Name | Measures | Good | Poor |\n|--------|-----------|----------|------|------|\n| **LCP** | Largest Contentful Paint | Loading | ≤2.5s | >4s |\n| **FID** | First Input Delay | Interactivity | ≤100ms | >300ms |\n| **CLS** | Cumulative Layout Shift | Visual stability | ≤0.1 | >0.25 |\n| **FCP** | First Contentful Paint | Initial render | ≤1.8s | >3s |\n| **TTFB** | Time to First Byte | Server response | ≤800ms | >1.8s |\n| **TTI** | Time to Interactive | Full interactivity | ≤3.8s | >7.3s |\n\n### Measuring\n\n```typescript\n// Using web-vitals library\nimport { getCLS, getFID, getLCP, getFCP, getTTFB } from 'web-vitals';\n\nfunction sendToAnalytics(metric) {\n  console.log(metric.name, metric.value);\n  \n  // Send to your analytics\n  fetch('/analytics', {\n    method: 'POST',\n    body: JSON.stringify({\n      name: metric.name,\n      value: metric.value,\n      id: metric.id,\n      navigationType: metric.navigationType,\n    }),\n  });\n}\n\ngetCLS(sendToAnalytics);\ngetFID(sendToAnalytics);\ngetLCP(sendToAnalytics);\ngetFCP(sendToAnalytics);\ngetTTFB(sendToAnalytics);\n```\n\n## LCP Optimization\n\n### What Affects LCP\n\n- Slow server response\n- Render-blocking resources\n- Slow resource load times\n- Client-side rendering\n\n### Optimizations\n\n```html\n<!-- Preload critical resources -->\n<link rel=\"preload\" href=\"/hero-image.jpg\" as=\"image\">\n<link rel=\"preload\" href=\"/critical.css\" as=\"style\">\n<link rel=\"preload\" href=\"/main.js\" as=\"script\">\n\n<!-- Preconnect to required origins -->\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://api.example.com\">\n\n<!-- Inline critical CSS -->\n<style>\n  /* Critical above-the-fold styles */\n  .hero { /* ... */ }\n</style>\n\n<!-- Defer non-critical CSS -->\n<link rel=\"preload\" href=\"/non-critical.css\" as=\"style\" onload=\"this.onload=null;this.rel='stylesheet'\">\n```\n\n```typescript\n// Optimize images\n// Next.js Image component\nimport Image from 'next/image';\n\n<Image\n  src=\"/hero.jpg\"\n  alt=\"Hero\"\n  width={1200}\n  height={600}\n  priority  // Preload LCP image\n  placeholder=\"blur\"\n  blurDataURL=\"data:image/jpeg;base64,...\"\n/>\n\n// Native lazy loading\n<img src=\"image.jpg\" loading=\"lazy\" alt=\"Below fold\">\n<img src=\"hero.jpg\" loading=\"eager\" alt=\"Above fold\">  // Don't lazy load LCP\n```\n\n## FID/INP Optimization\n\n### What Affects Interactivity\n\n- Long JavaScript tasks (>50ms)\n- Large JavaScript bundles\n- Main thread blocking\n\n### Optimizations\n\n```typescript\n// Break up long tasks\n// BAD: Long synchronous operation\nfunction processItems(items) {\n  for (const item of items) {\n    expensiveOperation(item);  // Blocks for 500ms\n  }\n}\n\n// GOOD: Yield to main thread\nasync function processItems(items) {\n  for (const item of items) {\n    expensiveOperation(item);\n    \n    // Yield every 50ms\n    if (shouldYield()) {\n      await yieldToMain();\n    }\n  }\n}\n\nfunction shouldYield() {\n  return performance.now() - lastYield > 50;\n}\n\nfunction yieldToMain() {\n  return new Promise(resolve => setTimeout(resolve, 0));\n}\n\n// Using scheduler.yield (when available)\nasync function processItems(items) {\n  for (const item of items) {\n    expensiveOperation(item);\n    await scheduler.yield();\n  }\n}\n```\n\n```typescript\n// Web Workers for heavy computation\n// main.js\nconst worker = new Worker('/worker.js');\n\nworker.postMessage({ data: largeDataSet });\nworker.onmessage = (e) => {\n  updateUI(e.data.result);\n};\n\n// worker.js\nself.onmessage = (e) => {\n  const result = heavyComputation(e.data);\n  self.postMessage({ result });\n};\n```\n\n## CLS Optimization\n\n### What Causes Layout Shift\n\n- Images without dimensions\n- Ads, embeds, iframes without dimensions\n- Dynamically injected content\n- Web fonts causing FOIT/FOUT\n\n### Optimizations\n\n```html\n<!-- Always include dimensions -->\n<img src=\"image.jpg\" width=\"800\" height=\"600\" alt=\"...\">\n\n<!-- Or use aspect-ratio -->\n<style>\n  .image-container {\n    aspect-ratio: 16 / 9;\n    width: 100%;\n  }\n</style>\n\n<!-- Reserve space for ads -->\n<style>\n  .ad-slot {\n    min-height: 250px;\n  }\n</style>\n\n<!-- Font loading strategy -->\n<style>\n  @font-face {\n    font-family: 'MyFont';\n    src: url('/font.woff2') format('woff2');\n    font-display: swap;  /* or optional */\n  }\n</style>\n\n<!-- Preload fonts -->\n<link rel=\"preload\" href=\"/font.woff2\" as=\"font\" type=\"font/woff2\" crossorigin>\n```\n\n```typescript\n// Avoid inserting content above existing content\n// BAD\nfunction addNotification(message) {\n  const notification = document.createElement('div');\n  notification.textContent = message;\n  document.body.prepend(notification);  // Shifts everything down\n}\n\n// GOOD\nfunction addNotification(message) {\n  const notification = document.createElement('div');\n  notification.textContent = message;\n  // Use fixed positioning or reserved space\n  document.getElementById('notification-area').appendChild(notification);\n}\n```\n\n## Bundle Optimization\n\n### Code Splitting\n\n```typescript\n// React lazy loading\nimport { lazy, Suspense } from 'react';\n\nconst HeavyComponent = lazy(() => import('./HeavyComponent'));\n\nfunction App() {\n  return (\n    <Suspense fallback={<Loading />}>\n      <HeavyComponent />\n    </Suspense>\n  );\n}\n\n// Route-based splitting\nconst routes = [\n  {\n    path: '/dashboard',\n    component: lazy(() => import('./pages/Dashboard')),\n  },\n  {\n    path: '/settings',\n    component: lazy(() => import('./pages/Settings')),\n  },\n];\n```\n\n### Tree Shaking\n\n```typescript\n// BAD: Import entire library\nimport _ from 'lodash';\n_.debounce(fn, 100);\n\n// GOOD: Import specific functions\nimport debounce from 'lodash/debounce';\ndebounce(fn, 100);\n\n// Or use lodash-es\nimport { debounce } from 'lodash-es';\n```\n\n### Bundle Analysis\n\n```bash\n# webpack-bundle-analyzer\nnpm install --save-dev webpack-bundle-analyzer\n\n# In webpack.config.js\nconst BundleAnalyzerPlugin = require('webpack-bundle-analyzer').BundleAnalyzerPlugin;\n\nmodule.exports = {\n  plugins: [\n    new BundleAnalyzerPlugin()\n  ]\n};\n\n# Next.js\nANALYZE=true npm run build\n\n# Vite\nnpx vite-bundle-visualizer\n```\n\n## Image Optimization\n\n### Modern Formats\n\n```html\n<!-- Use picture element for format fallback -->\n<picture>\n  <source srcset=\"image.avif\" type=\"image/avif\">\n  <source srcset=\"image.webp\" type=\"image/webp\">\n  <img src=\"image.jpg\" alt=\"...\">\n</picture>\n\n<!-- Responsive images -->\n<img \n  srcset=\"image-320.jpg 320w,\n          image-640.jpg 640w,\n          image-1280.jpg 1280w\"\n  sizes=\"(max-width: 320px) 280px,\n         (max-width: 640px) 580px,\n         1200px\"\n  src=\"image-1280.jpg\"\n  alt=\"...\"\n>\n```\n\n### Lazy Loading\n\n```typescript\n// Native lazy loading\n<img src=\"image.jpg\" loading=\"lazy\" alt=\"...\">\n\n// Intersection Observer for more control\nconst observer = new IntersectionObserver((entries) => {\n  entries.forEach((entry) => {\n    if (entry.isIntersecting) {\n      const img = entry.target;\n      img.src = img.dataset.src;\n      observer.unobserve(img);\n    }\n  });\n});\n\ndocument.querySelectorAll('img[data-src]').forEach((img) => {\n  observer.observe(img);\n});\n```\n\n## Caching Strategy\n\n### HTTP Caching\n\n```\n# Static assets (immutable)\nCache-Control: public, max-age=31536000, immutable\n\n# HTML (revalidate)\nCache-Control: no-cache\n\n# API responses (short cache)\nCache-Control: private, max-age=60\n```\n\n### Service Worker\n\n```typescript\n// sw.js\nconst CACHE_NAME = 'v1';\nconst STATIC_ASSETS = [\n  '/',\n  '/styles.css',\n  '/app.js',\n];\n\nself.addEventListener('install', (event) => {\n  event.waitUntil(\n    caches.open(CACHE_NAME).then((cache) => {\n      return cache.addAll(STATIC_ASSETS);\n    })\n  );\n});\n\nself.addEventListener('fetch', (event) => {\n  event.respondWith(\n    caches.match(event.request).then((response) => {\n      // Cache first, then network\n      return response || fetch(event.request);\n    })\n  );\n});\n\n// Stale-while-revalidate\nself.addEventListener('fetch', (event) => {\n  event.respondWith(\n    caches.open(CACHE_NAME).then((cache) => {\n      return cache.match(event.request).then((cached) => {\n        const fetched = fetch(event.request).then((response) => {\n          cache.put(event.request, response.clone());\n          return response;\n        });\n        return cached || fetched;\n      });\n    })\n  );\n});\n```\n\n## Rendering Patterns\n\n### Server-Side Rendering (SSR)\n\n```typescript\n// Next.js\nexport async function getServerSideProps() {\n  const data = await fetchData();\n  return { props: { data } };\n}\n\n// Benefits: Fast FCP, SEO\n// Drawbacks: Slower TTFB, server load\n```\n\n### Static Site Generation (SSG)\n\n```typescript\n// Next.js\nexport async function getStaticProps() {\n  const data = await fetchData();\n  return { props: { data }, revalidate: 60 };\n}\n\n// Benefits: Fastest TTFB, CDN cacheable\n// Drawbacks: Build time, stale data\n```\n\n### Client-Side Rendering (CSR)\n\n```typescript\n// React\nfunction Component() {\n  const [data, setData] = useState(null);\n  \n  useEffect(() => {\n    fetchData().then(setData);\n  }, []);\n  \n  if (!data) return <Loading />;\n  return <Content data={data} />;\n}\n\n// Benefits: Rich interactivity\n// Drawbacks: Slow FCP, poor SEO\n```\n\n### Streaming SSR\n\n```typescript\n// React 18 with streaming\nimport { renderToPipeableStream } from 'react-dom/server';\n\napp.get('/', (req, res) => {\n  const { pipe } = renderToPipeableStream(<App />, {\n    bootstrapScripts: ['/app.js'],\n    onShellReady() {\n      res.setHeader('Content-Type', 'text/html');\n      pipe(res);\n    },\n  });\n});\n```\n\n## Performance Budget\n\n### Budget Configuration\n\n```json\n// budget.json (Lighthouse CI)\n{\n  \"ci\": {\n    \"assert\": {\n      \"assertions\": {\n        \"categories:performance\": [\"error\", { \"minScore\": 0.9 }],\n        \"first-contentful-paint\": [\"error\", { \"maxNumericValue\": 1800 }],\n        \"largest-contentful-paint\": [\"error\", { \"maxNumericValue\": 2500 }],\n        \"cumulative-layout-shift\": [\"error\", { \"maxNumericValue\": 0.1 }],\n        \"total-blocking-time\": [\"error\", { \"maxNumericValue\": 300 }],\n        \"resource-summary:script:size\": [\"error\", { \"maxNumericValue\": 100000 }]\n      }\n    }\n  }\n}\n```\n\n### Monitoring\n\n```typescript\n// Track performance in production\nif ('PerformanceObserver' in window) {\n  // Long tasks\n  const longTaskObserver = new PerformanceObserver((list) => {\n    for (const entry of list.getEntries()) {\n      if (entry.duration > 50) {\n        console.warn('Long task detected:', entry);\n        // Send to monitoring\n      }\n    }\n  });\n  longTaskObserver.observe({ entryTypes: ['longtask'] });\n  \n  // Layout shifts\n  const clsObserver = new PerformanceObserver((list) => {\n    for (const entry of list.getEntries()) {\n      if (!entry.hadRecentInput) {\n        console.warn('Layout shift:', entry);\n      }\n    }\n  });\n  clsObserver.observe({ entryTypes: ['layout-shift'] });\n}\n```\n\n## Performance Checklist\n\n```markdown\n## Frontend Performance Checklist\n\n### Critical Rendering Path\n- [ ] Inline critical CSS\n- [ ] Defer non-critical CSS\n- [ ] Async/defer JavaScript\n- [ ] Preload key resources\n- [ ] Minimize render-blocking resources\n\n### Images\n- [ ] Use modern formats (WebP, AVIF)\n- [ ] Responsive images with srcset\n- [ ] Lazy load below-fold images\n- [ ] Specify dimensions\n- [ ] Compress appropriately\n\n### JavaScript\n- [ ] Code splitting implemented\n- [ ] Tree shaking configured\n- [ ] Bundle size under budget\n- [ ] No long tasks (>50ms)\n- [ ] Web Workers for heavy computation\n\n### Fonts\n- [ ] Preload critical fonts\n- [ ] Use font-display: swap\n- [ ] Subset fonts if possible\n- [ ] Self-host if beneficial\n\n### Caching\n- [ ] Appropriate Cache-Control headers\n- [ ] Service Worker for offline\n- [ ] CDN for static assets\n- [ ] API response caching\n\n### Monitoring\n- [ ] Core Web Vitals tracking\n- [ ] Performance budget enforced\n- [ ] Alerts for regressions\n```\n"
    },
    {
      "name": "load-testing.md",
      "path": "references/load-testing.md",
      "content": "# Load Testing\n\nTools and techniques for performance testing under load.\n\n## Load Test Types\n\n| Type | Purpose | Pattern | Duration |\n|------|---------|---------|----------|\n| **Smoke** | Verify basics work | Minimal load | 1-2 min |\n| **Load** | Normal production load | Expected users | 10-30 min |\n| **Stress** | Find breaking point | Ramp until failure | Until failure |\n| **Spike** | Handle traffic bursts | Sudden spike, then normal | 5-10 min |\n| **Soak/Endurance** | Find memory leaks | Moderate sustained load | 2-24 hours |\n\n## k6 Load Testing\n\n### Installation\n\n```bash\n# macOS\nbrew install k6\n\n# Linux\nsudo gpg -k\nsudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69\necho \"deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main\" | sudo tee /etc/apt/sources.list.d/k6.list\nsudo apt-get update\nsudo apt-get install k6\n\n# Docker\ndocker run -i grafana/k6 run - <script.js\n```\n\n### Basic Test\n\n```javascript\n// basic-test.js\nimport http from 'k6/http';\nimport { check, sleep } from 'k6';\n\nexport const options = {\n  vus: 10,           // 10 virtual users\n  duration: '30s',   // Run for 30 seconds\n};\n\nexport default function () {\n  const res = http.get('http://localhost:3000/api/users');\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'response time < 200ms': (r) => r.timings.duration < 200,\n  });\n  \n  sleep(1);  // Wait 1 second between iterations\n}\n```\n\n### Staged Load Test\n\n```javascript\n// staged-test.js\nimport http from 'k6/http';\nimport { check } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 50 },   // Ramp up to 50 users\n    { duration: '5m', target: 50 },   // Stay at 50 users\n    { duration: '2m', target: 100 },  // Ramp up to 100 users\n    { duration: '5m', target: 100 },  // Stay at 100 users\n    { duration: '2m', target: 0 },    // Ramp down to 0\n  ],\n  thresholds: {\n    http_req_duration: ['p(95)<500'],  // 95% of requests under 500ms\n    http_req_failed: ['rate<0.01'],    // Error rate under 1%\n  },\n};\n\nexport default function () {\n  const res = http.get('http://localhost:3000/api/users');\n  check(res, { 'status is 200': (r) => r.status === 200 });\n}\n```\n\n### Stress Test\n\n```javascript\n// stress-test.js\nimport http from 'k6/http';\nimport { check } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 100 },   // Below normal load\n    { duration: '5m', target: 100 },\n    { duration: '2m', target: 200 },   // Normal load\n    { duration: '5m', target: 200 },\n    { duration: '2m', target: 300 },   // Around breaking point\n    { duration: '5m', target: 300 },\n    { duration: '2m', target: 400 },   // Beyond breaking point\n    { duration: '5m', target: 400 },\n    { duration: '10m', target: 0 },    // Recovery\n  ],\n};\n\nexport default function () {\n  const res = http.get('http://localhost:3000/api/users');\n  check(res, { 'status is 200': (r) => r.status === 200 });\n}\n```\n\n### Spike Test\n\n```javascript\n// spike-test.js\nexport const options = {\n  stages: [\n    { duration: '1m', target: 50 },    // Normal load\n    { duration: '10s', target: 500 },  // Spike!\n    { duration: '3m', target: 500 },   // Stay at spike\n    { duration: '10s', target: 50 },   // Scale down\n    { duration: '3m', target: 50 },    // Recovery\n    { duration: '1m', target: 0 },\n  ],\n};\n```\n\n### Soak Test\n\n```javascript\n// soak-test.js\nexport const options = {\n  stages: [\n    { duration: '5m', target: 100 },   // Ramp up\n    { duration: '4h', target: 100 },   // Sustained load for 4 hours\n    { duration: '5m', target: 0 },     // Ramp down\n  ],\n};\n```\n\n### Realistic Scenario\n\n```javascript\n// realistic-test.js\nimport http from 'k6/http';\nimport { check, group, sleep } from 'k6';\nimport { randomItem } from 'https://jslib.k6.io/k6-utils/1.2.0/index.js';\n\nconst BASE_URL = 'http://localhost:3000';\n\nexport const options = {\n  scenarios: {\n    browse: {\n      executor: 'ramping-vus',\n      startVUs: 0,\n      stages: [\n        { duration: '2m', target: 50 },\n        { duration: '5m', target: 50 },\n        { duration: '2m', target: 0 },\n      ],\n      exec: 'browseProducts',\n    },\n    purchase: {\n      executor: 'ramping-vus',\n      startVUs: 0,\n      stages: [\n        { duration: '2m', target: 10 },\n        { duration: '5m', target: 10 },\n        { duration: '2m', target: 0 },\n      ],\n      exec: 'purchaseFlow',\n    },\n  },\n  thresholds: {\n    'http_req_duration{scenario:browse}': ['p(95)<300'],\n    'http_req_duration{scenario:purchase}': ['p(95)<1000'],\n    http_req_failed: ['rate<0.01'],\n  },\n};\n\nexport function browseProducts() {\n  group('Browse Products', () => {\n    const res = http.get(`${BASE_URL}/api/products`);\n    check(res, { 'products loaded': (r) => r.status === 200 });\n    \n    sleep(randomItem([1, 2, 3]));\n    \n    const products = res.json();\n    if (products.length > 0) {\n      const product = randomItem(products);\n      const detailRes = http.get(`${BASE_URL}/api/products/${product.id}`);\n      check(detailRes, { 'product detail loaded': (r) => r.status === 200 });\n    }\n  });\n  \n  sleep(randomItem([2, 3, 4, 5]));\n}\n\nexport function purchaseFlow() {\n  group('Purchase Flow', () => {\n    // Login\n    const loginRes = http.post(`${BASE_URL}/api/auth/login`, JSON.stringify({\n      email: 'test@example.com',\n      password: 'password123',\n    }), { headers: { 'Content-Type': 'application/json' } });\n    \n    check(loginRes, { 'logged in': (r) => r.status === 200 });\n    \n    const token = loginRes.json('token');\n    const authHeaders = {\n      'Authorization': `Bearer ${token}`,\n      'Content-Type': 'application/json',\n    };\n    \n    sleep(1);\n    \n    // Add to cart\n    const cartRes = http.post(`${BASE_URL}/api/cart`, JSON.stringify({\n      productId: 'prod_1',\n      quantity: 1,\n    }), { headers: authHeaders });\n    \n    check(cartRes, { 'added to cart': (r) => r.status === 200 });\n    \n    sleep(2);\n    \n    // Checkout\n    const checkoutRes = http.post(`${BASE_URL}/api/checkout`, JSON.stringify({\n      paymentMethod: 'card',\n    }), { headers: authHeaders });\n    \n    check(checkoutRes, { 'checkout completed': (r) => r.status === 200 });\n  });\n  \n  sleep(randomItem([5, 10, 15]));\n}\n```\n\n### Running k6\n\n```bash\n# Run test\nk6 run test.js\n\n# With more output\nk6 run --out json=results.json test.js\n\n# Cloud execution\nk6 cloud test.js\n\n# With environment variables\nk6 run -e BASE_URL=https://staging.example.com test.js\n```\n\n## Artillery\n\n### Installation\n\n```bash\nnpm install -g artillery\n```\n\n### Basic Test\n\n```yaml\n# basic-test.yml\nconfig:\n  target: \"http://localhost:3000\"\n  phases:\n    - duration: 60\n      arrivalRate: 10\n      name: \"Warm up\"\n    - duration: 300\n      arrivalRate: 50\n      name: \"Sustained load\"\n\nscenarios:\n  - name: \"Get users\"\n    flow:\n      - get:\n          url: \"/api/users\"\n          expect:\n            - statusCode: 200\n            - contentType: application/json\n```\n\n### Running Artillery\n\n```bash\n# Run test\nartillery run test.yml\n\n# With report\nartillery run --output report.json test.yml\nartillery report report.json\n```\n\n## Analyzing Results\n\n### Key Metrics\n\n| Metric | Description | Target |\n|--------|-------------|--------|\n| **Response Time (p50)** | Median response time | <100ms |\n| **Response Time (p95)** | 95th percentile | <500ms |\n| **Response Time (p99)** | 99th percentile | <1s |\n| **Throughput** | Requests per second | Depends |\n| **Error Rate** | % of failed requests | <1% |\n| **Concurrency** | Simultaneous users | Depends |\n\n### k6 Output Analysis\n\n```\n          /\\      |‾‾| /‾‾/   /‾‾/   \n     /\\  /  \\     |  |/  /   /  /    \n    /  \\/    \\    |     (   /   ‾‾\\  \n   /          \\   |  |\\  \\ |  (‾)  | \n  / __________ \\  |__| \\__\\ \\_____/ .io\n\n  execution: local\n     script: test.js\n     output: -\n\n  scenarios: (100.00%) 1 scenario, 100 max VUs, 10m30s max duration\n           * default: Up to 100 looping VUs for 10m0s\n\nrunning (10m00.0s), 000/100 VUs, 54321 complete iterations\ndefault ✓ [======================================] 100 VUs  10m0s\n\n     ✓ status is 200\n     ✓ response time < 500ms\n\n     checks.........................: 100.00% ✓ 108642     ✗ 0     \n     data_received..................: 52 MB   87 kB/s\n     data_sent......................: 6.5 MB  11 kB/s\n     http_req_blocked...............: avg=1.2ms   min=0s      med=0s      max=120ms   p(95)=0s      p(99)=45ms   \n     http_req_connecting............: avg=0.8ms   min=0s      med=0s      max=115ms   p(95)=0s      p(99)=42ms   \n     http_req_duration..............: avg=45.2ms  min=5ms     med=35ms    max=850ms   p(95)=120ms   p(99)=250ms  \n       { expected_response:true }...: avg=45.2ms  min=5ms     med=35ms    max=850ms   p(95)=120ms   p(99)=250ms  \n     http_req_failed................: 0.00%   ✓ 0          ✗ 54321 \n     http_req_receiving.............: avg=0.5ms   min=0s      med=0s      max=50ms    p(95)=1ms     p(99)=5ms    \n     http_req_sending...............: avg=0.1ms   min=0s      med=0s      max=25ms    p(95)=0s      p(99)=1ms    \n     http_req_tls_handshaking.......: avg=0s      min=0s      med=0s      max=0s      p(95)=0s      p(99)=0s     \n     http_req_waiting...............: avg=44.6ms  min=5ms     med=34ms    max=845ms   p(95)=118ms   p(99)=248ms  \n     http_reqs......................: 54321   90.535/s\n     iteration_duration.............: avg=1.05s   min=1.01s   med=1.04s   max=1.85s   p(95)=1.12s   p(99)=1.25s  \n     iterations.....................: 54321   90.535/s\n     vus............................: 100     min=100      max=100 \n     vus_max........................: 100     min=100      max=100 \n```\n\n### Report Template\n\n```markdown\n## Load Test Report\n\n**Date:** 2024-01-15\n**Environment:** Staging\n**Duration:** 10 minutes\n**Scenario:** Ramping to 100 users\n\n### Results Summary\n\n| Metric | Value | Target | Status |\n|--------|-------|--------|--------|\n| Throughput | 90 req/s | 100 req/s | ⚠️ Below target |\n| p50 Response | 35ms | <100ms | ✅ Pass |\n| p95 Response | 120ms | <500ms | ✅ Pass |\n| p99 Response | 250ms | <1s | ✅ Pass |\n| Error Rate | 0% | <1% | ✅ Pass |\n| Max Concurrent | 100 | 100 | ✅ Pass |\n\n### Observations\n\n1. Response times remain stable up to 80 users\n2. At 90+ users, p99 starts increasing\n3. No errors observed\n4. Database CPU reached 70% at peak\n\n### Recommendations\n\n1. Optimize slow database query (identified via APM)\n2. Add caching for /api/products endpoint\n3. Consider horizontal scaling at 80+ concurrent users\n\n### Graphs\n\n[Attach response time over time graph]\n[Attach throughput over time graph]\n[Attach error rate graph]\n```\n\n## CI/CD Integration\n\n```yaml\n# .github/workflows/load-test.yml\nname: Load Test\n\non:\n  schedule:\n    - cron: '0 2 * * *'  # Nightly\n  workflow_dispatch:\n\njobs:\n  load-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Install k6\n        run: |\n          curl https://github.com/grafana/k6/releases/download/v0.47.0/k6-v0.47.0-linux-amd64.tar.gz -L | tar xvz\n          sudo mv k6-v0.47.0-linux-amd64/k6 /usr/local/bin/\n      \n      - name: Run load test\n        run: k6 run --out json=results.json tests/load/test.js\n        env:\n          K6_BASE_URL: ${{ secrets.STAGING_URL }}\n      \n      - name: Upload results\n        uses: actions/upload-artifact@v3\n        with:\n          name: k6-results\n          path: results.json\n      \n      - name: Check thresholds\n        run: |\n          if grep -q '\"thresholds\":{\".*\":{\"ok\":false' results.json; then\n            echo \"Load test thresholds failed!\"\n            exit 1\n          fi\n```\n"
    },
    {
      "name": "optimization-patterns.md",
      "path": "references/optimization-patterns.md",
      "content": "# Optimization Patterns\n\nCommon patterns for improving application performance.\n\n## Caching Patterns\n\n### Cache-Aside (Lazy Loading)\n\n```typescript\nasync function getUser(userId: string): Promise<User> {\n  // 1. Check cache\n  const cached = await cache.get(`user:${userId}`);\n  if (cached) return JSON.parse(cached);\n  \n  // 2. Load from database\n  const user = await db.users.findById(userId);\n  \n  // 3. Store in cache\n  await cache.set(`user:${userId}`, JSON.stringify(user), 'EX', 3600);\n  \n  return user;\n}\n\n// Pros: Only caches what's actually used\n// Cons: Cache miss is slow (DB + cache write)\n```\n\n### Write-Through\n\n```typescript\nasync function updateUser(userId: string, data: Partial<User>): Promise<User> {\n  // 1. Update database\n  const user = await db.users.update(userId, data);\n  \n  // 2. Update cache synchronously\n  await cache.set(`user:${userId}`, JSON.stringify(user), 'EX', 3600);\n  \n  return user;\n}\n\n// Pros: Cache always consistent with DB\n// Cons: Write latency increased\n```\n\n### Write-Behind (Write-Back)\n\n```typescript\nclass WriteBackCache {\n  private dirty = new Map<string, any>();\n  private flushInterval: NodeJS.Timeout;\n  \n  constructor() {\n    // Flush every 5 seconds\n    this.flushInterval = setInterval(() => this.flush(), 5000);\n  }\n  \n  async set(key: string, value: any): Promise<void> {\n    // Write to cache immediately\n    await cache.set(key, JSON.stringify(value));\n    // Mark as dirty for later DB write\n    this.dirty.set(key, value);\n  }\n  \n  private async flush(): Promise<void> {\n    const entries = [...this.dirty.entries()];\n    this.dirty.clear();\n    \n    // Batch write to database\n    await db.batchUpdate(entries);\n  }\n}\n\n// Pros: Fast writes\n// Cons: Data loss risk if crash before flush\n```\n\n### Cache Invalidation Strategies\n\n```typescript\n// 1. Time-based (TTL)\nawait cache.set('key', 'value', 'EX', 3600);  // Expire in 1 hour\n\n// 2. Event-based\nasync function updateProduct(id: string, data: any) {\n  await db.products.update(id, data);\n  await cache.del(`product:${id}`);           // Invalidate specific\n  await cache.del(`products:list`);           // Invalidate list\n}\n\n// 3. Version-based\nasync function getProducts(): Promise<Product[]> {\n  const version = await cache.get('products:version');\n  const cached = await cache.get(`products:list:${version}`);\n  if (cached) return JSON.parse(cached);\n  \n  const products = await db.products.findAll();\n  await cache.set(`products:list:${version}`, JSON.stringify(products));\n  return products;\n}\n\nasync function invalidateProducts() {\n  await cache.incr('products:version');  // Bump version\n}\n\n// 4. Tag-based invalidation\nasync function cacheWithTags(key: string, value: any, tags: string[]) {\n  await cache.set(key, JSON.stringify(value));\n  for (const tag of tags) {\n    await cache.sadd(`tag:${tag}`, key);\n  }\n}\n\nasync function invalidateTag(tag: string) {\n  const keys = await cache.smembers(`tag:${tag}`);\n  await cache.del(...keys, `tag:${tag}`);\n}\n```\n\n### Cache Stampede Prevention\n\n```typescript\n// Problem: Cache expires, 100 requests all hit DB simultaneously\n\n// Solution 1: Locking\nasync function getWithLock(key: string, fetchFn: () => Promise<any>) {\n  const cached = await cache.get(key);\n  if (cached) return JSON.parse(cached);\n  \n  const lockKey = `lock:${key}`;\n  const acquired = await cache.set(lockKey, '1', 'NX', 'EX', 10);\n  \n  if (!acquired) {\n    // Wait and retry\n    await sleep(100);\n    return getWithLock(key, fetchFn);\n  }\n  \n  try {\n    const value = await fetchFn();\n    await cache.set(key, JSON.stringify(value), 'EX', 3600);\n    return value;\n  } finally {\n    await cache.del(lockKey);\n  }\n}\n\n// Solution 2: Probabilistic early expiration\nasync function getWithJitter(key: string, fetchFn: () => Promise<any>) {\n  const data = await cache.get(key);\n  if (!data) {\n    return refreshAndCache(key, fetchFn);\n  }\n  \n  const { value, expiresAt } = JSON.parse(data);\n  const ttl = expiresAt - Date.now();\n  \n  // Probabilistically refresh before expiry\n  const refreshProbability = Math.exp(-ttl / 60000);  // Higher as TTL decreases\n  if (Math.random() < refreshProbability) {\n    refreshAndCache(key, fetchFn);  // Fire and forget\n  }\n  \n  return value;\n}\n```\n\n## Async and Parallel Patterns\n\n### Parallel Execution\n\n```typescript\n// BAD: Sequential\nconst user = await getUser(id);\nconst orders = await getOrders(id);\nconst preferences = await getPreferences(id);\n\n// GOOD: Parallel\nconst [user, orders, preferences] = await Promise.all([\n  getUser(id),\n  getOrders(id),\n  getPreferences(id),\n]);\n\n// With error handling\nconst results = await Promise.allSettled([\n  getUser(id),\n  getOrders(id),\n  getPreferences(id),\n]);\n\nconst [userResult, ordersResult, prefsResult] = results;\nconst user = userResult.status === 'fulfilled' ? userResult.value : null;\n```\n\n### Concurrency Limiting\n\n```typescript\nimport pLimit from 'p-limit';\n\n// Limit to 5 concurrent operations\nconst limit = pLimit(5);\n\nconst urls = ['url1', 'url2', /* ... 100 more */];\n\n// Without limit: 100 concurrent requests\n// const results = await Promise.all(urls.map(fetch));\n\n// With limit: max 5 concurrent\nconst results = await Promise.all(\n  urls.map(url => limit(() => fetch(url)))\n);\n```\n\n### Batching\n\n```typescript\nimport DataLoader from 'dataloader';\n\n// Create batch loader\nconst userLoader = new DataLoader(async (ids: string[]) => {\n  // Single query for all IDs\n  const users = await db.users.findMany({ where: { id: { in: ids } } });\n  \n  // Return in same order as input\n  const userMap = new Map(users.map(u => [u.id, u]));\n  return ids.map(id => userMap.get(id) ?? null);\n});\n\n// Usage: automatically batched\nasync function getOrderWithUser(orderId: string) {\n  const order = await db.orders.findById(orderId);\n  const user = await userLoader.load(order.userId);  // Batched!\n  return { ...order, user };\n}\n\n// Multiple calls in same tick are batched\nconst [user1, user2, user3] = await Promise.all([\n  userLoader.load('1'),\n  userLoader.load('2'),\n  userLoader.load('3'),\n]);\n// Single DB query: SELECT * FROM users WHERE id IN ('1', '2', '3')\n```\n\n### Background Processing\n\n```typescript\n// Move slow operations out of request path\nimport { Queue, Worker } from 'bullmq';\n\nconst emailQueue = new Queue('email');\n\n// In request handler: quick enqueue\napp.post('/orders', async (req, res) => {\n  const order = await createOrder(req.body);\n  \n  // Don't wait for email\n  await emailQueue.add('order-confirmation', { orderId: order.id });\n  \n  res.json(order);  // Response in <100ms\n});\n\n// Worker processes in background\nconst worker = new Worker('email', async (job) => {\n  const order = await getOrder(job.data.orderId);\n  await sendEmail(order.user.email, 'Order Confirmed', ...);\n});\n```\n\n## Algorithm Optimizations\n\n### Memoization\n\n```typescript\n// Simple memoization\nfunction memoize<T extends (...args: any[]) => any>(fn: T): T {\n  const cache = new Map();\n  \n  return ((...args: any[]) => {\n    const key = JSON.stringify(args);\n    if (cache.has(key)) return cache.get(key);\n    \n    const result = fn(...args);\n    cache.set(key, result);\n    return result;\n  }) as T;\n}\n\nconst expensiveCalculation = memoize((n: number) => {\n  // Expensive computation\n  return fibonacci(n);\n});\n\n// With size limit\nimport { LRUCache } from 'lru-cache';\n\nfunction memoizeWithLimit<T extends (...args: any[]) => any>(fn: T, max = 1000): T {\n  const cache = new LRUCache<string, any>({ max });\n  \n  return ((...args: any[]) => {\n    const key = JSON.stringify(args);\n    const cached = cache.get(key);\n    if (cached !== undefined) return cached;\n    \n    const result = fn(...args);\n    cache.set(key, result);\n    return result;\n  }) as T;\n}\n```\n\n### Lazy Evaluation\n\n```typescript\n// Eager: computes everything\nfunction getFullReport(data: Data[]) {\n  const processed = data.map(expensiveProcess);      // All items\n  const filtered = processed.filter(isRelevant);     // All items again\n  return filtered.slice(0, 10);                      // Only need 10!\n}\n\n// Lazy: computes only what's needed\nfunction* lazyProcess(data: Data[]) {\n  for (const item of data) {\n    const processed = expensiveProcess(item);\n    if (isRelevant(processed)) {\n      yield processed;\n    }\n  }\n}\n\nfunction getFullReport(data: Data[]) {\n  const results = [];\n  for (const item of lazyProcess(data)) {\n    results.push(item);\n    if (results.length >= 10) break;  // Stop early!\n  }\n  return results;\n}\n```\n\n### Data Structure Selection\n\n| Operation | Array | Set | Map | Object |\n|-----------|-------|-----|-----|--------|\n| Access by index | O(1) | - | - | - |\n| Access by key | O(n) | - | O(1) | O(1) |\n| Search | O(n) | O(1) | O(1) | O(1) |\n| Insert | O(n)* | O(1) | O(1) | O(1) |\n| Delete | O(n) | O(1) | O(1) | O(1) |\n\n```typescript\n// BAD: Array for lookups\nconst users = [{ id: '1', name: 'Alice' }, { id: '2', name: 'Bob' }];\nconst user = users.find(u => u.id === targetId);  // O(n)\n\n// GOOD: Map for lookups\nconst userMap = new Map(users.map(u => [u.id, u]));\nconst user = userMap.get(targetId);  // O(1)\n\n// BAD: Array for uniqueness\nconst uniqueIds: string[] = [];\nfor (const id of ids) {\n  if (!uniqueIds.includes(id)) {  // O(n) each check\n    uniqueIds.push(id);\n  }\n}\n\n// GOOD: Set for uniqueness\nconst uniqueIds = new Set(ids);  // O(n) total\n```\n\n### Early Termination\n\n```typescript\n// BAD: Always processes all items\nfunction hasAdmin(users: User[]): boolean {\n  return users.filter(u => u.role === 'admin').length > 0;\n}\n\n// GOOD: Stops at first match\nfunction hasAdmin(users: User[]): boolean {\n  return users.some(u => u.role === 'admin');\n}\n\n// BAD: Sorts entire array for top N\nfunction getTop10(items: Item[]): Item[] {\n  return items.sort((a, b) => b.score - a.score).slice(0, 10);\n}\n\n// GOOD: Partial sort / heap\nfunction getTop10(items: Item[]): Item[] {\n  const heap = new MinHeap<Item>((a, b) => a.score - b.score);\n  \n  for (const item of items) {\n    heap.push(item);\n    if (heap.size > 10) heap.pop();  // Remove smallest\n  }\n  \n  return heap.toArray().reverse();\n}\n```\n\n## Memory Optimization\n\n### Object Pooling\n\n```typescript\nclass ObjectPool<T> {\n  private pool: T[] = [];\n  private create: () => T;\n  private reset: (obj: T) => void;\n  \n  constructor(create: () => T, reset: (obj: T) => void, initialSize = 10) {\n    this.create = create;\n    this.reset = reset;\n    \n    for (let i = 0; i < initialSize; i++) {\n      this.pool.push(create());\n    }\n  }\n  \n  acquire(): T {\n    return this.pool.pop() ?? this.create();\n  }\n  \n  release(obj: T): void {\n    this.reset(obj);\n    this.pool.push(obj);\n  }\n}\n\n// Usage for expensive objects\nconst bufferPool = new ObjectPool(\n  () => Buffer.allocUnsafe(1024),\n  (buf) => buf.fill(0)\n);\n\nconst buffer = bufferPool.acquire();\n// Use buffer...\nbufferPool.release(buffer);\n```\n\n### Streaming\n\n```typescript\n// BAD: Load entire file into memory\nconst content = await fs.readFile('large-file.json', 'utf8');\nconst data = JSON.parse(content);\nfor (const item of data) {\n  process(item);\n}\n\n// GOOD: Stream processing\nimport { createReadStream } from 'fs';\nimport { parser } from 'stream-json';\nimport { streamArray } from 'stream-json/streamers/StreamArray';\n\nconst pipeline = createReadStream('large-file.json')\n  .pipe(parser())\n  .pipe(streamArray());\n\npipeline.on('data', ({ value }) => {\n  process(value);\n});\n\n// For CSV\nimport csv from 'csv-parser';\n\ncreateReadStream('large-file.csv')\n  .pipe(csv())\n  .on('data', (row) => process(row));\n```\n\n### String Optimization\n\n```typescript\n// BAD: String concatenation in loop\nlet result = '';\nfor (const item of items) {\n  result += item.toString() + ',';  // Creates new string each iteration\n}\n\n// GOOD: Array join\nconst result = items.map(item => item.toString()).join(',');\n\n// GOOD: StringBuilder pattern for many operations\nconst parts: string[] = [];\nfor (const item of items) {\n  parts.push(item.toString());\n}\nconst result = parts.join(',');\n```\n\n## Network Optimization\n\n### Request Coalescing\n\n```typescript\nclass RequestCoalescer<T> {\n  private pending = new Map<string, Promise<T>>();\n  \n  async fetch(key: string, fetcher: () => Promise<T>): Promise<T> {\n    // If request already in flight, return same promise\n    const existing = this.pending.get(key);\n    if (existing) return existing;\n    \n    const promise = fetcher().finally(() => {\n      this.pending.delete(key);\n    });\n    \n    this.pending.set(key, promise);\n    return promise;\n  }\n}\n\nconst coalescer = new RequestCoalescer<User>();\n\n// 100 concurrent calls for same user = 1 actual request\nawait Promise.all(\n  Array(100).fill(null).map(() => \n    coalescer.fetch('user:123', () => fetchUser('123'))\n  )\n);\n```\n\n### Connection Reuse\n\n```typescript\n// HTTP Keep-Alive\nimport { Agent } from 'http';\n\nconst agent = new Agent({\n  keepAlive: true,\n  maxSockets: 50,\n  maxFreeSockets: 10,\n  timeout: 60000,\n});\n\nfetch(url, { agent });\n\n// Database connection pooling (covered in database-optimization.md)\n```\n\n### Compression\n\n```typescript\nimport compression from 'compression';\n\n// Enable gzip compression\napp.use(compression({\n  filter: (req, res) => {\n    if (req.headers['x-no-compression']) return false;\n    return compression.filter(req, res);\n  },\n  level: 6,  // 1-9, higher = more compression, more CPU\n  threshold: 1024,  // Don't compress below 1KB\n}));\n```\n"
    },
    {
      "name": "profiling-tools.md",
      "path": "references/profiling-tools.md",
      "content": "# Profiling Tools\n\nTools and techniques for identifying performance bottlenecks.\n\n## Node.js Profiling\n\n### Built-in Profiler\n\n```bash\n# Generate V8 profile\nnode --prof app.js\n\n# Process the profile\nnode --prof-process isolate-*.log > profile.txt\n\n# Look for hot functions in output:\n# [JavaScript]:\n#   ticks  total  nonlib   name\n#   1234   45.0%   50.0%  processOrder\n```\n\n### Chrome DevTools (Node Inspector)\n\n```bash\n# Start with inspector\nnode --inspect app.js\n\n# Or break on start\nnode --inspect-brk app.js\n\n# Open chrome://inspect in Chrome\n# Go to \"Performance\" tab to record\n```\n\n### Clinic.js Suite\n\n```bash\n# Install\nnpm install -g clinic\n\n# Doctor: Overall health check\nclinic doctor -- node app.js\n\n# Flame: CPU profiling (flame graphs)\nclinic flame -- node app.js\n\n# Bubbleprof: Async profiling\nclinic bubbleprof -- node app.js\n\n# HeapProfiler: Memory profiling\nclinic heapprofiler -- node app.js\n```\n\n### Console Timing\n\n```typescript\n// Simple timing\nconsole.time('operation');\nawait doSomething();\nconsole.timeEnd('operation');\n// operation: 123.456ms\n\n// Nested timing\nconsole.time('total');\nconsole.time('step1');\nawait step1();\nconsole.timeEnd('step1');\nconsole.time('step2');\nawait step2();\nconsole.timeEnd('step2');\nconsole.timeEnd('total');\n```\n\n### Performance Hooks (Node.js)\n\n```typescript\nimport { performance, PerformanceObserver } from 'perf_hooks';\n\n// Observe performance entries\nconst obs = new PerformanceObserver((list) => {\n  for (const entry of list.getEntries()) {\n    console.log(`${entry.name}: ${entry.duration}ms`);\n  }\n});\nobs.observe({ entryTypes: ['measure'] });\n\n// Mark and measure\nperformance.mark('start');\nawait doSomething();\nperformance.mark('end');\nperformance.measure('doSomething', 'start', 'end');\n\n// Function timing wrapper\nfunction timedAsync<T>(name: string, fn: () => Promise<T>): Promise<T> {\n  const start = performance.now();\n  return fn().finally(() => {\n    const duration = performance.now() - start;\n    console.log(`${name}: ${duration.toFixed(2)}ms`);\n  });\n}\n```\n\n## Memory Profiling\n\n### Heap Snapshots\n\n```typescript\nimport v8 from 'v8';\nimport fs from 'fs';\n\n// Take heap snapshot\nfunction takeHeapSnapshot(filename: string) {\n  const snapshotStream = v8.writeHeapSnapshot(filename);\n  console.log(`Heap snapshot written to ${snapshotStream}`);\n}\n\n// Compare before/after\ntakeHeapSnapshot('before.heapsnapshot');\n// ... do operations ...\ntakeHeapSnapshot('after.heapsnapshot');\n\n// Open in Chrome DevTools Memory tab\n```\n\n### Memory Usage Tracking\n\n```typescript\n// Track memory over time\nfunction logMemoryUsage() {\n  const used = process.memoryUsage();\n  console.log({\n    rss: `${Math.round(used.rss / 1024 / 1024)}MB`,        // Resident Set Size\n    heapTotal: `${Math.round(used.heapTotal / 1024 / 1024)}MB`,\n    heapUsed: `${Math.round(used.heapUsed / 1024 / 1024)}MB`,\n    external: `${Math.round(used.external / 1024 / 1024)}MB`,\n  });\n}\n\n// Periodic logging\nsetInterval(logMemoryUsage, 10000);\n\n// Force garbage collection (requires --expose-gc flag)\nif (global.gc) {\n  global.gc();\n  logMemoryUsage();\n}\n```\n\n### Finding Memory Leaks\n\n```typescript\n// Pattern: Growing arrays/maps\nclass LeakyCache {\n  private cache = new Map();  // Never cleared!\n  \n  set(key: string, value: any) {\n    this.cache.set(key, value);\n  }\n}\n\n// Fix: Bounded cache with LRU eviction\nimport LRU from 'lru-cache';\n\nclass BoundedCache {\n  private cache = new LRU({ max: 1000 });\n  \n  set(key: string, value: any) {\n    this.cache.set(key, value);\n  }\n}\n\n// Pattern: Event listener leaks\nclass LeakyComponent {\n  constructor() {\n    // Listener never removed!\n    window.addEventListener('resize', this.onResize);\n  }\n}\n\n// Fix: Clean up listeners\nclass ProperComponent {\n  constructor() {\n    window.addEventListener('resize', this.onResize);\n  }\n  \n  destroy() {\n    window.removeEventListener('resize', this.onResize);\n  }\n}\n```\n\n## Database Profiling\n\n### PostgreSQL\n\n```sql\n-- Enable query logging\nALTER SYSTEM SET log_min_duration_statement = 100;  -- Log queries > 100ms\nSELECT pg_reload_conf();\n\n-- Enable pg_stat_statements\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- Find slowest queries\nSELECT \n  substring(query, 1, 100) as query,\n  calls,\n  round(total_exec_time::numeric, 2) as total_ms,\n  round(mean_exec_time::numeric, 2) as mean_ms,\n  round((100 * total_exec_time / sum(total_exec_time) OVER ())::numeric, 2) as percent\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 20;\n\n-- Find queries with most calls\nSELECT \n  substring(query, 1, 100) as query,\n  calls,\n  round(mean_exec_time::numeric, 2) as mean_ms\nFROM pg_stat_statements\nORDER BY calls DESC\nLIMIT 20;\n\n-- Reset statistics\nSELECT pg_stat_statements_reset();\n```\n\n### Query Execution Plans\n\n```sql\n-- Basic explain\nEXPLAIN SELECT * FROM orders WHERE user_id = 123;\n\n-- With actual execution stats\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT * FROM orders WHERE user_id = 123;\n\n-- What to look for:\n-- Seq Scan: Table scan (might need index)\n-- Nested Loop: Potentially slow for large datasets\n-- Sort: Memory-intensive operation\n-- Hash Join: Usually efficient\n-- Index Scan: Good, using index\n\n-- Example output analysis:\n/*\nSeq Scan on orders  (cost=0.00..1234.00 rows=100 width=50)\n                     (actual time=0.015..45.123 rows=95 loops=1)\n  Filter: (user_id = 123)\n  Rows Removed by Filter: 9905\n  Buffers: shared hit=500 read=100\nPlanning Time: 0.150 ms\nExecution Time: 45.500 ms\n\nProblem: Seq Scan reading 10,000 rows to find 95\nSolution: CREATE INDEX idx_orders_user_id ON orders(user_id);\n*/\n```\n\n### MongoDB\n\n```javascript\n// Enable profiler\ndb.setProfilingLevel(2);  // Log all queries\n\n// Find slow queries\ndb.system.profile.find({ millis: { $gt: 100 } }).sort({ ts: -1 }).limit(10);\n\n// Explain query\ndb.orders.find({ userId: \"123\" }).explain(\"executionStats\");\n\n// Index usage stats\ndb.orders.aggregate([{ $indexStats: {} }]);\n\n// Disable profiler\ndb.setProfilingLevel(0);\n```\n\n## Application Performance Monitoring (APM)\n\n### OpenTelemetry Setup\n\n```typescript\n// tracing.ts\nimport { NodeSDK } from '@opentelemetry/sdk-node';\nimport { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';\nimport { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';\n\nconst sdk = new NodeSDK({\n  traceExporter: new OTLPTraceExporter({\n    url: 'http://jaeger:4318/v1/traces',\n  }),\n  instrumentations: [getNodeAutoInstrumentations()],\n});\n\nsdk.start();\n\n// Custom spans\nimport { trace } from '@opentelemetry/api';\n\nconst tracer = trace.getTracer('my-service');\n\nasync function processOrder(orderId: string) {\n  return tracer.startActiveSpan('processOrder', async (span) => {\n    span.setAttribute('orderId', orderId);\n    \n    try {\n      await tracer.startActiveSpan('validateOrder', async (validateSpan) => {\n        await validateOrder(orderId);\n        validateSpan.end();\n      });\n      \n      await tracer.startActiveSpan('chargePayment', async (paymentSpan) => {\n        await chargePayment(orderId);\n        paymentSpan.end();\n      });\n      \n      span.setStatus({ code: SpanStatusCode.OK });\n    } catch (error) {\n      span.setStatus({ code: SpanStatusCode.ERROR, message: error.message });\n      throw error;\n    } finally {\n      span.end();\n    }\n  });\n}\n```\n\n### Custom Metrics\n\n```typescript\nimport { metrics } from '@opentelemetry/api';\n\nconst meter = metrics.getMeter('my-service');\n\n// Counter\nconst requestCounter = meter.createCounter('http_requests_total', {\n  description: 'Total HTTP requests',\n});\n\n// Histogram\nconst requestDuration = meter.createHistogram('http_request_duration_ms', {\n  description: 'HTTP request duration in milliseconds',\n});\n\n// Usage in middleware\napp.use((req, res, next) => {\n  const start = Date.now();\n  \n  res.on('finish', () => {\n    const duration = Date.now() - start;\n    \n    requestCounter.add(1, {\n      method: req.method,\n      route: req.route?.path || 'unknown',\n      status: res.statusCode,\n    });\n    \n    requestDuration.record(duration, {\n      method: req.method,\n      route: req.route?.path || 'unknown',\n    });\n  });\n  \n  next();\n});\n```\n\n## Browser Profiling\n\n### Chrome DevTools Performance\n\n```markdown\n## Recording a Performance Profile\n\n1. Open DevTools (F12)\n2. Go to Performance tab\n3. Click Record (or Ctrl+E)\n4. Perform the action you want to profile\n5. Click Stop\n\n## What to Look For\n\n### Main Thread Activity\n- Yellow: JavaScript execution\n- Purple: Rendering (style, layout, paint)\n- Green: Painting\n- Gray: Idle\n\n### Long Tasks\n- Any task > 50ms blocks the main thread\n- Look for red triangles indicating long tasks\n\n### Common Issues\n- Forced synchronous layout (layout thrashing)\n- Large JavaScript execution blocks\n- Excessive paint/composite operations\n```\n\n### Lighthouse\n\n```bash\n# CLI\nnpm install -g lighthouse\nlighthouse https://example.com --output html --output-path report.html\n\n# Programmatic\nimport lighthouse from 'lighthouse';\nimport * as chromeLauncher from 'chrome-launcher';\n\nasync function runLighthouse(url: string) {\n  const chrome = await chromeLauncher.launch({ chromeFlags: ['--headless'] });\n  \n  const result = await lighthouse(url, {\n    port: chrome.port,\n    output: 'json',\n  });\n  \n  await chrome.kill();\n  return result;\n}\n```\n\n### Web Vitals Monitoring\n\n```typescript\nimport { getCLS, getFID, getFCP, getLCP, getTTFB } from 'web-vitals';\n\nfunction sendToAnalytics(metric: Metric) {\n  const body = JSON.stringify({\n    name: metric.name,\n    value: metric.value,\n    id: metric.id,\n  });\n  \n  // Use sendBeacon for reliability\n  navigator.sendBeacon('/analytics', body);\n}\n\ngetCLS(sendToAnalytics);\ngetFID(sendToAnalytics);\ngetFCP(sendToAnalytics);\ngetLCP(sendToAnalytics);\ngetTTFB(sendToAnalytics);\n```\n\n## Flame Graphs\n\n### Reading Flame Graphs\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    main()                               │ ← Entry point (bottom)\n├───────────────────────┬─────────────────────────────────┤\n│     processOrder()    │        sendEmail()              │ ← Called by main\n├───────────┬───────────┼─────────────────────────────────┤\n│validateOrder│calculate│                                 │ ← Called by processOrder\n├───────────┴───────────┤                                 │\n│     dbQuery()         │                                 │ ← Hot spot (wide = slow)\n└───────────────────────┴─────────────────────────────────┘\n\nWidth = Time spent\nHeight = Call depth\nLook for wide boxes = bottlenecks\n```\n\n### Generating Flame Graphs\n\n```bash\n# Node.js with 0x\nnpm install -g 0x\n0x app.js\n# Opens flame graph in browser\n\n# Node.js with clinic flame\nclinic flame -- node app.js\n\n# Linux perf + FlameGraph\nperf record -g node app.js\nperf script | stackcollapse-perf.pl | flamegraph.pl > flame.svg\n```\n"
    }
  ],
  "tags": [
    "performance",
    "validation",
    "optimization",
    "profiling",
    "core-workflow"
  ],
  "dependsOn": [
    "implement"
  ]
}