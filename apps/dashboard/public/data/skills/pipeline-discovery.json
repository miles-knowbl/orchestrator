{
  "id": "pipeline-discovery",
  "name": "pipeline-discovery",
  "version": "1.0.0",
  "description": "Identify backend data pipelines (P-series) in the codebase. Discovers server-side data flows triggered by user actions or system events, documenting triggers, steps, and outcomes. Foundation for MECE failure mode analysis.",
  "phase": "INIT",
  "category": "core",
  "content": "# Pipeline Discovery\n\nIdentify backend data pipelines (P-series).\n\n## When to Use\n\n- **Starting an audit** — Runs in INIT phase to map backend flows\n- **Understanding data flows** — Document how data moves through the system\n- **Preparing for failure mode analysis** — Identify what can break\n- When you say: \"find the pipelines\", \"map the backend\", \"what data flows exist?\"\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `pipeline-identification.md` | How to find pipelines in code |\n| `pipeline-template.md` | How to document each pipeline |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `common-patterns.md` | Recognize typical pipeline patterns |\n\n**Verification:** All major backend data flows are documented with triggers and outcomes.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| Pipeline inventory | `AUDIT-SCOPE.md` | Always (P-series section) |\n| State update | `audit-state.json` | Always (backend_pipelines array) |\n\n## Core Concept\n\nPipeline Discovery answers: **\"What are the major backend data flows?\"**\n\nA pipeline is:\n- **Triggered** by user action or system event\n- **Processes** data through multiple steps\n- **Produces** a persistent outcome\n\nExamples:\n- P1: Source Ingestion (file upload → parsed schema)\n- P2: Content Generation (generate button → artifact created)\n- P3: Publishing (publish button → post live on platform)\n\n## Pipeline Identification\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│               PIPELINE DISCOVERY PROCESS                    │\n│                                                             │\n│  1. FIND ENTRY POINTS                                       │\n│     ├─→ API routes (POST, PUT, DELETE)                     │\n│     ├─→ Background job handlers                            │\n│     ├─→ Webhook receivers                                  │\n│     └─→ Event listeners                                    │\n│                                                             │\n│  2. TRACE DATA FLOW                                         │\n│     ├─→ Input validation/parsing                           │\n│     ├─→ Business logic                                     │\n│     ├─→ External service calls                             │\n│     └─→ Database writes                                    │\n│                                                             │\n│  3. DOCUMENT EACH PIPELINE                                  │\n│     ├─→ Trigger (what starts it)                           │\n│     ├─→ Steps (what happens)                               │\n│     └─→ Outcome (what it produces)                         │\n│                                                             │\n│  4. ASSIGN P-SERIES IDS                                     │\n│     └─→ P1, P2, P3... in order of discovery                │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Where to Look\n\n### API Routes\n```typescript\n// Next.js API routes\napp/api/*/route.ts\n\n// Express routes\nroutes/*.ts\ncontrollers/*.ts\n```\n\n### Background Jobs\n```typescript\n// Job processors\njobs/*.ts\nworkers/*.ts\nqueues/*.ts\n```\n\n### Database Operations\n```typescript\n// Database writes indicate pipeline endpoints\nsupabase.from('table').insert()\nprisma.model.create()\n```\n\n### External Services\n```typescript\n// External API calls often indicate pipelines\nopenai.chat.completions.create()\ntwitter.post()\n```\n\n## Pipeline Documentation Format\n\n```markdown\n### P1: Source Ingestion\n\n**Trigger:** User uploads file via /sources/upload\n**Frequency:** ~50/day\n\n**Steps:**\n1. File received at `api/sources/upload/route.ts:23`\n2. File type validated (`lib/validators.ts:45`)\n3. Content parsed by type (`lib/parsers/index.ts:12`)\n4. Schema extracted (`lib/schema-extractor.ts:78`)\n5. Source record created (`lib/db/sources.ts:34`)\n6. Embedding generated (`lib/embeddings.ts:56`)\n\n**Outcome:**\n- `sources` table: new row with metadata\n- `source_embeddings` table: vector for search\n- `source_schema` JSON: extracted structure\n\n**Key Files:**\n- `api/sources/upload/route.ts`\n- `lib/parsers/*.ts`\n- `lib/schema-extractor.ts`\n```\n\n## Output Format\n\n### In AUDIT-SCOPE.md\n\n```markdown\n## Backend Pipelines (P-series)\n\n| ID | Name | Trigger | Outcome |\n|----|------|---------|---------|\n| P1 | Source Ingestion | File upload | source_schema populated |\n| P2 | Content Generation | Generate button | Artifact created |\n| P3 | Publishing | Publish button | Post live on platform |\n\n### P1: Source Ingestion\n[detailed documentation]\n\n### P2: Content Generation\n[detailed documentation]\n```\n\n### In audit-state.json\n\n```json\n{\n  \"backend_pipelines\": [\n    {\n      \"id\": \"P1\",\n      \"name\": \"Source Ingestion\",\n      \"trigger\": \"File upload via /sources/upload\",\n      \"outcome\": \"source_schema populated\",\n      \"key_files\": [\"api/sources/upload/route.ts\", \"lib/parsers/index.ts\"],\n      \"step_count\": 6\n    }\n  ]\n}\n```\n\n## Discovery Checklist\n\n- [ ] All POST/PUT/DELETE API routes examined\n- [ ] Background job handlers identified\n- [ ] Database write operations traced\n- [ ] External API calls documented\n- [ ] Each pipeline has trigger, steps, outcome\n- [ ] P-series IDs assigned consistently\n\n## Common Pipeline Patterns\n\n| Pattern | Example | Indicators |\n|---------|---------|------------|\n| CRUD Create | User registration | POST route → validate → insert |\n| File Processing | Document upload | POST multipart → parse → store |\n| Generation | AI content | POST → LLM call → store result |\n| Publishing | Social post | POST → external API → update status |\n| Batch Job | Daily report | Cron → query → aggregate → email |\n\n## Validation\n\nBefore completing, verify:\n\n- [ ] All major data flows are documented\n- [ ] Each pipeline has a unique P-series ID\n- [ ] Triggers are user-observable actions or system events\n- [ ] Outcomes are persistent (database writes, external effects)\n- [ ] Key files are identified for each pipeline\n- [ ] Step counts are accurate",
  "references": [
    {
      "name": "common-patterns.md",
      "path": "references/common-patterns.md",
      "content": "# Common Pipeline Patterns\n\nTypical backend pipeline patterns to recognize.\n\n## CRUD Patterns\n\n### Create Pattern\n```\nPOST request → Validate input → Create record → Return ID\n\nExample: User registration\nPOST /users → validate email/password → insert user → return user_id\n```\n\n### Update Pattern\n```\nPUT/PATCH request → Load existing → Validate changes → Update record\n\nExample: Profile update\nPATCH /users/:id → load user → validate fields → update user\n```\n\n### Delete Pattern\n```\nDELETE request → Load existing → Check permissions → Delete/soft-delete\n\nExample: Source deletion\nDELETE /sources/:id → load source → check owner → soft delete\n```\n\n## Processing Patterns\n\n### File Ingestion\n```\nUpload → Validate type → Parse content → Extract metadata → Store\n\nExample: Document upload\nPOST /upload → check mime type → parse PDF → extract text → store in sources\n```\n\n### AI Generation\n```\nRequest → Gather context → Build prompt → Call LLM → Parse response → Store\n\nExample: Content generation\nPOST /generate → get sources → build prompt → call GPT → parse → store artifact\n```\n\n### Transformation\n```\nInput → Load data → Transform → Validate output → Store\n\nExample: Format conversion\nPOST /convert → load source → convert format → validate → store result\n```\n\n## Integration Patterns\n\n### Publish to External\n```\nRequest → Load content → Format for platform → Call external API → Update status\n\nExample: Twitter publish\nPOST /publish → load artifact → format tweet → call Twitter API → mark published\n```\n\n### Webhook Receiver\n```\nWebhook → Validate signature → Parse payload → Update internal state\n\nExample: Payment webhook\nPOST /webhooks/stripe → verify signature → parse event → update subscription\n```\n\n### Sync Pattern\n```\nTrigger → Fetch external data → Compare with local → Update local state\n\nExample: Calendar sync\nPOST /sync/calendar → fetch Google events → diff with local → update events\n```\n\n## Batch Patterns\n\n### Scheduled Job\n```\nCron trigger → Query data → Process batch → Update records\n\nExample: Daily digest\nCron 9am → query unread items → generate digest → send email → mark sent\n```\n\n### Queue Worker\n```\nJob picked → Load job data → Process → Update job status\n\nExample: Video encoding\nPick job → load video → encode formats → upload to CDN → mark complete\n```\n\n## Pattern Indicators\n\n| Pattern | Code Indicators |\n|---------|----------------|\n| CRUD | REST routes, model operations |\n| File Ingestion | Multipart parsing, file type checks |\n| AI Generation | LLM client calls, prompt building |\n| External Publish | OAuth tokens, API clients |\n| Webhook | Signature verification, event parsing |\n| Batch | Cron expressions, queue.process() |\n\n## Anti-Patterns (Not Pipelines)\n\n| Pattern | Why Not a Pipeline |\n|---------|-------------------|\n| Health check | No state change |\n| Auth middleware | Shared, not a flow |\n| Utility functions | No trigger/outcome |\n| Event logging | Side effect only |\n"
    },
    {
      "name": "pipeline-identification.md",
      "path": "references/pipeline-identification.md",
      "content": "# Pipeline Identification\n\nHow to find backend pipelines in a codebase.\n\n## What Qualifies as a Pipeline\n\nA backend pipeline must have:\n\n1. **Trigger** — Something that starts it (user action, system event)\n2. **Processing** — Multiple steps that transform data\n3. **Outcome** — Persistent result (database write, external effect)\n\n## Discovery Strategy\n\n### 1. Start with Entry Points\n\nLook for:\n```\napp/api/*/route.ts          # Next.js App Router\npages/api/*.ts              # Next.js Pages Router\nroutes/*.ts                 # Express\ncontrollers/*.ts            # MVC patterns\nfunctions/*.ts              # Serverless\n```\n\n### 2. Filter to State-Changing Operations\n\nFocus on:\n- POST requests (create)\n- PUT/PATCH requests (update)\n- DELETE requests (remove)\n\nSkip:\n- GET requests (read-only, not pipelines)\n- OPTIONS/HEAD (metadata)\n\n### 3. Trace the Data Flow\n\nFrom entry point, follow:\n```\nRequest → Validation → Business Logic → Database → Response\n```\n\nDocument each step with file:line references.\n\n### 4. Identify External Effects\n\nPipelines often involve:\n- Database writes (Supabase, Prisma, etc.)\n- External API calls (OpenAI, Twitter, etc.)\n- File storage (S3, local fs)\n- Queue operations (add job, publish event)\n\n## Code Patterns to Search\n\n### Database Writes\n```typescript\n// Supabase\nsupabase.from('table').insert()\nsupabase.from('table').update()\nsupabase.from('table').upsert()\n\n// Prisma\nprisma.model.create()\nprisma.model.update()\n\n// Raw SQL\ndb.execute('INSERT INTO...')\n```\n\n### External API Calls\n```typescript\n// OpenAI\nopenai.chat.completions.create()\n\n// HTTP clients\nfetch('https://api.external.com', { method: 'POST' })\naxios.post()\n```\n\n### Job Queues\n```typescript\n// Bull/BullMQ\nqueue.add('jobName', data)\n\n// Custom\njobRunner.enqueue()\n```\n\n## Non-Pipeline Patterns\n\nDon't document these as pipelines:\n\n| Pattern | Why Not |\n|---------|---------|\n| GET requests | Read-only, no state change |\n| Pure functions | No persistent outcome |\n| Middleware | Shared logic, not a flow |\n| Utilities | Helpers, not data flows |\n| Auth checks | Gate, not a pipeline |\n\n## Pipeline Naming Convention\n\n```\nP{N}: {Noun} {Action}\n\nP1: Source Ingestion\nP2: Content Generation\nP3: Post Publishing\nP4: User Registration\nP5: Subscription Update\n```\n\n## Minimum Documentation\n\nEach pipeline needs at minimum:\n1. ID (P1, P2, etc.)\n2. Name (descriptive)\n3. Trigger (what starts it)\n4. Outcome (what it produces)\n5. Key files (entry point at least)\n"
    },
    {
      "name": "pipeline-template.md",
      "path": "references/pipeline-template.md",
      "content": "# Pipeline Template\n\nStandard format for documenting a backend pipeline.\n\n## Full Template\n\n```markdown\n### P{N}: {Pipeline Name}\n\n**Trigger:** {What starts this pipeline}\n**Frequency:** {How often it runs, approximate}\n\n**Steps:**\n1. {Step description} (`{file}:{line}`)\n2. {Step description} (`{file}:{line}`)\n3. {Step description} (`{file}:{line}`)\n...\n\n**Outcome:**\n- {Database table}: {what changes}\n- {External system}: {what happens}\n\n**Key Files:**\n- `{primary file path}`\n- `{secondary file path}`\n\n**Dependencies:**\n- {External service or internal dependency}\n\n**Notes:**\n- {Any important context}\n```\n\n## Example: Content Generation\n\n```markdown\n### P2: Content Generation\n\n**Trigger:** User clicks \"Generate\" button with sources selected\n**Frequency:** ~200/day\n\n**Steps:**\n1. Request received at `api/generate/route.ts:45`\n2. Sources fetched from context (`lib/context.ts:23`)\n3. Source content aggregated (`lib/sources/aggregate.ts:67`)\n4. Prompt constructed (`lib/prompts/builder.ts:89`)\n5. OpenAI completion requested (`lib/llm/openai.ts:34`)\n6. Response parsed and validated (`lib/parsers/content.ts:12`)\n7. Artifact record created (`lib/db/artifacts.ts:56`)\n8. Generation job marked complete (`lib/jobs/status.ts:78`)\n\n**Outcome:**\n- `artifacts` table: new row with generated content\n- `generation_jobs` table: status updated to 'complete'\n- User sees new artifact in UI\n\n**Key Files:**\n- `api/generate/route.ts` — Entry point\n- `lib/llm/openai.ts` — LLM integration\n- `lib/db/artifacts.ts` — Persistence\n\n**Dependencies:**\n- OpenAI API (gpt-4)\n- Supabase (database)\n\n**Notes:**\n- Generation can take 10-30 seconds\n- Implements retry logic for API failures\n```\n\n## Minimal Template\n\nFor quick documentation:\n\n```markdown\n### P{N}: {Name}\n\n**Trigger:** {What starts it}\n**Outcome:** {What it produces}\n**Entry:** `{main file path}`\n```\n\n## Step Documentation\n\nEach step should include:\n- Action verb (received, validated, parsed, created)\n- Object (request, data, record)\n- Location (file:line)\n\n**Good:**\n```\n3. Source content aggregated (`lib/sources/aggregate.ts:67`)\n```\n\n**Bad:**\n```\n3. Aggregate sources\n```\n\n## Outcome Documentation\n\nBe specific about effects:\n\n**Good:**\n```\n- `artifacts` table: new row with type, content, metadata\n- `jobs` table: status changed from 'pending' to 'complete'\n```\n\n**Bad:**\n```\n- Database updated\n```\n"
    }
  ],
  "tags": [
    "audit",
    "pipeline",
    "discovery",
    "backend",
    "data-flow"
  ],
  "dependsOn": [
    "requirements"
  ]
}