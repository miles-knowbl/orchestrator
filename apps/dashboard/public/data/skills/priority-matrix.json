{
  "id": "priority-matrix",
  "name": "priority-matrix",
  "version": "2.0.0",
  "description": "Rank opportunities and work threads across multiple prioritization dimensions. Produces actionable priority matrices with calibrated scores, bias-checked rationale, and stakeholder-aligned rankings. Creates the prioritized backlog that feeds into proposal building.",
  "phase": "IMPLEMENT",
  "category": "specialized",
  "content": "# Priority Matrix\n\nRank opportunities and work threads across multiple prioritization dimensions.\n\n## When to Use\n\n- **Multiple competing opportunities** -- Several options identified, need systematic ranking\n- **Resource allocation decisions** -- Limited capacity, must choose what to pursue first\n- **Stakeholder alignment** -- Different stakeholders advocate for different priorities\n- **Post-cultivation synthesis** -- Context cultivation surfaced many threads, need to converge\n- **Strategic planning** -- Quarterly or initiative-level prioritization across a portfolio\n- When you say: \"prioritize these\", \"what should we focus on\", \"rank the options\", \"which one first?\"\n\n## Reference Requirements\n\n**MUST read before applying this skill:**\n\n| Reference | Why Required |\n|-----------|--------------|\n| `prioritization-frameworks.md` | Catalog of frameworks (RICE, MoSCoW, ICE, Eisenhower) to select from |\n| `scoring-calibration.md` | Techniques for consistent, unbiased scoring across options |\n\n**Read if applicable:**\n\n| Reference | When Needed |\n|-----------|-------------|\n| `bias-mitigation.md` | When scoring involves subjective judgment or political dynamics |\n| `stakeholder-alignment.md` | When multiple stakeholders must agree on priorities |\n| `effort-estimation.md` | When effort dimension requires detailed estimation |\n\n**Verification:** Ensure at least one scored matrix is produced with documented rationale for every score.\n\n## Required Deliverables\n\n| Deliverable | Location | Condition |\n|-------------|----------|-----------|\n| `PRIORITIES.md` | Project root | Always -- ranked list with rationale |\n| `MATRIX.md` | Project root | Always -- full scoring matrix with dimension breakdowns |\n| `CALIBRATION-LOG.md` | Project root | When 5+ options are scored -- documents calibration decisions |\n\n## Core Concept\n\nPriority Matrix answers: **\"What should we do first, and why?\"**\n\nPrioritization decisions are:\n- **Systematic** -- Use structured frameworks, not gut feeling\n- **Transparent** -- Every score has documented rationale\n- **Calibrated** -- Scores are consistent across options and dimensions\n- **Stakeholder-aligned** -- Methodology agreed upon before scoring begins\n- **Actionable** -- Results in a clear, ordered backlog with next steps\n\nPrioritization is NOT:\n- Voting or popularity contests (that is opinion gathering)\n- One-time decisions set in stone (priorities evolve; re-score periodically)\n- A substitute for strategy (strategy defines goals; prioritization orders the path)\n- Detailed project planning (that is `spec` and `scaffold`)\n\n## The Priority Matrix Process\n\n```\n+---------------------------------------------------------------+\n|                    PRIORITY MATRIX PROCESS                     |\n|                                                                |\n|  1. OPTION ENUMERATION                                         |\n|     +-> Collect and define all candidate work threads          |\n|                                                                |\n|  2. FRAMEWORK SELECTION                                        |\n|     +-> Choose the right prioritization framework              |\n|                                                                |\n|  3. DIMENSION DEFINITION                                       |\n|     +-> Define and weight scoring dimensions                   |\n|                                                                |\n|  4. CALIBRATION                                                |\n|     +-> Establish scoring anchors and reference points         |\n|                                                                |\n|  5. SCORING                                                    |\n|     +-> Score each option on each dimension with rationale     |\n|                                                                |\n|  6. AGGREGATION                                                |\n|     +-> Calculate weighted scores and produce rankings         |\n|                                                                |\n|  7. BIAS CHECK                                                 |\n|     +-> Review for cognitive biases and political distortion   |\n|                                                                |\n|  8. VALIDATION                                                 |\n|     +-> Sanity check results against intuition and strategy    |\n+---------------------------------------------------------------+\n```\n\n## Step 1: Option Enumeration\n\nBefore scoring anything, build the complete list of candidates.\n\n| Aspect | Questions |\n|--------|-----------|\n| **Completeness** | Have we captured every thread from context cultivation? |\n| **Granularity** | Are options at a comparable level of scope? |\n| **Independence** | Can each option be pursued independently? |\n| **Clarity** | Is each option defined well enough to score? |\n| **Duplicates** | Have we merged overlapping or redundant options? |\n\n### Option Definition Template\n\nFor each candidate, capture enough context to score it:\n\n```markdown\n### Option: [Name]\n\n**Description:** [One-paragraph summary of what this work thread entails]\n\n**Origin:** [Where this option came from -- cultivation theme, stakeholder request, gap analysis]\n\n**Scope estimate:** [T-shirt size: XS / S / M / L / XL]\n\n**Dependencies:** [Other options or external factors this depends on]\n\n**Stakeholder champion:** [Who is advocating for this option, if anyone]\n```\n\n### Enumeration Checklist\n\n```markdown\n- [ ] All context cultivation themes represented\n- [ ] All stakeholder requests captured\n- [ ] All gap analysis items included\n- [ ] Options are at comparable granularity\n- [ ] Duplicates and overlaps merged\n- [ ] Each option has a clear one-line description\n- [ ] Total option count is manageable (3-15 is ideal)\n```\n\n**If you have more than 15 options:** Group them into categories first, prioritize categories, then prioritize within the top 2-3 categories. Avoid scoring 20+ items in a single matrix -- cognitive load degrades scoring quality.\n\n## Step 2: Framework Selection\n\nChoose the prioritization framework that fits your context. No single framework is universally best.\n\n### Framework Comparison\n\n| Framework | Best For | Dimensions | Complexity |\n|-----------|----------|------------|------------|\n| **Weighted Scoring** | General-purpose, customizable | Custom (3-6) | Medium |\n| **RICE** | Product features, growth teams | Reach, Impact, Confidence, Effort | Medium |\n| **ICE** | Quick scoring, startups | Impact, Confidence, Ease | Low |\n| **MoSCoW** | Scope negotiation, fixed timelines | Must/Should/Could/Won't | Low |\n| **Eisenhower** | Personal or team task triage | Urgency, Importance | Low |\n| **Cost of Delay** | Economic optimization | Value, Time criticality, Risk reduction | High |\n\n### Framework Selection Guide\n\n**Use Weighted Scoring when:**\n- You need full customization of dimensions\n- Stakeholders need to see transparent methodology\n- Options span diverse categories (technical, business, operational)\n\n**Use RICE when:**\n- Prioritizing product features or growth initiatives\n- You have data on reach (users affected) and can estimate confidence\n- The team is familiar with product management practices\n\n**Use ICE when:**\n- Speed matters more than precision\n- Early-stage exploration with high uncertainty\n- You need a quick first pass before deeper analysis\n\n**Use MoSCoW when:**\n- Working with a fixed deadline or scope boundary\n- Stakeholders think in terms of \"must have\" vs \"nice to have\"\n- Negotiating scope for an MVP or release\n\n**Use Eisenhower when:**\n- Triaging a mix of urgent and important work\n- Team is overwhelmed and needs to cut non-essential work\n- Distinguishing between reactive and proactive priorities\n\n**Use Cost of Delay when:**\n- Options have different time sensitivities\n- Delay has measurable economic impact\n- You want to optimize sequencing, not just ranking\n\n### Framework Decision Record\n\n```markdown\n### Framework Choice: [Name]\n\n**Selected:** [Framework name]\n**Reason:** [Why this framework fits]\n**Alternatives considered:** [What else was considered and why rejected]\n**Adaptations:** [Any modifications to the standard framework]\n```\n\n## Step 3: Dimension Definition\n\nDefine the scoring dimensions and their weights. Dimensions should be:\n\n- **Independent** -- Avoid double-counting the same factor\n- **Measurable** -- Each dimension must be scorable (even if subjective)\n- **Relevant** -- Every dimension should matter for this specific decision\n- **Balanced** -- No single dimension should dominate unless intentional\n\n### Default Weighted Scoring Dimensions\n\n| Dimension | Weight | Description | Score Guide |\n|-----------|--------|-------------|-------------|\n| **Impact** | 30% | Business value delivered if completed | 1=negligible, 5=moderate, 10=transformative |\n| **Effort** | 25% | Resources required (inverse: high effort = low score) | 1=massive, 5=moderate, 10=trivial |\n| **Urgency** | 25% | Time sensitivity and cost of delay | 1=can wait indefinitely, 5=this quarter, 10=this week |\n| **Alignment** | 20% | Fit with strategy, goals, and capabilities | 1=off-strategy, 5=partially aligned, 10=core to strategy |\n\n### RICE Dimensions\n\n| Dimension | Description | Scale |\n|-----------|-------------|-------|\n| **Reach** | How many users/customers affected per quarter | Absolute number (e.g., 500, 5000) |\n| **Impact** | How much each person is affected | 0.25=minimal, 0.5=low, 1=medium, 2=high, 3=massive |\n| **Confidence** | How sure are we about the estimates | 50%=low, 80%=medium, 100%=high |\n| **Effort** | Person-months of work required | Absolute number (lower = better) |\n\nRICE score = (Reach x Impact x Confidence) / Effort\n\n### ICE Dimensions\n\n| Dimension | Description | Scale |\n|-----------|-------------|-------|\n| **Impact** | How much will this move the needle | 1-10 |\n| **Confidence** | How sure are we this will work | 1-10 |\n| **Ease** | How easy is this to implement | 1-10 |\n\nICE score = Impact x Confidence x Ease\n\n### Custom Dimension Template\n\n```markdown\n### Dimension: [Name]\n\n**Weight:** [Percentage]\n**Description:** [What this dimension measures]\n\n**Scoring guide:**\n- 1-2: [Low anchor description]\n- 3-4: [Below average description]\n- 5-6: [Average description]\n- 7-8: [Above average description]\n- 9-10: [High anchor description]\n\n**Example scores:**\n- [Concrete example] = [Score] because [rationale]\n- [Concrete example] = [Score] because [rationale]\n```\n\n### Weight Allocation Checklist\n\n```markdown\n- [ ] Weights sum to 100%\n- [ ] No single dimension exceeds 40% (unless deliberately chosen)\n- [ ] Stakeholders agree on dimension definitions\n- [ ] Stakeholders agree on weight distribution\n- [ ] Score guides are documented before scoring begins\n```\n\n## Step 4: Calibration\n\nCalibration ensures scores are consistent and meaningful. This is the most overlooked step and the most important for producing trustworthy results.\n\n### Calibration Techniques\n\n| Technique | Description | When to Use |\n|-----------|-------------|-------------|\n| **Anchor setting** | Score 2-3 reference options first as benchmarks | Always (5+ options) |\n| **End-point definition** | Define concrete examples for 1, 5, and 10 | Always |\n| **Independent scoring** | Multiple people score independently, then compare | When reducing individual bias |\n| **Relative ranking first** | Rank options within each dimension before assigning numbers | When absolute scoring feels arbitrary |\n| **Score normalization** | Adjust scores to use the full range (avoid clustering at 7-8) | When scores bunch together |\n\n### Anchor Setting Process\n\n1. Choose 2-3 well-understood options as anchors\n2. Score anchors first on all dimensions\n3. Use anchors as reference points when scoring remaining options\n4. If a new score feels wrong relative to an anchor, adjust\n\n```markdown\n### Calibration Anchors\n\n| Anchor Option | Dimension | Score | Rationale |\n|---------------|-----------|-------|-----------|\n| [Well-known option A] | Impact | 8 | [Why this is an 8] |\n| [Well-known option B] | Impact | 3 | [Why this is a 3] |\n| [Well-known option A] | Effort | 5 | [Why this is a 5] |\n| [Well-known option B] | Effort | 9 | [Why this is a 9] |\n```\n\n### Score Distribution Health Check\n\nAfter scoring, verify the distribution looks reasonable:\n\n```markdown\n- [ ] Scores use at least 60% of the 1-10 range (not all clustered at 6-8)\n- [ ] At least one option scores below 5 on some dimension\n- [ ] At least one option scores above 8 on some dimension\n- [ ] No dimension has all identical scores (that dimension is useless)\n- [ ] Anchor options still feel correctly placed relative to others\n```\n\n## Step 5: Scoring\n\nScore every option on every dimension. Document rationale for every score.\n\n### Scoring Template\n\n```markdown\n## Scores: [Option Name]\n\n| Dimension | Score | Rationale |\n|-----------|-------|-----------|\n| Impact | [1-10] | [Why this score -- specific evidence or reasoning] |\n| Effort | [1-10] | [Why this score -- reference comparable past work] |\n| Urgency | [1-10] | [Why this score -- what happens if we delay] |\n| Alignment | [1-10] | [Why this score -- which strategic goals does it serve] |\n```\n\n### Scoring Discipline Rules\n\n1. **Score one dimension at a time across all options** -- Not one option at a time. This improves consistency.\n2. **Write rationale before moving to the next score** -- Forces deliberate thinking.\n3. **Reference anchors frequently** -- \"Is this more or less impactful than Anchor A?\"\n4. **Flag low-confidence scores** -- Mark scores where you are guessing with a `(?)` suffix.\n5. **Separate fact from opinion** -- Note when a score is data-driven vs. judgment-based.\n\n### Handling Uncertainty\n\n| Confidence Level | Approach |\n|-----------------|----------|\n| **High** (data available) | Score directly, cite the data |\n| **Medium** (informed judgment) | Score with rationale, flag as estimate |\n| **Low** (guessing) | Use range (e.g., 4-7), take midpoint, flag for validation |\n| **Unknown** | Assign neutral score (5), heavily flag, seek more information |\n\n## Step 6: Aggregation\n\nCalculate weighted scores and produce the final ranking.\n\n### Weighted Score Calculation\n\nFor each option: `Final Score = SUM(dimension_score x dimension_weight)`\n\n### Full Matrix Template\n\n```markdown\n# Priority Matrix\n\n**Framework:** [Framework name]\n**Date:** [Date scored]\n**Scored by:** [Who participated]\n\n## Dimension Weights\n\n| Dimension | Weight |\n|-----------|--------|\n| Impact | 30% |\n| Effort | 25% |\n| Urgency | 25% |\n| Alignment | 20% |\n\n## Scoring Matrix\n\n| # | Option | Impact | Effort | Urgency | Alignment | Weighted Score | Rank |\n|---|--------|--------|--------|---------|-----------|----------------|------|\n| 1 | Option Alpha | 9 | 7 | 8 | 9 | **8.25** | 1 |\n| 2 | Option Beta | 8 | 6 | 9 | 7 | **7.65** | 2 |\n| 3 | Option Gamma | 7 | 9 | 5 | 8 | **7.10** | 3 |\n| 4 | Option Delta | 6 | 8 | 4 | 6 | **5.90** | 4 |\n\n## Score Breakdown\n\n[Include rationale tables from Step 5 for each option]\n```\n\n### RICE Score Calculation\n\n```markdown\n| Option | Reach | Impact | Confidence | Effort | RICE Score | Rank |\n|--------|-------|--------|------------|--------|------------|------|\n| Alpha | 5000 | 2 | 80% | 3 | 2667 | 1 |\n| Beta | 2000 | 3 | 100% | 2 | 3000 | -- |\n```\n\nNote: RICE produces absolute scores, not 1-10 normalized scores. Rankings compare relative positions.\n\n### Tie-Breaking Rules\n\nWhen two options have the same or very close scores (within 5%):\n\n1. **Impact wins** -- Higher impact option ranks higher\n2. **Effort wins** -- If impact is equal, lower effort option ranks higher\n3. **Confidence wins** -- If still tied, higher confidence option ranks higher\n4. **Stakeholder tiebreak** -- Escalate to the decision maker\n\n## Step 7: Bias Check\n\nAfter scoring and ranking, explicitly check for common cognitive biases that distort prioritization.\n\n### Bias Checklist\n\n| Bias | Description | Check Question |\n|------|-------------|----------------|\n| **Anchoring** | Over-weighting the first information received | Did the order we scored options affect results? |\n| **Recency** | Favoring recently discussed options | Are older but valid options ranked too low? |\n| **Sunk cost** | Favoring options with prior investment | Are we scoring future value, not past spend? |\n| **Champion bias** | Favoring options pushed by senior stakeholders | Would the ranking change if options were anonymous? |\n| **Availability** | Overweighting vivid or memorable options | Are less flashy options getting fair scores? |\n| **Groupthink** | Conforming to perceived team consensus | Did anyone score independently before group discussion? |\n| **Optimism** | Underestimating effort, overestimating impact | Are effort scores realistic? Compare to past projects. |\n| **Status quo** | Favoring options that preserve current state | Are transformative options getting fair impact scores? |\n\n### Bias Mitigation Techniques\n\n```markdown\n- [ ] Score independently before group discussion\n- [ ] Randomize the order options are presented\n- [ ] Assign a \"devil's advocate\" for top-ranked options\n- [ ] Compare effort estimates to actual past effort on similar work\n- [ ] Ask: \"Would we still rank this #1 if [champion] were not advocating?\"\n- [ ] Ask: \"What would need to be true for the bottom option to be #1?\"\n- [ ] Re-score any flagged low-confidence scores after bias review\n```\n\n> See `references/bias-mitigation.md`\n\n## Step 8: Validation\n\nThe matrix produces a ranking. Validation ensures the ranking makes sense.\n\n### Validation Questions\n\n```markdown\n- [ ] Does the #1 option feel right? If not, what is the matrix missing?\n- [ ] Would a reasonable stakeholder object to the top 3? Why?\n- [ ] Do the bottom 3 genuinely deserve to be deprioritized?\n- [ ] Does the ranking align with stated strategy and goals?\n- [ ] Are there dependencies that make the ranking impractical?\n- [ ] Is there an obvious \"do first\" option the matrix ranked low?\n- [ ] Has the methodology been explained and accepted by stakeholders?\n```\n\n### The Gut Check Test\n\nIf the matrix ranking contradicts strong intuition, do not simply override it. Instead:\n\n1. **Identify the gap** -- Which dimension is causing the surprising result?\n2. **Check scoring** -- Was that dimension scored correctly for the surprising option?\n3. **Check weighting** -- Should that dimension's weight be adjusted?\n4. **Resolve explicitly** -- Either fix the model or document why intuition is wrong\n\n```markdown\n### Gut Check: [Surprising Result]\n\n**Expected rank:** [What you expected]\n**Actual rank:** [What the matrix produced]\n**Root cause:** [Which dimension/weight caused the surprise]\n**Resolution:** [Adjusted score/weight OR accepted the matrix result because...]\n```\n\n### Dependency Validation\n\nEven if Option A outranks Option B, if A depends on B, then B must come first. After ranking:\n\n```markdown\n- [ ] Check for dependency cycles (A needs B, B needs A)\n- [ ] Adjust execution order for hard dependencies\n- [ ] Note where parallel execution is possible\n- [ ] Document any re-ordering and rationale\n```\n\n## Output Formats\n\n### Quick Priority List (3-5 Options)\n\n```markdown\n# Priorities: [Context]\n\n**Date:** [Date]\n**Framework:** ICE (quick scoring)\n\n## Priority Stack\n\n1. **[Option Name]** (Score: 8.5)\n   - Why first: [One-line rationale]\n   - Next action: [Concrete next step]\n\n2. **[Option Name]** (Score: 7.2)\n   - Why second: [One-line rationale]\n   - Next action: [Concrete next step]\n\n3. **[Option Name]** (Score: 6.8)\n   - Why third: [One-line rationale]\n   - Next action: [Concrete next step]\n\n## Deprioritized\n\n- **[Option Name]** (Score: 4.1) -- [Why not now]\n- **[Option Name]** (Score: 3.5) -- [Why not now]\n\n## Methodology\n\n[Brief description of framework and dimensions used]\n```\n\n### Full Priority Matrix (6+ Options)\n\n```markdown\n# Priority Matrix: [Context]\n\n**Date:** [Date]\n**Framework:** [Framework name]\n**Scored by:** [Participants]\n**Confidence:** [Overall confidence level]\n\n## Executive Summary\n\n[Top 3 priorities in one paragraph. Key trade-offs and rationale.]\n\n## Priority Stack\n\n### Tier 1: Do Now\n| Rank | Option | Score | Key Rationale |\n|------|--------|-------|---------------|\n| 1 | [Name] | [Score] | [Why] |\n| 2 | [Name] | [Score] | [Why] |\n\n### Tier 2: Do Next\n| Rank | Option | Score | Key Rationale |\n|------|--------|-------|---------------|\n| 3 | [Name] | [Score] | [Why] |\n| 4 | [Name] | [Score] | [Why] |\n\n### Tier 3: Do Later\n| Rank | Option | Score | Key Rationale |\n|------|--------|-------|---------------|\n| 5 | [Name] | [Score] | [Why] |\n| 6 | [Name] | [Score] | [Why] |\n\n### Not Now\n| Option | Score | Why Deprioritized |\n|--------|-------|-------------------|\n| [Name] | [Score] | [Reason] |\n\n## Full Scoring Matrix\n\n[Include the complete matrix table from Step 6]\n\n## Dimension Definitions and Weights\n\n[Include the dimension table from Step 3]\n\n## Calibration Notes\n\n[Include anchors and calibration decisions from Step 4]\n\n## Score Rationale\n\n[Include per-option rationale tables from Step 5]\n\n## Bias Review\n\n[Include completed bias checklist from Step 7]\n\n## Validation Notes\n\n[Include completed validation checklist from Step 8]\n\n## Execution Dependencies\n\n[Dependency map and any re-ordering from validation]\n\n## Next Steps\n\n1. [Concrete action for priority #1]\n2. [Concrete action for priority #2]\n3. [When to re-score -- trigger conditions]\n```\n\n## Common Patterns\n\n### The Portfolio Prioritization\n\nScore strategic initiatives across business dimensions: revenue impact, customer satisfaction, operational efficiency, competitive positioning.\n\n**Use when:** Annual or quarterly planning, allocating a fixed budget across initiatives.\n\n### The Feature Backlog Triage\n\nApply RICE scoring to a product feature backlog. Emphasize reach and confidence to avoid building low-impact, high-uncertainty features.\n\n**Use when:** Sprint planning, product roadmap creation, feature grooming sessions.\n\n### The Technical Debt Matrix\n\nScore technical debt items on: blast radius (what breaks if ignored), fix effort, frequency of pain, and dependency blocking. Weight blast radius and dependency blocking heavily.\n\n**Use when:** Deciding which tech debt to address during a maintenance sprint or platform stabilization.\n\n### The Opportunity Assessment\n\nCombine market analysis with feasibility scoring. Dimensions include: market size, competitive intensity, capability fit, time to value.\n\n**Use when:** Evaluating new business opportunities, partnership proposals, or market entry decisions.\n\n## Relationship to Other Skills\n\n| Skill | Relationship |\n|-------|--------------|\n| `context-cultivation` | Cultivation outputs become the option list for prioritization |\n| `context-ingestion` | Raw context feeds cultivation which feeds prioritization |\n| `proposal-builder` | Priorities feed directly into proposal scope and sequencing |\n| `architect` | Top-priority options may need architectural analysis |\n| `spec` | After prioritization, top items move into specification |\n| `implement` | Execution order follows the priority stack |\n| `code-review` | Review effort can be prioritized using a lightweight matrix |\n\n## Key Principles\n\n**Methodology before scoring.** Agree on framework, dimensions, and weights before anyone scores a single option. Changing methodology mid-scoring invalidates prior scores.\n\n**Rationale is the product.** The ranked list is useful; the documented rationale is invaluable. Future prioritization decisions build on today's reasoning, not just today's numbers.\n\n**Calibrate relentlessly.** Uncalibrated scores are meaningless. Two people giving \"7 out of 10\" may mean completely different things. Anchors and examples make scores comparable.\n\n**Separate scoring from advocacy.** The person who proposed an option should not be the only person scoring it. Separation reduces champion bias and increases trust in results.\n\n**Prioritization is perishable.** A priority matrix has a shelf life. Re-score when new information emerges, when market conditions change, or at regular intervals (monthly or quarterly).\n\n**Explicit is better than implicit.** A mediocre framework applied transparently beats a brilliant decision made in someone's head. The matrix makes reasoning visible, challengeable, and improvable.\n\n## References\n\n- `references/prioritization-frameworks.md`: Detailed framework implementations (RICE, MoSCoW, ICE, Eisenhower, Cost of Delay)\n- `references/scoring-calibration.md`: Calibration techniques, anchor setting, score normalization\n- `references/bias-mitigation.md`: Cognitive biases in prioritization and countermeasures\n- `references/stakeholder-alignment.md`: Techniques for building consensus on methodology and results\n- `references/effort-estimation.md`: Effort estimation approaches for the effort/ease dimension",
  "references": [
    {
      "name": "bias-mitigation.md",
      "path": "references/bias-mitigation.md",
      "content": "# Bias Mitigation Reference\n\nCognitive biases systematically distort prioritization. This reference catalogs the most common biases, detection techniques, and mitigation strategies.\n\n## Bias Catalog\n\n### 1. Anchoring Bias\n\nThe first information received disproportionately influences all subsequent judgments.\n\n**Detect:** First option scored lands at 5-7; later options cluster around it. Randomizing order changes the ranking.\n**Mitigate:** Score dedicated anchors first. Randomize option order. Score one dimension across all options before moving to the next.\n\n### 2. Recency Bias\n\nFavoring options recently discussed, requested, or that recently caused pain.\n\n**Detect:** Last week's hot topic ranks disproportionately high. Older strategic options rank lower than expected.\n**Mitigate:** Ask \"Would this rank the same if it came up 3 months ago?\" Review option origins for temporal skew.\n\n### 3. Sunk Cost Bias\n\nFavoring options with prior investment, regardless of future value.\n\n**Detect:** Rationale includes \"we already invested X.\" Abandoning partial work feels disproportionately painful.\n**Mitigate:** Score only future value. Ask \"If starting fresh, would we still choose this?\" Separate continue/abandon from prioritize.\n\n### 4. Champion Bias\n\nOptions from senior or vocal individuals receive inflated scores.\n\n**Detect:** Leadership-proposed options rank consistently higher. Anonymous scoring produces a different ranking.\n**Mitigate:** Use independent scoring before group discussion. Present options without attribution. Ask \"Would this rank #1 without [champion]?\"\n\n### 5. Availability Bias\n\nOverweighting vivid or memorable options; underweighting abstract ones.\n\n**Detect:** Flashy features outrank foundational work. Infrastructure and security score low despite known importance.\n**Mitigate:** Include \"risk reduction\" dimension. Force concrete examples for abstract options. Review bottom-ranked items for this bias.\n\n### 6. Groupthink\n\nTeam converges without genuine critical evaluation due to social pressure.\n\n**Detect:** Scores converge after one person shares. No scores below 5. Dissenters self-censor.\n**Mitigate:** Always score independently first. Invite dissent explicitly. Rotate a devil's advocate role. Use anonymous scoring.\n\n### 7. Optimism Bias\n\nSystematically overestimating benefits and underestimating effort and risk.\n\n**Detect:** Impact averages above 7. Effort averages above 6. Past estimates were 40-100% too low.\n**Mitigate:** Use reference class forecasting. Apply 0.7x optimism discount to effort. Require at least one `(?)` flag.\n\n### 8. Status Quo Bias\n\nFavoring options that preserve the current state; undervaluing change.\n\n**Detect:** Incremental improvements outrank transformative options. Risk of change is weighed but risk of inaction is not.\n**Mitigate:** Score \"cost of inaction\" explicitly. Ask \"What happens if we do nothing for 12 months?\"\n\n## Pre-Mortem Technique\n\nRun a pre-mortem on the top 1-2 ranked options before finalizing.\n\n**Process:**\n1. Assume the top-ranked option has been completed but **failed to deliver expected value**\n2. Each participant writes 2-3 reasons why it might have failed\n3. Assess whether any failure reasons reveal scoring errors (inflated impact, underestimated effort)\n4. Adjust scores if the pre-mortem surfaces genuine concerns\n\n**Template:**\n```\nPRE-MORTEM: [Top Ranked Option]\nScenario: It is 6 months from now. We shipped this. It failed.\nFailure reasons:\n- [Reason 1] --> Does this change Impact score? Confidence?\n- [Reason 2] --> Does this change Effort score?\nAdjustment: [None / Revised scores / Flagged for monitoring]\n```\n\n## Red Team Technique\n\nAssign 1-2 people to argue against the top-ranked option and for the lowest-ranked option.\n\n**Prompts:** \"Make the strongest case that #1 should be #5.\" / \"What evidence would change the ranking?\" / \"Which score is most likely wrong, and in which direction?\"\n\n**Rules:** Red team arguments must reference specific dimension scores. They need to stress-test the ranking, not win. If compelling, re-score the affected dimension.\n\n## Bias Review Checklist\n\nRun this checklist after scoring is complete and before finalizing the ranking.\n\n```\n- [ ] Anchoring: Were anchors set before real scoring began?\n- [ ] Recency: Are recently-discussed options disproportionately high?\n- [ ] Sunk cost: Are any scores influenced by past investment rather than future value?\n- [ ] Champion: Would the ranking change if options were anonymous?\n- [ ] Availability: Are \"boring\" but important options (infra, security) fairly scored?\n- [ ] Groupthink: Did anyone score independently before group discussion?\n- [ ] Optimism: Are effort estimates realistic compared to past actuals?\n- [ ] Status quo: Are transformative options getting fair impact scores?\n- [ ] Pre-mortem: Has the top option been stress-tested for failure scenarios?\n- [ ] Red team: Has anyone argued against the consensus ranking?\n```\n\n## When to Escalate Bias Concerns\n\nIf a bias check reveals significant distortion (3+ point swing on any dimension), do not silently adjust. Instead:\n\n1. Name the specific bias detected and the evidence\n2. Propose specific score adjustments with rationale\n3. Re-run aggregation and compare old vs. new ranking\n4. Present both rankings to the decision maker with the bias analysis\n5. Let the decision maker choose, with full transparency about what changed and why\n"
    },
    {
      "name": "effort-estimation.md",
      "path": "references/effort-estimation.md",
      "content": "# Effort Estimation Reference\n\nTechniques for estimating effort accurately enough to score the effort/ease dimension in a priority matrix. The goal is not project-planning precision -- it is enough fidelity to differentiate options and produce a fair ranking.\n\n## Estimation Approaches\n\n| Approach | Speed | Precision | Best For |\n|----------|-------|-----------|----------|\n| **T-shirt sizing** | Very fast | Low | Quick triage, early-stage options, non-technical audiences |\n| **Story points** | Medium | Medium | Teams with established velocity, sprint-level planning |\n| **Time-based** | Slow | High | Well-defined scope, contractual commitments |\n| **Reference class** | Medium | Medium-High | Options similar to completed past work |\n\nChoose the approach that matches the precision needed for your prioritization framework. For ICE or MoSCoW, T-shirt sizing is sufficient. For Weighted Scoring or Cost of Delay, story points or reference class estimates add value.\n\n## T-Shirt Sizing\n\n### The Scale\n\n| Size | Definition | Typical Range | Effort Score (1-10) |\n|------|-----------|---------------|-------------------|\n| **XS** | One person, under a day | < 1 day | 10 |\n| **S** | One person, a few days | 1-3 days | 8 |\n| **M** | One person-week or small team, short sprint | 1-2 weeks | 6 |\n| **L** | Multi-person, multi-week effort | 3-6 weeks | 4 |\n| **XL** | Large team, multi-month initiative | 2-6 months | 2 |\n| **XXL** | Program-level, quarter+ | 6+ months | 1 |\n\n**Mapping to effort score:** The effort dimension in a priority matrix is typically inverse -- low effort = high score (easier = better). The table above reflects this inversion.\n\n### How to T-Shirt Size\n\n1. Start with one option that the team knows well and assign it a size\n2. For each subsequent option, ask: \"Is this bigger or smaller than [reference]?\"\n3. Place it in the appropriate bucket\n4. If an option sits between two sizes, round up (overestimate effort)\n5. Do not debate exact sizes for more than 2 minutes per option\n\n### When T-Shirt Sizing Breaks Down\n\n- Options vary by more than 100x in effort (XS and XXL in the same matrix)\n- Multiple options land in the same T-shirt size, and effort differentiation matters\n- Stakeholders need time/cost estimates, not relative sizes\n\nIn these cases, decompose large options or switch to a more precise method.\n\n## Story Points\n\n### Fibonacci Scale\n\n| Points | Meaning | Rough Time Equivalent (team-dependent) |\n|--------|---------|---------------------------------------|\n| 1 | Trivial, well-understood | Hours |\n| 2 | Small, straightforward | 1-2 days |\n| 3 | Small with minor complexity | 2-3 days |\n| 5 | Medium, some unknowns | 1 week |\n| 8 | Medium-large, multiple components | 1-2 weeks |\n| 13 | Large, significant complexity | 2-3 weeks |\n| 21 | Very large, should probably be decomposed | 3-5 weeks |\n\n**Key principle:** Story points measure complexity and uncertainty, not just time. An 8-point story is not \"8 hours\" -- it is roughly 3x the complexity of a 3-point story.\n\n### Converting Story Points to Effort Scores\n\nIf your team has established story point baselines:\n\n| Story Points | Effort Score (1-10 inverse) |\n|-------------|---------------------------|\n| 1-2 | 9-10 |\n| 3-5 | 7-8 |\n| 8 | 5-6 |\n| 13 | 3-4 |\n| 21+ | 1-2 |\n\n### Story Points vs. Hours\n\n| Aspect | Story Points | Hours |\n|--------|-------------|-------|\n| What they measure | Complexity + uncertainty | Calendar time |\n| Anchoring risk | Lower (abstract scale) | Higher (people think in hours) |\n| Useful for prioritization | Yes -- relative comparison | Overkill unless Cost of Delay |\n| Common failure mode | Point inflation over time | Optimism bias |\n\n**For priority matrices:** Prefer story points or T-shirt sizes over hours. The matrix needs relative comparison, not schedules.\n\n## Estimation Techniques\n\n### Planning Poker\n\nBest for: team estimation sessions with 3-8 people and up to 15 options.\n\n**Process:**\n1. Each estimator independently selects a Fibonacci card (1, 2, 3, 5, 8, 13, 21)\n2. All cards revealed simultaneously\n3. If estimates converge (within one step), take the higher value\n4. If divergent (2+ steps apart), highest and lowest explain reasoning, then re-estimate once\n5. Take the median of round-2 estimates\n\n**Ground rules:** No sharing before reveal. Option proposer estimates last. Time-box to 3 minutes per option.\n\n### Affinity Mapping\n\nBest for: quickly sizing 15+ options without detailed discussion. Speed: 20 options in 15-20 minutes.\n\n**Process:**\n1. Write each option on a card\n2. Place the first option in center; for each subsequent, place left (smaller) or right (larger) silently\n3. After all placed, draw T-shirt size boundaries and review edge cases together\n\n### Three-Point Estimation\n\nBest for: options with high uncertainty where a single estimate is misleading.\n\n**Formula:** `Expected = (Optimistic + 4*MostLikely + Pessimistic) / 6` | `Uncertainty = (P - O) / 6`\n\n| Option | Optimistic | Most Likely | Pessimistic | Expected | Uncertainty |\n|--------|-----------|-------------|-------------|----------|-------------|\n| Auth overhaul | 2 weeks | 4 weeks | 10 weeks | 4.7 weeks | 1.3 weeks |\n| Search feature | 1 week | 2 weeks | 4 weeks | 2.2 weeks | 0.5 weeks |\n\nFlag options where uncertainty exceeds 50% of expected value for further investigation.\n\n## Accounting for Uncertainty and Risk\n\n### Uncertainty Multipliers\n\nWhen converting raw effort estimates to effort scores, apply multipliers for uncertainty factors:\n\n| Factor | Multiplier | When to Apply |\n|--------|-----------|---------------|\n| **New technology** | 1.5x | Team has not used this tech before |\n| **Unclear requirements** | 1.5x | Scope is loosely defined |\n| **External dependency** | 1.3x | Depends on a third party or another team |\n| **Integration complexity** | 1.3x | Touches 3+ existing systems |\n| **Regulatory/compliance** | 1.5x | Subject to review processes outside the team |\n| **First-of-kind** | 2.0x | Nothing like this has been done before in the org |\n\n**Application:** Multiply the base effort estimate by all applicable factors, then convert to an effort score. Multiple factors compound.\n\n**Example:** Base estimate is M (1-2 weeks), but involves new technology (1.5x) and unclear requirements (1.5x). Adjusted estimate: 1.5 * 1.5 * M = 2.25 * M, which pushes it to L territory. Effort score drops from 6 to 4.\n\n### Risk-Adjusted Effort Scoring\n\nBuild risk into the effort score directly: well-understood work keeps its standard score; known approach with unknowns gets a 1-point discount; novel approach gets a 2-point discount; uncharted territory is treated as L or XL regardless of raw size.\n\n### When to Decompose Instead of Estimate\n\nIf estimated at XL+ or pessimistic-to-optimistic ratio exceeds 5x, decompose into sub-options before scoring. **Test:** Can you identify 2-4 independently deliverable pieces? If not, flag with a placeholder effort score of 2 `(?)`.\n"
    },
    {
      "name": "prioritization-frameworks.md",
      "path": "references/prioritization-frameworks.md",
      "content": "# Prioritization Frameworks Reference\n\nDetailed framework implementations for the Priority Matrix skill. Select the framework that fits your context, then apply its specific formulas and scoring approach.\n\n## Framework Comparison Matrix\n\n| Framework | Inputs Required | Precision | Speed | Best Team Size | Quantitative? |\n|-----------|----------------|-----------|-------|----------------|---------------|\n| **Weighted Scoring** | Custom dimensions + weights | High | Slow | Any | Yes |\n| **RICE** | Reach, impact, confidence, effort | High | Medium | Product teams | Yes |\n| **ICE** | Impact, confidence, ease | Medium | Fast | Small teams | Yes |\n| **MoSCoW** | Bucket classification | Low | Fast | Any | No |\n| **Eisenhower** | Urgency + importance classification | Low | Very fast | Individual/small | No |\n| **Cost of Delay** | Value rate, time criticality, risk | Very high | Slow | Strategic teams | Yes |\n\n## Framework Details\n\n### Weighted Scoring\n\n**Formula:** `Score = SUM(dimension_score * dimension_weight)`\n\nSetup: Define 3-6 dimensions, assign weights summing to 100%, create 1-10 rubrics, score all options.\n\n**Example:**\n\n| Option | Impact (30%) | Effort (25%) | Urgency (25%) | Alignment (20%) | Final |\n|--------|-------------|-------------|---------------|-----------------|-------|\n| Auth overhaul | 9 * 0.30 = 2.70 | 4 * 0.25 = 1.00 | 7 * 0.25 = 1.75 | 8 * 0.20 = 1.60 | **7.05** |\n| Search feature | 7 * 0.30 = 2.10 | 8 * 0.25 = 2.00 | 5 * 0.25 = 1.25 | 9 * 0.20 = 1.80 | **7.15** |\n\n**Strengths:** Fully customizable, transparent, stakeholder-friendly.\n**Weaknesses:** Weight selection is subjective, slower for large option sets.\n\n### RICE\n\n**Formula:** `RICE Score = (Reach * Impact * Confidence) / Effort`\n\n| Dimension | How to Measure | Scale |\n|-----------|---------------|-------|\n| Reach | Users or events affected per quarter | Raw number (100, 1000, 10000) |\n| Impact | Degree of effect per person | 0.25 = minimal, 0.5 = low, 1 = medium, 2 = high, 3 = massive |\n| Confidence | Certainty in estimates | 50% = speculative, 80% = reasonable, 100% = data-backed |\n| Effort | Person-months to complete | Raw number (0.5, 1, 3, 6) |\n\n**Example calculation:**\n\n| Option | Reach | Impact | Confidence | Effort | RICE Score |\n|--------|-------|--------|------------|--------|------------|\n| Onboarding flow | 3000 | 2 | 80% | 2 | **2400** |\n| Dashboard redesign | 5000 | 1 | 100% | 4 | **1250** |\n| API v2 | 800 | 3 | 50% | 3 | **400** |\n\n**Strengths:** Forces quantitative thinking, separates confidence from impact.\n**Weaknesses:** Requires reach data, less useful for internal/infrastructure work.\n\n### ICE\n\n**Formula:** `ICE Score = Impact * Confidence * Ease` (each on 1-10 scale)\n\n**Example calculation:**\n\n| Option | Impact | Confidence | Ease | ICE Score |\n|--------|--------|------------|------|-----------|\n| Quick win A | 6 | 9 | 9 | **486** |\n| Big bet B | 10 | 4 | 3 | **120** |\n| Safe bet C | 5 | 8 | 7 | **280** |\n\n**Strengths:** Fast to apply, easy to explain, good for rapid first-pass.\n**Weaknesses:** Multiplicative formula amplifies scoring inconsistencies.\n\n### MoSCoW\n\n**Categories (not scores):**\n\n| Category | Definition | Decision Rule |\n|----------|-----------|---------------|\n| **Must** | Non-negotiable for this release/milestone | Without it, the release fails or is pointless |\n| **Should** | Important but the release still works without it | Painful to omit but not fatal |\n| **Could** | Desirable, include if resources permit | Nice to have, no material impact if omitted |\n| **Won't** | Explicitly out of scope for this cycle | Acknowledged and deferred, not forgotten |\n\n**Application rule:** Musts should not exceed 60% of capacity. If they do, scope is too large.\n\n**Strengths:** Simple, great for scope negotiation, non-technical stakeholders understand it.\n**Weaknesses:** No granularity within categories, no ranking within Must.\n\n### Eisenhower Matrix\n\n**Two dimensions, four quadrants:**\n\n|  | **Urgent** | **Not Urgent** |\n|--|-----------|----------------|\n| **Important** | Q1: Do immediately | Q2: Schedule deliberately |\n| **Not Important** | Q3: Delegate or timebox | Q4: Eliminate |\n\n**Application:** Classify each option into one quadrant, then work Q1 first, invest in Q2, minimize Q3, eliminate Q4.\n\n**Strengths:** Cuts noise fast, forces importance vs. urgency distinction.\n**Weaknesses:** Binary classification loses nuance, no ranking within quadrants.\n\n### Cost of Delay\n\n**Formula:** `CD3 Score = Cost of Delay / Duration`\n\nCost of Delay is composed of three elements:\n\n| Element | Question | Scale |\n|---------|----------|-------|\n| **User/Business Value** | How much value does this deliver? | 1-10 |\n| **Time Criticality** | How much does value decay with delay? | 1-10 |\n| **Risk Reduction** | How much risk does this retire? | 1-10 |\n\n`Cost of Delay = User Value + Time Criticality + Risk Reduction`\n`CD3 = Cost of Delay / Duration (in weeks)`\n\n**Example:**\n\n| Option | Value | Time Crit. | Risk Red. | CoD | Duration | CD3 |\n|--------|-------|-----------|-----------|-----|----------|-----|\n| Compliance fix | 4 | 10 | 9 | 23 | 2 weeks | **11.5** |\n| New feature | 9 | 3 | 1 | 13 | 6 weeks | **2.2** |\n| Infra upgrade | 6 | 5 | 7 | 18 | 4 weeks | **4.5** |\n\n**Strengths:** Captures time sensitivity, optimal for sequencing decisions.\n**Weaknesses:** Requires duration estimates, harder to explain to non-technical stakeholders.\n\n## Framework Selection Decision Tree\n\nAnswer these questions in order to select a framework:\n\n1. **Do you need a quick triage?** (under 30 minutes, fewer than 8 options)\n   - Yes, and options are tasks/work items --> **Eisenhower**\n   - Yes, and options are features/initiatives --> **ICE**\n\n2. **Are you negotiating scope for a fixed deadline?**\n   - Yes --> **MoSCoW**\n\n3. **Do you have user reach data and a product team?**\n   - Yes --> **RICE**\n\n4. **Are options time-sensitive with varying urgency?**\n   - Yes --> **Cost of Delay (CD3)**\n\n5. **None of the above, or mixed option types?**\n   - --> **Weighted Scoring** (customize dimensions to fit)\n\n## Combining Frameworks\n\nFor complex prioritization, use a two-pass approach: MoSCoW or Eisenhower first to eliminate non-starters, then RICE, Weighted Scoring, or Cost of Delay to rank within the \"Must\" / \"Do\" categories. This avoids wasting detailed scoring effort on options that are clearly out of scope.\n"
    },
    {
      "name": "scoring-calibration.md",
      "path": "references/scoring-calibration.md",
      "content": "# Scoring Calibration Reference\n\nTechniques for producing consistent, meaningful scores across options and dimensions. Uncalibrated scoring is the single most common failure mode in prioritization.\n\n## Why Calibration Matters\n\nWithout calibration, \"7 out of 10\" means different things to different scorers, scores cluster at 6-8 (destroying discriminating power), and the first option scored anchors all others. With calibration, scores are comparable across scorers, the full 1-10 range is used deliberately, and results are reproducible.\n\n## Calibration Techniques\n\n### 1. Anchor Setting\n\nThe most important calibration technique. Always use it when scoring 5+ options.\n\n**Process:**\n1. Choose 2-3 options the team understands well (ideally past completed work)\n2. Score anchors first on every dimension before scoring anything else\n3. Document anchor scores and rationale explicitly\n4. Reference anchors when scoring each subsequent option\n\n**Selecting good anchors:**\n- One should be clearly high on most dimensions (the \"strong option\")\n- One should be clearly low on most dimensions (the \"weak option\")\n- Optionally, one should be mid-range (the \"typical option\")\n- Anchors should be familiar to all scorers\n\n**Anchor documentation template:**\n\n```\nANCHOR: [Option Name]\n  Impact: 8 -- Delivered measurable revenue lift in Q2\n  Effort: 3 -- Required 4 engineers for 3 months\n  Urgency: 6 -- Important but no hard deadline\n  Alignment: 9 -- Directly supports company OKR #1\n```\n\n### 2. End-Point Definition\n\nDefine concrete, real-world examples for score values 1, 5, and 10 on each dimension before anyone scores.\n\n| Score | Impact Example | Effort Example | Urgency Example |\n|-------|---------------|----------------|-----------------|\n| **1** | No measurable business change | Trivial -- one person, one day | Can wait 12+ months |\n| **5** | Moderate improvement to one metric | Standard -- 2 people, 4-6 weeks | Should happen this quarter |\n| **10** | Transformative to core business | Massive -- full team, 6+ months | Must happen this week or we lose the opportunity |\n\nIntermediate values (2-4, 6-9) are interpolated between these anchor points.\n\n### 3. Reference Class Forecasting\n\nInstead of estimating from scratch, compare each option to a \"reference class\" of similar past work.\n\n**Process:**\n1. For each option, identify 2-3 past projects with similar characteristics\n2. Look up actual outcomes (effort, impact, timeline) of those past projects\n3. Use the reference class distribution to inform scores\n\n**Example:**\n- Scoring effort for \"Build OAuth integration\"\n- Reference class: Previous integrations (Stripe: 3 weeks, Slack: 5 weeks, Salesforce: 8 weeks)\n- Reference class average: ~5 weeks --> Effort score: 5-6\n\n### 4. Delphi Method (Multi-Scorer Calibration)\n\nUse when multiple people are scoring and you need to reduce individual bias.\n\n**Process:**\n1. Each scorer scores independently (no discussion)\n2. Collect all scores and display them anonymously\n3. Identify dimensions where scores diverge by 3+ points\n4. Discuss divergent scores -- share reasoning, not just numbers\n5. Each scorer re-scores independently after discussion\n6. Take the median of round-2 scores as final\n\n**Key rules:**\n- Never average round-1 scores directly (that rewards extreme outliers)\n- Discussion focuses on evidence and reasoning, not persuasion\n- Scorers can change their scores but are not required to\n\n### 5. Relative Ranking Before Absolute Scoring\n\nWhen absolute scores feel arbitrary, rank first, then assign numbers. Sort all options per dimension from highest to lowest, assign the top a score (e.g., 9) and the bottom a score (e.g., 3), then fill in middle options with proportional spacing.\n\n## Score Distribution Health Checks\n\nAfter scoring, run these diagnostics:\n\n| Check | What to Look For | Fix |\n|-------|-----------------|-----|\n| **Range usage** | Min-max range < 4 points per dimension | Redefine end-points, rescore using relative ranking |\n| **Clustering** | >50% of scores on a dimension are identical | Dimension is not differentiating; recalibrate |\n| **Cross-correlation** | Two dimensions produce nearly identical rankings | Merge or drop the less important one |\n| **Scorer agreement** | Standard deviation > 2.5 on any option-dimension pair | Discuss the disagreement, share reasoning |\n| **Anchor consistency** | Anchor options no longer feel correctly placed | Recalibrate all scores against anchors |\n\n## Common Calibration Pitfalls\n\n| Pitfall | Symptom | Fix |\n|---------|---------|-----|\n| **Central tendency** | All scores between 5-7 | Redefine end-points, force at least one 2 and one 9 |\n| **Inflation** | All scores above 7 | Remind scorers that 5 is \"average\", not \"bad\" |\n| **First-option anchoring** | First option scored is always mid-range; others cluster around it | Score anchors first, not the first option on the list |\n| **Dimension leakage** | Impact score considers effort (\"it's impactful but hard\") | Score one dimension at a time across all options |\n| **Round-number bias** | Scores are disproportionately 5, 7, or 10 | Use forced ranking to identify true relative positions |\n| **Confidence conflation** | Low-confidence options get low scores instead of flagged mid-range | Separate confidence from the dimension score |\n\n## Calibration Session Facilitation Guide\n\n**Duration:** 15-30 minutes (before scoring begins)\n\n**Agenda:**\n1. **(5 min)** Review dimensions and weight allocations\n2. **(5 min)** Walk through end-point definitions for each dimension\n3. **(10 min)** Score 2-3 anchor options together, discussing and aligning\n4. **(5 min)** Confirm scoring ground rules:\n   - Score one dimension at a time across all options\n   - Write rationale before moving to the next score\n   - Flag low-confidence scores with `(?)`\n   - Reference anchors when uncertain\n\n**Post-scoring review (10 min):**\n1. Run health checks (range, clustering, correlation)\n2. Identify and discuss any outlier scores\n3. Confirm anchors still feel correctly placed\n4. Lock scores and move to aggregation\n\n## Recalibration Triggers\n\nRescore the matrix when any of these occur:\n- New information materially changes an option's expected impact or effort\n- A new option is added that would rank in the top 3\n- More than 30 days have passed since last scoring\n- A scored option has been completed and actual results diverge significantly from predicted\n- Team composition changes (new scorer brings different baseline)\n"
    },
    {
      "name": "stakeholder-alignment.md",
      "path": "references/stakeholder-alignment.md",
      "content": "# Stakeholder Alignment Reference\n\nPrioritization is only useful if stakeholders trust and adopt the results. This reference covers techniques for building consensus on methodology, resolving disagreements, communicating decisions, and escalating when alignment fails.\n\n## Alignment Sequence\n\nAlignment must happen in this order. Skipping earlier steps undermines later ones.\n\n| Step | What to Align On | When | Why First |\n|------|-----------------|------|-----------|\n| 1. Goal | What we are optimizing for | Before framework selection | Disagreement on goals makes all scoring contentious |\n| 2. Framework | Which prioritization framework to use | Before dimension definition | The framework determines how decisions are made |\n| 3. Dimensions | What dimensions and weights to use | Before scoring | Weights encode values; agree on values first |\n| 4. Scores | Individual option scores | During scoring | Scoring disagreements are healthy and expected |\n| 5. Results | The final ranking and action plan | After aggregation | Easier to accept when methodology was agreed upon |\n\n## Building Consensus on Methodology\n\n### Technique 1: Framework Proposal with Options\n\nPresent 2-3 framework candidates with pros/cons rather than dictating one.\n\n**Template:**\n```\nPRIORITIZATION METHODOLOGY PROPOSAL\nContext: We have [N] options to prioritize for [goal/timeline].\n\nOption A: Weighted Scoring -- Pro: customizable, transparent / Con: slower\nOption B: RICE -- Pro: standard, quantitative / Con: requires reach data\nOption C: ICE -- Pro: fast, simple / Con: less precise\n\nRecommendation: [Your pick and why]\n```\n\n### Technique 2: Weight Allocation Workshop\n\nWhen stakeholders disagree on dimension weights, use a structured allocation exercise.\n\n**Process:**\n1. Give each stakeholder 100 points to distribute across dimensions\n2. Collect allocations independently (no discussion during allocation)\n3. Display all allocations side by side\n4. Discuss dimensions where allocations diverge by more than 10 points\n5. Converge on final weights through discussion, not averaging\n\n**Example output:**\n\n| Dimension | Stakeholder A | Stakeholder B | Stakeholder C | Avg | Final (negotiated) |\n|-----------|--------------|--------------|--------------|-----|-------------------|\n| Impact | 40 | 25 | 30 | 32 | 30 |\n| Effort | 20 | 30 | 25 | 25 | 25 |\n| Urgency | 15 | 30 | 25 | 23 | 25 |\n| Alignment | 25 | 15 | 20 | 20 | 20 |\n\n**Key rule:** Final weights should reflect discussion outcomes, not raw averages. Averaging silences important reasoning behind outlier allocations.\n\n### Technique 3: Decision Rights Clarity\n\nBefore any prioritization session, establish who has which role.\n\n| Role | Responsibility | Typical Person |\n|------|---------------|----------------|\n| **Decision maker** | Final authority on ranking; breaks ties | Product lead, VP, or sponsor |\n| **Methodology owner** | Proposes and facilitates the framework | PM, analyst, or facilitator |\n| **Scorer** | Provides scores with rationale | Subject matter experts |\n| **Reviewer** | Validates results and checks for bias | Peer or external reviewer |\n| **Informed** | Receives the results but does not score | Wider team, leadership |\n\n## Handling Disagreements\n\n### On Dimension Weights\n\nAsk each stakeholder what outcome they are optimizing for. Often the disagreement is about goals, not weights. If goals differ, run two matrices and compare. If goals align, use the weight allocation workshop.\n\n### On Individual Scores\n\nWhen scorers disagree by 3+ points: share rationale (not just numbers), identify if the disagreement is factual or value-based. Factual: find data. Value-based: take the median and document dissent. Never pressure a scorer without new evidence.\n\n### On the Final Ranking\n\nIf a stakeholder rejects the ranking despite agreeing to methodology: ask which specific rank position is wrong and which score would need to change. If they can identify a scoring error, re-examine. If not, the disagreement is with weights -- revisit or escalate to the decision maker.\n\n## Communication Templates\n\n### Priority Decision Announcement\n\n```\nPRIORITY DECISION: [Context]\n\nWe scored [N] options using [Framework] with dimensions:\n[Dimension 1] ([Weight]%), [Dimension 2] ([Weight]%), ...\n\nTop 3 priorities (in order):\n1. [Option] -- Score: [X] -- Rationale: [one sentence]\n2. [Option] -- Score: [X] -- Rationale: [one sentence]\n3. [Option] -- Score: [X] -- Rationale: [one sentence]\n\nDeferred:\n- [Option] -- Reason: [why not now]\n\nMethodology and full scoring matrix: [link to MATRIX.md]\n\nNext steps: [what happens now for the top priorities]\nQuestions or concerns: [who to contact and by when]\n```\n\n### Disagreement Acknowledgment\n\nWhen a stakeholder's preferred option was deprioritized, proactively acknowledge it:\n\n```\nRegarding [Option Name]:\nThis scored [X] overall, ranking #[N] of [Total].\nFactors keeping it from top tier: [Dimension] scored [X] because [rationale].\nWhat would move it up: If [condition], it would likely move to tier [Z].\nRe-evaluation: [trigger or date for next review]\n```\n\n### Requesting Re-Prioritization\n\n```\nRE-PRIORITIZATION REQUEST\nTrigger: [What changed]  |  Affected options: [Which ones]\nExpected impact: [Which scores change, which direction]\nProposed action: [Re-score all / Re-score affected only / Adjust weights]\n```\n\n## Escalation Paths\n\nWhen alignment fails despite good-faith methodology, follow this escalation ladder:\n\n| Level | Situation | Action |\n|-------|-----------|--------|\n| **1. Scoring dispute** | Scorers disagree on a specific score | Share rationale, resolve with evidence, take median if unresolved |\n| **2. Weight dispute** | Stakeholders disagree on dimension weights | Run weight allocation workshop, decision maker resolves |\n| **3. Framework dispute** | Stakeholders disagree on the framework itself | Run two frameworks in parallel, compare results, decision maker chooses |\n| **4. Goal dispute** | Stakeholders disagree on what to optimize for | Escalate to leadership for goal alignment before any prioritization |\n| **5. Trust failure** | A stakeholder rejects the process entirely | One-on-one conversation to understand concerns, offer to redesign methodology together |\n\n**Escalation principle:** Always escalate the smallest possible disagreement. A scoring dispute should not escalate to a goal dispute unless there is genuine evidence that goals are misaligned.\n\n## Maintaining Alignment Over Time\n\n- **Regular cadence:** Re-score monthly or quarterly, even if nothing seems to have changed\n- **Trigger-based:** When new information invalidates a key assumption, re-score immediately\n- **Visible backlog:** Keep the matrix accessible, not buried in documents\n- **Retrospectives:** Compare actual outcomes to predicted scores after completing top items\n- **Re-entry:** Score new options against the existing matrix rather than creating a new one\n"
    }
  ],
  "tags": [
    "planning",
    "prioritization",
    "analysis",
    "decision-making",
    "frameworks"
  ],
  "dependsOn": [
    "context-cultivation"
  ]
}